{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.1.117:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from flask import Flask\n",
    "from flask import Flask, request, jsonify, render_template_string\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize the Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Your SentimentInference class\n",
    "class SentimentInference:\n",
    "    def __init__(self, model_path: str):\n",
    "        \"\"\"\n",
    "        Initializes the SentimentInference class.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): Path to the trained model and tokenizer.\n",
    "        \"\"\"\n",
    "        self.model_path = Path(model_path)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        self.sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "    def predict_sentiment(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Predicts the sentiment of the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text for sentiment prediction.\n",
    "\n",
    "        Returns:\n",
    "            str: Predicted sentiment ('negative', 'neutral', 'positive').\n",
    "        \"\"\"\n",
    "        # Tokenize the input text\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        \n",
    "        # Get model predictions\n",
    "        outputs = self.model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "        \n",
    "        # Map the predicted class to sentiment\n",
    "        return self.sentiment_map[predicted_class]\n",
    "\n",
    "# Initialize the SentimentInference class\n",
    "model_path = \"C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/trained_model\"\n",
    "sentiment_inference = SentimentInference(model_path)\n",
    "\n",
    "# Home page route\n",
    "@app.route(\"/\", methods=[\"GET\"])\n",
    "def home():\n",
    "    return \"\"\"\n",
    "    <h1>Welcome to Sentiment Analysis / Test Classification</h1>\n",
    "    <p>Click <a href=\"/predict\">here</a> to go to the prediction page.</p>\n",
    "    \"\"\"\n",
    "\n",
    "# Predict page route\n",
    "@app.route(\"/predict\", methods=[\"GET\", \"POST\"])\n",
    "def predict():\n",
    "    if request.method == \"GET\":\n",
    "        # Render a basic HTML form for text input\n",
    "        return render_template_string('''\n",
    "        <h1>Sentiment Prediction</h1>\n",
    "        <form method=\"POST\">\n",
    "            <label for=\"text\">Enter your text:</label><br>\n",
    "            <textarea id=\"text\" name=\"text\" rows=\"4\" cols=\"50\"></textarea><br><br>\n",
    "            <input type=\"submit\" value=\"Predict\">\n",
    "        </form>\n",
    "        ''')\n",
    "    elif request.method == \"POST\":\n",
    "        # Get the input text from the form\n",
    "        text = request.form.get(\"text\")\n",
    "        \n",
    "        # Validate the input\n",
    "        if not text:\n",
    "            return jsonify({\"error\": \"No text provided\"}), 400\n",
    "        \n",
    "        # Predict the sentiment\n",
    "        try:\n",
    "            sentiment = sentiment_inference.predict_sentiment(text)\n",
    "            return jsonify({\"sentiment\": sentiment})\n",
    "        except Exception as e:\n",
    "            return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the Flask app\n",
    "    app.run(host=\"0.0.0.0\", port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PredictionPipeline:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the PredictionPipeline class.\n",
    "        \"\"\"\n",
    "        self.model_path = Path(\"artifacts/training/trained_model\")  # Path to the trained model\n",
    "        self.sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"\n",
    "        Loads the trained model and tokenizer.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "            logger.info(\"Model and tokenizer loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model or tokenizer: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def predict(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Predicts the sentiment of the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text for sentiment prediction.\n",
    "\n",
    "        Returns:\n",
    "            str: Predicted sentiment ('negative', 'neutral', 'positive').\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Tokenize the input text\n",
    "            inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "            \n",
    "            # Get model predictions\n",
    "            outputs = self.model(**inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "            \n",
    "            # Map the predicted class to sentiment\n",
    "            return self.sentiment_map[predicted_class]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during prediction: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PredictionPipeline:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the PredictionPipeline class.\n",
    "        \"\"\"\n",
    "        self.model_path = Path(\"artifacts/training/trained_model\")  # Path to the trained model\n",
    "        self.sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"\n",
    "        Loads the trained model and tokenizer.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "            logger.info(\"Model and tokenizer loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model or tokenizer: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def predict(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Predicts the sentiment of the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text for sentiment prediction.\n",
    "\n",
    "        Returns:\n",
    "            str: Predicted sentiment ('negative', 'neutral', 'positive').\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Tokenize the input text\n",
    "            inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "            \n",
    "            # Get model predictions\n",
    "            outputs = self.model(**inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "            \n",
    "            # Map the predicted class to sentiment\n",
    "            return self.sentiment_map[predicted_class]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during prediction: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PredictionPipeline' from 'textClassifier.pipeline.prediction' (c:\\users\\prass\\onedrive\\desktop\\practise\\new_env\\text-classification-using-bert\\src\\textClassifier\\pipeline\\prediction.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mflask\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Flask, request, jsonify, render_template_string\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtextClassifier\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprediction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PredictionPipeline\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtextClassifier\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprediction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PredictionPipeline\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'PredictionPipeline' from 'textClassifier.pipeline.prediction' (c:\\users\\prass\\onedrive\\desktop\\practise\\new_env\\text-classification-using-bert\\src\\textClassifier\\pipeline\\prediction.py)"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, render_template_string\n",
    "from textClassifier.pipeline.prediction import PredictionPipeline\n",
    "from textClassifier.pipeline.prediction import PredictionPipeline\n",
    "import logging\n",
    "\n",
    "# Initialize the Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize the PredictionPipeline\n",
    "try:\n",
    "    sentiment_inference = PredictionPipeline()\n",
    "    logger.info(\"PredictionPipeline initialized successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize PredictionPipeline: {e}\")\n",
    "    raise e\n",
    "\n",
    "# Home page route\n",
    "@app.route(\"/\", methods=[\"GET\"])\n",
    "def home():\n",
    "    return \"\"\"\n",
    "    <h1>Welcome to Sentiment Analysis / Text Classification</h1>\n",
    "    <p>Click <a href=\"/predict\">here</a> to go to the prediction page.</p>\n",
    "    \"\"\"\n",
    "\n",
    "# Predict page route\n",
    "@app.route(\"/predict\", methods=[\"GET\", \"POST\"])\n",
    "def predict():\n",
    "    if request.method == \"GET\":\n",
    "        # Render a basic HTML form for text input\n",
    "        return render_template_string('''\n",
    "        <h1>Sentiment Prediction</h1>\n",
    "        <form method=\"POST\">\n",
    "            <label for=\"text\">Enter your text:</label><br>\n",
    "            <textarea id=\"text\" name=\"text\" rows=\"4\" cols=\"50\"></textarea><br><br>\n",
    "            <input type=\"submit\" value=\"Predict\">\n",
    "        </form>\n",
    "        ''')\n",
    "    elif request.method == \"POST\":\n",
    "        # Get the input text from the form\n",
    "        text = request.form.get(\"text\")\n",
    "        \n",
    "        # Validate the input\n",
    "        if not text:\n",
    "            return jsonify({\"error\": \"No text provided\"}), 400\n",
    "        \n",
    "        # Predict the sentiment\n",
    "        try:\n",
    "            sentiment = sentiment_inference.predict(text)\n",
    "            logger.info(f\"Prediction successful for text: {text}\")\n",
    "            return jsonify({\"sentiment\": sentiment})\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Prediction failed for text: {text}. Error: {e}\")\n",
    "            return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the Flask app\n",
    "    app.run(host=\"0.0.0.0\", port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PredictionPipeline:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the PredictionPipeline class.\n",
    "        \"\"\"\n",
    "        self.model_path = Path(\"artifacts/training/trained_model\")  # Ensure this path is correct\n",
    "        self.sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "\n",
    "        # Debug: Print the model path\n",
    "        print(\"Model Path:\", self.model_path)\n",
    "\n",
    "        # Debug: Check if the path exists\n",
    "        if not self.model_path.exists():\n",
    "            raise FileNotFoundError(f\"Model path does not exist: {self.model_path}\")\n",
    "\n",
    "        self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"\n",
    "        Loads the trained model and tokenizer.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "            logger.info(\"Model and tokenizer loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model or tokenizer: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def predict(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Predicts the sentiment of the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text for sentiment prediction.\n",
    "\n",
    "        Returns:\n",
    "            str: Predicted sentiment ('negative', 'neutral', 'positive').\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Tokenize the input text\n",
    "            inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "            \n",
    "            # Get model predictions\n",
    "            outputs = self.model(**inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "            \n",
    "            # Map the predicted class to sentiment\n",
    "            return self.sentiment_map[predicted_class]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during prediction: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied: config.json\n",
      "Copied: model.safetensors\n",
      "Copied: special_tokens_map.json\n",
      "Copied: tokenizer.json\n",
      "Copied: tokenizer_config.json\n",
      "Copied: training_args.bin\n",
      "Copied: vocab.txt\n",
      "All files copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Define source and destination paths\n",
    "source_dir = Path(\"artifacts/training/trained_model\")\n",
    "destination_dir = Path(\"model\")\n",
    "\n",
    "# Create the destination folder if it doesn't exist\n",
    "destination_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy files\n",
    "for file in source_dir.iterdir():\n",
    "    if file.is_file():\n",
    "        shutil.copy(file, destination_dir / file.name)\n",
    "        print(f\"Copied: {file.name}\")\n",
    "\n",
    "print(\"All files copied successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
