{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "02_feature_engineering_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective:\n",
    "    1. Reading the file from the loacation\n",
    "    2. Doing data preprcessing like extracting relevant features, maping values.\n",
    "    3. Let's do 1 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature_engineering: \\\n",
    "  root_dir: artifacts/feature_engineering  \\\n",
    "  training_data_path: artifacts/feature_engineering  \\\n",
    "  training_data_file: artifacts/feature_engineering/tweets.cs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataPreprocessingConfig:\n",
    "    root_dir: Path\n",
    "    training_data_path: Path\n",
    "    training_data_file: Path\n",
    "    training_cleansed_data: Path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT\\\\research'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CONFIG_FILE_PATH = Path(\"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT/config/config.yaml\")\n",
    "PARAMS_FILE_PATH = Path(\"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT/params.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.textClassifier.constants import *\n",
    "from src.textClassifier.utils.commons import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "\n",
    "    def get_data_preprocessing_config(self) -> DataPreprocessingConfig:\n",
    "        config = self.config.feature_engineering\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_preprocessing_config = DataPreprocessingConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            training_data_path=config.training_data_path,\n",
    "            training_data_file=config.training_data_file,\n",
    "            training_cleansed_data=config.training_cleansed_data\n",
    "        )\n",
    "\n",
    "        return data_preprocessing_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textClassifier import logger\n",
    "# import zipfile\n",
    "# import os\n",
    "# from textClassifier.utils.commons import get_size\n",
    "# import gdown\n",
    "# import spacy\n",
    "# from nltk.corpus import stopwords\n",
    "# import string\n",
    "\n",
    "\n",
    "\n",
    "# class DataPreprocessing:\n",
    "#     def __init__(self, config: DataPreprocessingConfig):\n",
    "#         self.config = config\n",
    "\n",
    "#     def preprocess_text(self):\n",
    "#         nlp = spacy.load('en_core_web_sm')\n",
    "#         stop_words = set(stopwords.words('english'))\n",
    "#         text = text.lower() # Convert to lowercase\n",
    "\n",
    "#         # Remove punctuation\n",
    "#         text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "#         # Tokenize and lemmatize\n",
    "#         doc = nlp(text)\n",
    "#         tokens = [token.lemma_ for token in doc if token.text not in stop_words]\n",
    "#         return ' '.join(tokens)\n",
    "\n",
    "#     def pass_data_to_text_process(self):\n",
    "#         # self.config.training_data_file\n",
    "#         data['cleaned_text'] = data['text'].apply(preprocess_text)\n",
    "#         return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp = DataPreprocessing(config_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\artifacts\\data_ingestion\\Tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi dude happy'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dp.preprocess_text('hi dude Happy was how are you doing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataPreprocessingConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mDataPreprocessing\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: DataPreprocessingConfig):\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m, in \u001b[0;36mDataPreprocessing\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mDataPreprocessing\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: \u001b[43mDataPreprocessingConfig\u001b[49m):\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DataPreprocessingConfig' is not defined"
     ]
    }
   ],
   "source": [
    "from textClassifier import logger\n",
    "import zipfile\n",
    "import os\n",
    "from textClassifier.utils.commons import get_size\n",
    "import gdown\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class DataPreprocessing:\n",
    "    def __init__(self, config: DataPreprocessingConfig):\n",
    "        self.config = config\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "    def text_process(self, text: str) -> str:\n",
    "        text = text.lower()\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        doc = self.nlp(text)\n",
    "        tokens = [token.lemma_ for token in doc if token.text not in self.stop_words]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def get_data_from_features(self) -> pd.DataFrame:\n",
    "        data = pd.read_csv(self.config.training_data_file)\n",
    "        data['cleaned_text'] = data['text'].apply(self.text_process)\n",
    "        return data\n",
    "    \n",
    "    def save_data(self, data: pd.DataFrame, path_name: str='cleaned_tweets.csv') -> None:\n",
    "        out_path_name = Path(self.config.training_cleansed_data)\n",
    "        data.to_csv(out_path_name, index=False)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from textClassifier import logger\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DataPreprocessing:\n",
    "    def __init__(self, config: DataPreprocessingConfig):\n",
    "        self.config = config\n",
    "        self.nlp = spacy.load('en_core_web_sm')  # Load spaCy model\n",
    "        self.stop_words = set(stopwords.words('english'))  # Load stopwords\n",
    "        self.mapping_labels = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        \n",
    "\n",
    "    def text_process(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocesses the input text by:\n",
    "        1. Converting to lowercase\n",
    "        2. Removing punctuation\n",
    "        3. Tokenizing and lemmatizing\n",
    "        4. Removing stopwords\n",
    "        \"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # Tokenize and lemmatize\n",
    "        doc = self.nlp(text)\n",
    "        tokens = [token.lemma_ for token in doc if token.text not in self.stop_words]\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def get_data_from_features(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads the dataset from the path specified in the config and applies preprocessing to the text column.\n",
    "        \"\"\"\n",
    "        # Check if the file exists\n",
    "        if not Path(self.config.training_data_file).exists():\n",
    "            raise FileNotFoundError(f\"File not found: {self.config.training_data_file}\")\n",
    "\n",
    "        # Load the dataset\n",
    "        data = pd.read_csv(self.config.training_data_file)\n",
    "\n",
    "        # Apply preprocessing to the text column\n",
    "        data['cleaned_text'] = data['text'].apply(self.text_process)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def save_data(self, data: pd.DataFrame, path_name: str = 'cleaned_tweets.csv') -> None:\n",
    "        \"\"\"\n",
    "        Saves the preprocessed data to the specified output file.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): The preprocessed DataFrame to save.\n",
    "            path_name (str): The name of the output file. Defaults to \"cleaned_tweets.csv\".\n",
    "        \"\"\"\n",
    "        # Create the full output file path\n",
    "        out_path_name = Path(self.config.training_cleansed_data) / path_name\n",
    "\n",
    "        # Save the DataFrame to CSV\n",
    "        data.to_csv(out_path_name, index=False)\n",
    "        logger.info(f\"Preprocessed data saved to {out_path_name}\")\n",
    "\n",
    "\n",
    "    def mapping_labels_func(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "        data['label'] = data['airline_sentiment'].map(self.mapping_labels)\n",
    "        logger.info(f\"values are mapped in the {data}\")\n",
    "        return data\n",
    "    \n",
    "    # def tokenizing_func(self, data: pd.DataFrame, text_column: str = 'cleaned_text') -> dict:\n",
    "    #     # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    #     # return tokenizer(data, padding='max_length', truncation=True, max_length=128)\n",
    "    \n",
    "    #     tokenized_inputs = self.tokenizer(\n",
    "    #         data[text_column].tolist(),  # Convert text column to list\n",
    "    #         padding='max_length',        # Pad to max length\n",
    "    #         truncation=True,             # Truncate to max length\n",
    "    #         max_length=128,              # Set max length\n",
    "    #         return_tensors='pt'          # Return PyTorch tensors\n",
    "    #     )\n",
    "    #     return tokenized_inputs\n",
    "\n",
    "    def tokenize_text(self, data: pd.DataFrame, text_column: str = 'cleaned_text') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Tokenizes the text in the specified column using BERT tokenizer.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): The DataFrame containing the text to tokenize.\n",
    "            text_column (str): The name of the column containing the text. Defaults to 'cleaned_text'.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The DataFrame with an additional 'tokenized' column.\n",
    "        \"\"\"\n",
    "        # Tokenize the text\n",
    "        data['tokenized'] = data[text_column].apply(\n",
    "            lambda x: self.tokenizer(x, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "        )\n",
    "        logger.info(f\"Text in column '{text_column}' has been tokenized.\")\n",
    "        return data\n",
    "    \n",
    "    def train_val_test_split(self, data: pd.DataFrame, test_size: float = 0.3, val_size: float = 0.5, random_state: int = 42):\n",
    "        \"\"\"\n",
    "        Splits the dataset into training, validation, and test sets.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): The DataFrame containing the tokenized data and labels.\n",
    "            test_size (float): Proportion of the dataset to include in the test split. Defaults to 0.3.\n",
    "            val_size (float): Proportion of the temp split to include in the validation split. Defaults to 0.5.\n",
    "            random_state (int): Random seed for reproducibility. Defaults to 42.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing the splits:\n",
    "            {\n",
    "                'train': {'texts': train_texts, 'labels': train_labels},\n",
    "                'val': {'texts': val_texts, 'labels': val_labels},\n",
    "                'test': {'texts': test_texts, 'labels': test_labels}\n",
    "            }\n",
    "        \"\"\"\n",
    "        # Split into train and temp (val + test)\n",
    "        train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "            data['tokenized'].to_list(), data['label'].to_list(), test_size=test_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "        # Split temp into validation and test\n",
    "        val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "            temp_texts, temp_labels, test_size=val_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Data split into train, validation, and test sets.\")\n",
    "        return {\n",
    "            'train': {'texts': train_texts, 'labels': train_labels},\n",
    "            'val': {'texts': val_texts, 'labels': val_labels},\n",
    "            'test': {'texts': test_texts, 'labels': test_labels}\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-12 12:14:12,361: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\config\\config.yaml loaded successfully]\n",
      "[2025-03-12 12:14:12,381: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\params.yaml loaded successfully]\n",
      "[2025-03-12 12:14:12,384: INFO: commons: created directory at: artifacts]\n",
      "[2025-03-12 12:14:12,386: INFO: commons: created directory at: artifacts/feature_engineering]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "text_process() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m\n\u001b[0;32m      9\u001b[0m     output \u001b[38;5;241m=\u001b[39m data_pre_process\u001b[38;5;241m.\u001b[39mtokenize_text(data\u001b[38;5;241m=\u001b[39moutput)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m get_data_pre_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_data_preprocessing_config()\n\u001b[0;32m      4\u001b[0m data_pre_process \u001b[38;5;241m=\u001b[39m DataPreprocessing(config\u001b[38;5;241m=\u001b[39mget_data_pre_config)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mdata_pre_process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m output \u001b[38;5;241m=\u001b[39m data_pre_process\u001b[38;5;241m.\u001b[39mget_data_from_features()\n\u001b[0;32m      7\u001b[0m data_pre_process\u001b[38;5;241m.\u001b[39msave_data(data\u001b[38;5;241m=\u001b[39moutput, path_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_tweets.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: text_process() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    get_data_pre_config = config.get_data_preprocessing_config()\n",
    "    data_pre_process = DataPreprocessing(config=get_data_pre_config)\n",
    "    data_pre_process.text_process()\n",
    "    output = data_pre_process.get_data_from_features()\n",
    "    data_pre_process.save_data(data=output, path_name='cleaned_tweets.csv')\n",
    "    output = data_pre_process.mapping_labels_func(data=output)\n",
    "    output = data_pre_process.tokenize_text(data=output)\n",
    "    \n",
    "\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-14 15:32:52,577: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\config\\config.yaml loaded successfully]\n",
      "[2025-02-14 15:32:52,590: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\params.yaml loaded successfully]\n",
      "[2025-02-14 15:32:52,593: INFO: commons: created directory at: artifacts]\n",
      "[2025-02-14 15:32:52,600: INFO: commons: created directory at: artifacts/feature_engineering]\n",
      "[2025-02-14 15:34:45,125: INFO: 2396406771: Preprocessed data saved to artifacts\\feature_engineering\\cleaned_tweets.csv]\n",
      "[2025-02-14 15:34:45,136: INFO: 2396406771: values are mapped in the                  tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
      "0      570306133677760513           neutral                        1.0000   \n",
      "1      570301130888122368          positive                        0.3486   \n",
      "2      570301083672813571           neutral                        0.6837   \n",
      "3      570301031407624196          negative                        1.0000   \n",
      "4      570300817074462722          negative                        1.0000   \n",
      "...                   ...               ...                           ...   \n",
      "14635  569587686496825344          positive                        0.3487   \n",
      "14636  569587371693355008          negative                        1.0000   \n",
      "14637  569587242672398336           neutral                        1.0000   \n",
      "14638  569587188687634433          negative                        1.0000   \n",
      "14639  569587140490866689           neutral                        0.6771   \n",
      "\n",
      "               negativereason  negativereason_confidence         airline  \\\n",
      "0                         NaN                        NaN  Virgin America   \n",
      "1                         NaN                     0.0000  Virgin America   \n",
      "2                         NaN                        NaN  Virgin America   \n",
      "3                  Bad Flight                     0.7033  Virgin America   \n",
      "4                  Can't Tell                     1.0000  Virgin America   \n",
      "...                       ...                        ...             ...   \n",
      "14635                     NaN                     0.0000        American   \n",
      "14636  Customer Service Issue                     1.0000        American   \n",
      "14637                     NaN                        NaN        American   \n",
      "14638  Customer Service Issue                     0.6659        American   \n",
      "14639                     NaN                     0.0000        American   \n",
      "\n",
      "      airline_sentiment_gold             name negativereason_gold  \\\n",
      "0                        NaN          cairdin                 NaN   \n",
      "1                        NaN         jnardino                 NaN   \n",
      "2                        NaN       yvonnalynn                 NaN   \n",
      "3                        NaN         jnardino                 NaN   \n",
      "4                        NaN         jnardino                 NaN   \n",
      "...                      ...              ...                 ...   \n",
      "14635                    NaN  KristenReenders                 NaN   \n",
      "14636                    NaN         itsropes                 NaN   \n",
      "14637                    NaN         sanyabun                 NaN   \n",
      "14638                    NaN       SraJackson                 NaN   \n",
      "14639                    NaN        daviddtwu                 NaN   \n",
      "\n",
      "       retweet_count                                               text  \\\n",
      "0                  0                @VirginAmerica What @dhepburn said.   \n",
      "1                  0  @VirginAmerica plus you've added commercials t...   \n",
      "2                  0  @VirginAmerica I didn't today... Must mean I n...   \n",
      "3                  0  @VirginAmerica it's really aggressive to blast...   \n",
      "4                  0  @VirginAmerica and it's a really big bad thing...   \n",
      "...              ...                                                ...   \n",
      "14635              0  @AmericanAir thank you we got on a different f...   \n",
      "14636              0  @AmericanAir leaving over 20 minutes Late Flig...   \n",
      "14637              0  @AmericanAir Please bring American Airlines to...   \n",
      "14638              0  @AmericanAir you have my money, you change my ...   \n",
      "14639              0  @AmericanAir we have 8 ppl so we need 2 know h...   \n",
      "\n",
      "      tweet_coord              tweet_created tweet_location  \\\n",
      "0             NaN  2015-02-24 11:35:52 -0800            NaN   \n",
      "1             NaN  2015-02-24 11:15:59 -0800            NaN   \n",
      "2             NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
      "3             NaN  2015-02-24 11:15:36 -0800            NaN   \n",
      "4             NaN  2015-02-24 11:14:45 -0800            NaN   \n",
      "...           ...                        ...            ...   \n",
      "14635         NaN  2015-02-22 12:01:01 -0800            NaN   \n",
      "14636         NaN  2015-02-22 11:59:46 -0800          Texas   \n",
      "14637         NaN  2015-02-22 11:59:15 -0800  Nigeria,lagos   \n",
      "14638         NaN  2015-02-22 11:59:02 -0800     New Jersey   \n",
      "14639         NaN  2015-02-22 11:58:51 -0800     dallas, TX   \n",
      "\n",
      "                    user_timezone  \\\n",
      "0      Eastern Time (US & Canada)   \n",
      "1      Pacific Time (US & Canada)   \n",
      "2      Central Time (US & Canada)   \n",
      "3      Pacific Time (US & Canada)   \n",
      "4      Pacific Time (US & Canada)   \n",
      "...                           ...   \n",
      "14635                         NaN   \n",
      "14636                         NaN   \n",
      "14637                         NaN   \n",
      "14638  Eastern Time (US & Canada)   \n",
      "14639                         NaN   \n",
      "\n",
      "                                            cleaned_text  label  \n",
      "0                             virginamerica dhepburn say      1  \n",
      "1      virginamerica plus add commercial experience t...      2  \n",
      "2      virginamerica not today must mean need take an...      1  \n",
      "3      virginamerica really aggressive blast obnoxiou...      0  \n",
      "4                     virginamerica really big bad thing      0  \n",
      "...                                                  ...    ...  \n",
      "14635     americanair thank get different flight chicago      2  \n",
      "14636  americanair leave 20 minute late flight warnin...      0  \n",
      "14637  americanair please bring american airline blac...      1  \n",
      "14638  americanair money change flight not answer pho...      0  \n",
      "14639  americanair 8 ppl need 2 know many seat next f...      1  \n",
      "\n",
      "[14640 rows x 17 columns]]\n",
      "[2025-02-14 15:34:54,168: INFO: 2396406771: Text in column 'cleaned_text' has been tokenized.]\n",
      "[2025-02-14 15:34:54,187: INFO: 2396406771: Data split into train, validation, and test sets.]\n",
      "Train set size: 10248\n",
      "Validation set size: 2196\n",
      "Test set size: 2196\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming ConfigurationManager is defined elsewhere\n",
    "    config_manager = ConfigurationManager()\n",
    "\n",
    "    # Get the data preprocessing config\n",
    "    get_data_pre_config = config_manager.get_data_preprocessing_config()\n",
    "\n",
    "    # Initialize DataPreprocessing with the config\n",
    "    data_processing = DataPreprocessing(config=get_data_pre_config)\n",
    "\n",
    "    # Load and preprocess the data\n",
    "    output = data_processing.get_data_from_features()\n",
    "\n",
    "    # Save the preprocessed data\n",
    "    data_processing.save_data(data=output, path_name='cleaned_tweets.csv')\n",
    "\n",
    "    # Map labels to numerical values\n",
    "    output = data_processing.mapping_labels_func(data=output)\n",
    "\n",
    "    # Tokenize the cleaned text\n",
    "    output = data_processing.tokenize_text(data=output)\n",
    "\n",
    "    # Split the data into train, validation, and test sets\n",
    "    splits = data_processing.train_val_test_split(data=output)\n",
    "\n",
    "    # Access the splits\n",
    "    train_data = splits['train']\n",
    "    val_data = splits['val']\n",
    "    test_data = splits['test']\n",
    "\n",
    "    # Display the sizes of the splits\n",
    "    print(f\"Train set size: {len(train_data['texts'])}\")\n",
    "    print(f\"Validation set size: {len(val_data['texts'])}\")\n",
    "    print(f\"Test set size: {len(test_data['texts'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "def convert_to_dicts(tokenized_texts):\n",
    "    \"\"\"\n",
    "    Converts a list of tokenized texts into a dictionary of input IDs and attention masks.\n",
    "\n",
    "    Args:\n",
    "        tokenized_texts (list): List of tokenized texts (each is a dictionary with 'input_ids' and 'attention_mask').\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing 'input_ids' and 'attention_mask' as lists.\n",
    "    \"\"\"\n",
    "    input_ids = [d['input_ids'].squeeze(0) for d in tokenized_texts]  # Remove batch dimension\n",
    "    attention_masks = [d['attention_mask'].squeeze(0) for d in tokenized_texts]  # Remove batch dimension\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_masks}\n",
    "\n",
    "\n",
    "def create_datasets(train_texts, train_labels, val_texts, val_labels, test_texts, test_labels):\n",
    "    \"\"\"\n",
    "    Creates SentimentDataset objects for train, validation, and test splits.\n",
    "\n",
    "    Args:\n",
    "        train_texts (list): List of tokenized texts for the training set.\n",
    "        train_labels (list): List of labels for the training set.\n",
    "        val_texts (list): List of tokenized texts for the validation set.\n",
    "        val_labels (list): List of labels for the validation set.\n",
    "        test_texts (list): List of tokenized texts for the test set.\n",
    "        test_labels (list): List of labels for the test set.\n",
    "\n",
    "    Returns:\n",
    "        train_dataset, val_dataset, test_dataset: SentimentDataset objects for each split.\n",
    "    \"\"\"\n",
    "    # Convert tokenized texts to encodings\n",
    "    train_encodings = convert_to_dicts(train_texts)\n",
    "    val_encodings = convert_to_dicts(val_texts)\n",
    "    test_encodings = convert_to_dicts(test_texts)\n",
    "\n",
    "    # Create SentimentDataset objects\n",
    "    train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "    val_dataset = SentimentDataset(val_encodings, val_labels)\n",
    "    test_dataset = SentimentDataset(test_encodings, test_labels)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-14 15:50:20,078: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\config\\config.yaml loaded successfully]\n",
      "[2025-02-14 15:50:20,082: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\params.yaml loaded successfully]\n",
      "[2025-02-14 15:50:20,084: INFO: commons: created directory at: artifacts]\n",
      "[2025-02-14 15:50:20,086: INFO: commons: created directory at: artifacts/feature_engineering]\n",
      "[2025-02-14 15:52:16,736: INFO: 2396406771: Preprocessed data saved to artifacts\\feature_engineering\\cleaned_tweets.csv]\n",
      "[2025-02-14 15:52:16,745: INFO: 2396406771: values are mapped in the                  tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
      "0      570306133677760513           neutral                        1.0000   \n",
      "1      570301130888122368          positive                        0.3486   \n",
      "2      570301083672813571           neutral                        0.6837   \n",
      "3      570301031407624196          negative                        1.0000   \n",
      "4      570300817074462722          negative                        1.0000   \n",
      "...                   ...               ...                           ...   \n",
      "14635  569587686496825344          positive                        0.3487   \n",
      "14636  569587371693355008          negative                        1.0000   \n",
      "14637  569587242672398336           neutral                        1.0000   \n",
      "14638  569587188687634433          negative                        1.0000   \n",
      "14639  569587140490866689           neutral                        0.6771   \n",
      "\n",
      "               negativereason  negativereason_confidence         airline  \\\n",
      "0                         NaN                        NaN  Virgin America   \n",
      "1                         NaN                     0.0000  Virgin America   \n",
      "2                         NaN                        NaN  Virgin America   \n",
      "3                  Bad Flight                     0.7033  Virgin America   \n",
      "4                  Can't Tell                     1.0000  Virgin America   \n",
      "...                       ...                        ...             ...   \n",
      "14635                     NaN                     0.0000        American   \n",
      "14636  Customer Service Issue                     1.0000        American   \n",
      "14637                     NaN                        NaN        American   \n",
      "14638  Customer Service Issue                     0.6659        American   \n",
      "14639                     NaN                     0.0000        American   \n",
      "\n",
      "      airline_sentiment_gold             name negativereason_gold  \\\n",
      "0                        NaN          cairdin                 NaN   \n",
      "1                        NaN         jnardino                 NaN   \n",
      "2                        NaN       yvonnalynn                 NaN   \n",
      "3                        NaN         jnardino                 NaN   \n",
      "4                        NaN         jnardino                 NaN   \n",
      "...                      ...              ...                 ...   \n",
      "14635                    NaN  KristenReenders                 NaN   \n",
      "14636                    NaN         itsropes                 NaN   \n",
      "14637                    NaN         sanyabun                 NaN   \n",
      "14638                    NaN       SraJackson                 NaN   \n",
      "14639                    NaN        daviddtwu                 NaN   \n",
      "\n",
      "       retweet_count                                               text  \\\n",
      "0                  0                @VirginAmerica What @dhepburn said.   \n",
      "1                  0  @VirginAmerica plus you've added commercials t...   \n",
      "2                  0  @VirginAmerica I didn't today... Must mean I n...   \n",
      "3                  0  @VirginAmerica it's really aggressive to blast...   \n",
      "4                  0  @VirginAmerica and it's a really big bad thing...   \n",
      "...              ...                                                ...   \n",
      "14635              0  @AmericanAir thank you we got on a different f...   \n",
      "14636              0  @AmericanAir leaving over 20 minutes Late Flig...   \n",
      "14637              0  @AmericanAir Please bring American Airlines to...   \n",
      "14638              0  @AmericanAir you have my money, you change my ...   \n",
      "14639              0  @AmericanAir we have 8 ppl so we need 2 know h...   \n",
      "\n",
      "      tweet_coord              tweet_created tweet_location  \\\n",
      "0             NaN  2015-02-24 11:35:52 -0800            NaN   \n",
      "1             NaN  2015-02-24 11:15:59 -0800            NaN   \n",
      "2             NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
      "3             NaN  2015-02-24 11:15:36 -0800            NaN   \n",
      "4             NaN  2015-02-24 11:14:45 -0800            NaN   \n",
      "...           ...                        ...            ...   \n",
      "14635         NaN  2015-02-22 12:01:01 -0800            NaN   \n",
      "14636         NaN  2015-02-22 11:59:46 -0800          Texas   \n",
      "14637         NaN  2015-02-22 11:59:15 -0800  Nigeria,lagos   \n",
      "14638         NaN  2015-02-22 11:59:02 -0800     New Jersey   \n",
      "14639         NaN  2015-02-22 11:58:51 -0800     dallas, TX   \n",
      "\n",
      "                    user_timezone  \\\n",
      "0      Eastern Time (US & Canada)   \n",
      "1      Pacific Time (US & Canada)   \n",
      "2      Central Time (US & Canada)   \n",
      "3      Pacific Time (US & Canada)   \n",
      "4      Pacific Time (US & Canada)   \n",
      "...                           ...   \n",
      "14635                         NaN   \n",
      "14636                         NaN   \n",
      "14637                         NaN   \n",
      "14638  Eastern Time (US & Canada)   \n",
      "14639                         NaN   \n",
      "\n",
      "                                            cleaned_text  label  \n",
      "0                             virginamerica dhepburn say      1  \n",
      "1      virginamerica plus add commercial experience t...      2  \n",
      "2      virginamerica not today must mean need take an...      1  \n",
      "3      virginamerica really aggressive blast obnoxiou...      0  \n",
      "4                     virginamerica really big bad thing      0  \n",
      "...                                                  ...    ...  \n",
      "14635     americanair thank get different flight chicago      2  \n",
      "14636  americanair leave 20 minute late flight warnin...      0  \n",
      "14637  americanair please bring american airline blac...      1  \n",
      "14638  americanair money change flight not answer pho...      0  \n",
      "14639  americanair 8 ppl need 2 know many seat next f...      1  \n",
      "\n",
      "[14640 rows x 17 columns]]\n",
      "[2025-02-14 15:52:25,797: INFO: 2396406771: Text in column 'cleaned_text' has been tokenized.]\n",
      "[2025-02-14 15:52:25,805: INFO: 2396406771: Data split into train, validation, and test sets.]\n",
      "{'input_ids': tensor([  101,  6892, 16558,  5657,  3462,  3770,  2620,  2620,  1055,  2595,\n",
      "        24798,  2102,  3501, 24316,  2051,  3902,  2681,  3309, 13114,  2570,\n",
      "         3742,  6724,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(1)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prass\\AppData\\Local\\Temp\\ipykernel_17580\\2803380171.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming ConfigurationManager is defined elsewhere\n",
    "    config_manager = ConfigurationManager()\n",
    "\n",
    "    # Get the data preprocessing config\n",
    "    get_data_pre_config = config_manager.get_data_preprocessing_config()\n",
    "\n",
    "    # Initialize DataPreprocessing with the config\n",
    "    data_processing = DataPreprocessing(config=get_data_pre_config)\n",
    "\n",
    "    # Load and preprocess the data\n",
    "    output = data_processing.get_data_from_features()\n",
    "\n",
    "    # Save the preprocessed data\n",
    "    data_processing.save_data(data=output, path_name='cleaned_tweets.csv')\n",
    "\n",
    "    # Map labels to numerical values\n",
    "    output = data_processing.mapping_labels_func(data=output)\n",
    "\n",
    "    # Tokenize the cleaned text\n",
    "    output = data_processing.tokenize_text(data=output)\n",
    "\n",
    "    # Split the data into train, validation, and test sets\n",
    "    splits = data_processing.train_val_test_split(data=output)\n",
    "\n",
    "    # Access the splits\n",
    "    train_data = splits['train']\n",
    "    val_data = splits['val']\n",
    "    test_data = splits['test']\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset, val_dataset, test_dataset = create_datasets(\n",
    "        train_data['texts'], train_data['labels'],\n",
    "        val_data['texts'], val_data['labels'],\n",
    "        test_data['texts'], test_data['labels']\n",
    "    )\n",
    "\n",
    "    # Example of accessing data\n",
    "    print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map labels to integers\n",
    "# label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "# data['label'] = data['airline_sentiment'].map(label_mapping)\n",
    "\n",
    "# from transformers import BertTokenizer\n",
    "\n",
    "# # Tokenization\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Tokenize the data\n",
    "# def tokenize_function(text):\n",
    "#     return tokenizer(text, padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "# data['tokenized'] = data['cleaned_text'].apply(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-12 15:56:11,364: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\config\\config.yaml loaded successfully]\n",
      "[2025-02-12 15:56:11,368: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\params.yaml loaded successfully]\n",
      "[2025-02-12 15:56:11,370: INFO: commons: created directory at: artifacts]\n",
      "[2025-02-12 15:56:11,373: INFO: commons: created directory at: artifacts/feature_engineering]\n",
      "[2025-02-12 15:58:20,522: INFO: 751830446: Preprocessed data saved to artifacts\\feature_engineering\\cleaned_tweets.csv]\n",
      "[2025-02-12 15:58:20,533: INFO: 751830446: values are mapped in the                  tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
      "0      570306133677760513           neutral                        1.0000   \n",
      "1      570301130888122368          positive                        0.3486   \n",
      "2      570301083672813571           neutral                        0.6837   \n",
      "3      570301031407624196          negative                        1.0000   \n",
      "4      570300817074462722          negative                        1.0000   \n",
      "...                   ...               ...                           ...   \n",
      "14635  569587686496825344          positive                        0.3487   \n",
      "14636  569587371693355008          negative                        1.0000   \n",
      "14637  569587242672398336           neutral                        1.0000   \n",
      "14638  569587188687634433          negative                        1.0000   \n",
      "14639  569587140490866689           neutral                        0.6771   \n",
      "\n",
      "               negativereason  negativereason_confidence         airline  \\\n",
      "0                         NaN                        NaN  Virgin America   \n",
      "1                         NaN                     0.0000  Virgin America   \n",
      "2                         NaN                        NaN  Virgin America   \n",
      "3                  Bad Flight                     0.7033  Virgin America   \n",
      "4                  Can't Tell                     1.0000  Virgin America   \n",
      "...                       ...                        ...             ...   \n",
      "14635                     NaN                     0.0000        American   \n",
      "14636  Customer Service Issue                     1.0000        American   \n",
      "14637                     NaN                        NaN        American   \n",
      "14638  Customer Service Issue                     0.6659        American   \n",
      "14639                     NaN                     0.0000        American   \n",
      "\n",
      "      airline_sentiment_gold             name negativereason_gold  \\\n",
      "0                        NaN          cairdin                 NaN   \n",
      "1                        NaN         jnardino                 NaN   \n",
      "2                        NaN       yvonnalynn                 NaN   \n",
      "3                        NaN         jnardino                 NaN   \n",
      "4                        NaN         jnardino                 NaN   \n",
      "...                      ...              ...                 ...   \n",
      "14635                    NaN  KristenReenders                 NaN   \n",
      "14636                    NaN         itsropes                 NaN   \n",
      "14637                    NaN         sanyabun                 NaN   \n",
      "14638                    NaN       SraJackson                 NaN   \n",
      "14639                    NaN        daviddtwu                 NaN   \n",
      "\n",
      "       retweet_count                                               text  \\\n",
      "0                  0                @VirginAmerica What @dhepburn said.   \n",
      "1                  0  @VirginAmerica plus you've added commercials t...   \n",
      "2                  0  @VirginAmerica I didn't today... Must mean I n...   \n",
      "3                  0  @VirginAmerica it's really aggressive to blast...   \n",
      "4                  0  @VirginAmerica and it's a really big bad thing...   \n",
      "...              ...                                                ...   \n",
      "14635              0  @AmericanAir thank you we got on a different f...   \n",
      "14636              0  @AmericanAir leaving over 20 minutes Late Flig...   \n",
      "14637              0  @AmericanAir Please bring American Airlines to...   \n",
      "14638              0  @AmericanAir you have my money, you change my ...   \n",
      "14639              0  @AmericanAir we have 8 ppl so we need 2 know h...   \n",
      "\n",
      "      tweet_coord              tweet_created tweet_location  \\\n",
      "0             NaN  2015-02-24 11:35:52 -0800            NaN   \n",
      "1             NaN  2015-02-24 11:15:59 -0800            NaN   \n",
      "2             NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
      "3             NaN  2015-02-24 11:15:36 -0800            NaN   \n",
      "4             NaN  2015-02-24 11:14:45 -0800            NaN   \n",
      "...           ...                        ...            ...   \n",
      "14635         NaN  2015-02-22 12:01:01 -0800            NaN   \n",
      "14636         NaN  2015-02-22 11:59:46 -0800          Texas   \n",
      "14637         NaN  2015-02-22 11:59:15 -0800  Nigeria,lagos   \n",
      "14638         NaN  2015-02-22 11:59:02 -0800     New Jersey   \n",
      "14639         NaN  2015-02-22 11:58:51 -0800     dallas, TX   \n",
      "\n",
      "                    user_timezone  \\\n",
      "0      Eastern Time (US & Canada)   \n",
      "1      Pacific Time (US & Canada)   \n",
      "2      Central Time (US & Canada)   \n",
      "3      Pacific Time (US & Canada)   \n",
      "4      Pacific Time (US & Canada)   \n",
      "...                           ...   \n",
      "14635                         NaN   \n",
      "14636                         NaN   \n",
      "14637                         NaN   \n",
      "14638  Eastern Time (US & Canada)   \n",
      "14639                         NaN   \n",
      "\n",
      "                                            cleaned_text  label  \n",
      "0                             virginamerica dhepburn say      1  \n",
      "1      virginamerica plus add commercial experience t...      2  \n",
      "2      virginamerica not today must mean need take an...      1  \n",
      "3      virginamerica really aggressive blast obnoxiou...      0  \n",
      "4                     virginamerica really big bad thing      0  \n",
      "...                                                  ...    ...  \n",
      "14635     americanair thank get different flight chicago      2  \n",
      "14636  americanair leave 20 minute late flight warnin...      0  \n",
      "14637  americanair please bring american airline blac...      1  \n",
      "14638  americanair money change flight not answer pho...      0  \n",
      "14639  americanair 8 ppl need 2 know many seat next f...      1  \n",
      "\n",
      "[14640 rows x 17 columns]]\n",
      "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
      "0  570306133677760513           neutral                        1.0000   \n",
      "1  570301130888122368          positive                        0.3486   \n",
      "2  570301083672813571           neutral                        0.6837   \n",
      "3  570301031407624196          negative                        1.0000   \n",
      "4  570300817074462722          negative                        1.0000   \n",
      "\n",
      "  negativereason  negativereason_confidence         airline  \\\n",
      "0            NaN                        NaN  Virgin America   \n",
      "1            NaN                     0.0000  Virgin America   \n",
      "2            NaN                        NaN  Virgin America   \n",
      "3     Bad Flight                     0.7033  Virgin America   \n",
      "4     Can't Tell                     1.0000  Virgin America   \n",
      "\n",
      "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
      "0                    NaN     cairdin                 NaN              0   \n",
      "1                    NaN    jnardino                 NaN              0   \n",
      "2                    NaN  yvonnalynn                 NaN              0   \n",
      "3                    NaN    jnardino                 NaN              0   \n",
      "4                    NaN    jnardino                 NaN              0   \n",
      "\n",
      "                                                text tweet_coord  \\\n",
      "0                @VirginAmerica What @dhepburn said.         NaN   \n",
      "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
      "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
      "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
      "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
      "\n",
      "               tweet_created tweet_location               user_timezone  \\\n",
      "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)   \n",
      "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)   \n",
      "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)   \n",
      "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)   \n",
      "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)   \n",
      "\n",
      "                                        cleaned_text  label  \n",
      "0                         virginamerica dhepburn say      1  \n",
      "1  virginamerica plus add commercial experience t...      2  \n",
      "2  virginamerica not today must mean need take an...      1  \n",
      "3  virginamerica really aggressive blast obnoxiou...      0  \n",
      "4                 virginamerica really big bad thing      0  \n",
      "{'input_ids': tensor([[  101,  6261, 14074, 14735, 28144, 13699,  8022,  2360,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  6261, 14074, 14735,  4606,  5587,  3293,  3325, 26997,  2100,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  6261, 14074, 14735,  2025,  2651,  2442,  2812,  2342,  2202,\n",
      "          2178,  4440,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  6261, 14074, 14735,  2428,  9376,  8479, 27885,  3630, 25171,\n",
      "          4024,  4113,  2227, 23713,  2210, 28667, 22957,  2063,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  6261, 14074, 14735,  2428,  2502,  2919,  2518,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "config_manager = ConfigurationManager()\n",
    "\n",
    "# Get the data preprocessing config\n",
    "get_data_pre_config = config_manager.get_data_preprocessing_config()\n",
    "\n",
    "# Initialize DataPreprocessing with the config\n",
    "data_processing = DataPreprocessing(config=get_data_pre_config)\n",
    "\n",
    "# Load and preprocess the data\n",
    "output = data_processing.get_data_from_features()\n",
    "\n",
    "# Save the preprocessed data\n",
    "data_processing.save_data(data=output, path_name='cleaned_tweets.csv')\n",
    "\n",
    "# mapping = data_processing.mapping_labels_func()\n",
    "output = data_processing.mapping_labels_func(data = output)\n",
    "print(output.head())\n",
    "tokenized_data = data_processing.tokenizing_func(data=output)\n",
    "print(tokenized_data[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def train_test_split_func(tokenized_data, labels, test_size=0.3, val_size=0.5, random_state=42):\n",
    "    # Split into train and temp (val + test)\n",
    "    train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "        tokenized_data['input_ids'], labels, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Split temp into validation and test\n",
    "    val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "            temp_texts, temp_labels, test_size=val_size, random_state=random_state\n",
    "        )\n",
    "        \n",
    "    return train_texts, val_texts, test_texts, train_labels, val_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import Dataset\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# class DataPreprocessing:\n",
    "#     def __init__(self, config):\n",
    "#         \"\"\"\n",
    "#         Initializes the DataPreprocessing class.\n",
    "\n",
    "#         Args:\n",
    "#             config: Configuration object containing paths and settings.\n",
    "#         \"\"\"\n",
    "#         self.config = config\n",
    "#         self.nlp = spacy.load('en_core_web_sm')  # Load spaCy model\n",
    "#         self.stop_words = set(stopwords.words('english'))  # Load stopwords\n",
    "#         self.mapping_labels = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "#         self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "#         # Load the dataset during initialization\n",
    "#         self.data = self._load_data()\n",
    "\n",
    "#         # Set the default output file path\n",
    "#         self.output_file_path = Path(self.config.training_cleansed_data) / 'cleaned_tweets.csv'\n",
    "\n",
    "#     def _load_data(self) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Loads the dataset from the path specified in the config.\n",
    "\n",
    "#         Returns:\n",
    "#             pd.DataFrame: The loaded dataset.\n",
    "#         \"\"\"\n",
    "#         # Check if the file exists\n",
    "#         if not Path(self.config.training_data_file).exists():\n",
    "#             raise FileNotFoundError(f\"File not found: {self.config.training_data_file}\")\n",
    "\n",
    "#         # Load the dataset\n",
    "#         data = pd.read_csv(self.config.training_data_file)\n",
    "#         logger.info(f\"Dataset loaded from {self.config.training_data_file}\")\n",
    "#         return data\n",
    "\n",
    "#     def text_process(self) -> None:\n",
    "#         \"\"\"\n",
    "#         Preprocesses the text column in the dataset by:\n",
    "#         1. Converting to lowercase\n",
    "#         2. Removing punctuation\n",
    "#         3. Tokenizing and lemmatizing\n",
    "#         4. Removing stopwords\n",
    "#         \"\"\"\n",
    "#         # Apply preprocessing to the text column\n",
    "#         self.data['cleaned_text'] = self.data['text'].apply(self._process_single_text)\n",
    "#         logger.info(\"Text preprocessing completed.\")\n",
    "\n",
    "#     def _process_single_text(self, text: str) -> str:\n",
    "#         \"\"\"\n",
    "#         Helper method to preprocess a single text string.\n",
    "\n",
    "#         Args:\n",
    "#             text (str): The input text to preprocess.\n",
    "\n",
    "#         Returns:\n",
    "#             str: The preprocessed text.\n",
    "#         \"\"\"\n",
    "#         # Convert to lowercase\n",
    "#         text = text.lower()\n",
    "\n",
    "#         # Remove punctuation\n",
    "#         text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "#         # Tokenize and lemmatize\n",
    "#         doc = self.nlp(text)\n",
    "#         tokens = [token.lemma_ for token in doc if token.text not in self.stop_words]\n",
    "\n",
    "#         return ' '.join(tokens)\n",
    "\n",
    "#     def mapping_labels_func(self) -> None:\n",
    "#         \"\"\"\n",
    "#         Maps the sentiment labels to numerical values.\n",
    "#         \"\"\"\n",
    "#         self.data['label'] = self.data['airline_sentiment'].map(self.mapping_labels)\n",
    "#         logger.info(\"Labels mapped to numerical values.\")\n",
    "\n",
    "#     def tokenize_text(self) -> None:\n",
    "#         \"\"\"\n",
    "#         Tokenizes the cleaned text column using BERT tokenizer.\n",
    "#         \"\"\"\n",
    "#         # Tokenize the text\n",
    "#         self.data['tokenized'] = self.data['cleaned_text'].apply(\n",
    "#             lambda x: self.tokenizer(x, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "#         )\n",
    "#         logger.info(\"Text tokenization completed.\")\n",
    "\n",
    "#     def save_data(self) -> None:\n",
    "#         \"\"\"\n",
    "#         Saves the preprocessed data to the default output file.\n",
    "#         \"\"\"\n",
    "#         # Save the DataFrame to CSV\n",
    "#         self.data.to_csv(self.output_file_path, index=False)\n",
    "#         logger.info(f\"Preprocessed data saved to {self.output_file_path}\")\n",
    "\n",
    "#     def train_val_test_split(self, test_size: float = 0.3, val_size: float = 0.5, random_state: int = 42) -> dict:\n",
    "#         \"\"\"\n",
    "#         Splits the dataset into training, validation, and test sets.\n",
    "\n",
    "#         Args:\n",
    "#             test_size (float): Proportion of the dataset to include in the test split. Defaults to 0.3.\n",
    "#             val_size (float): Proportion of the temp split to include in the validation split. Defaults to 0.5.\n",
    "#             random_state (int): Random seed for reproducibility. Defaults to 42.\n",
    "\n",
    "#         Returns:\n",
    "#             A dictionary containing the splits:\n",
    "#             {\n",
    "#                 'train': {'texts': train_texts, 'labels': train_labels},\n",
    "#                 'val': {'texts': val_texts, 'labels': val_labels},\n",
    "#                 'test': {'texts': test_texts, 'labels': test_labels}\n",
    "#             }\n",
    "#         \"\"\"\n",
    "#         # Split into train and temp (val + test)\n",
    "#         train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "#             self.data['cleaned_text'].to_list(), self.data['label'].to_list(), test_size=test_size, random_state=random_state\n",
    "#         )\n",
    "\n",
    "#         # Split temp into validation and test\n",
    "#         val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "#             temp_texts, temp_labels, test_size=val_size, random_state=random_state\n",
    "#         )\n",
    "\n",
    "#         logger.info(\"Data split into train, validation, and test sets.\")\n",
    "#         return {\n",
    "#             'train': {'texts': train_texts, 'labels': train_labels},\n",
    "#             'val': {'texts': val_texts, 'labels': val_labels},\n",
    "#             'test': {'texts': test_texts, 'labels': test_labels}\n",
    "#         }\n",
    "\n",
    "#     def convert_to_tokenized_datasets(self, splits: dict) -> dict:\n",
    "#         \"\"\"\n",
    "#         Converts the train, validation, and test splits into tokenized datasets.\n",
    "\n",
    "#         Args:\n",
    "#             splits (dict): A dictionary containing the splits:\n",
    "#                 {\n",
    "#                     'train': {'texts': train_texts, 'labels': train_labels},\n",
    "#                     'val': {'texts': val_texts, 'labels': val_labels},\n",
    "#                     'test': {'texts': test_texts, 'labels': test_labels}\n",
    "#                 }\n",
    "\n",
    "#         Returns:\n",
    "#             dict: A dictionary containing the tokenized datasets:\n",
    "#                 {\n",
    "#                     'train': tokenized_train_dataset,\n",
    "#                     'val': tokenized_val_dataset,\n",
    "#                     'test': tokenized_test_dataset\n",
    "#                 }\n",
    "#         \"\"\"\n",
    "#         # Convert splits to DataFrames\n",
    "#         train_df = pd.DataFrame({'cleaned_text': splits['train']['texts'], 'label': splits['train']['labels']})\n",
    "#         val_df = pd.DataFrame({'cleaned_text': splits['val']['texts'], 'label': splits['val']['labels']})\n",
    "#         test_df = pd.DataFrame({'cleaned_text': splits['test']['texts'], 'label': splits['test']['labels']})\n",
    "\n",
    "#         # Convert DataFrames to datasets.Dataset\n",
    "#         train_dataset = Dataset.from_pandas(train_df)\n",
    "#         val_dataset = Dataset.from_pandas(val_df)\n",
    "#         test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "#         # Tokenize the datasets\n",
    "#         def tokenize_data(examples):\n",
    "#             return self.tokenizer(examples[\"cleaned_text\"], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "#         tokenized_train = train_dataset.map(tokenize_data, batched=True)\n",
    "#         tokenized_val = val_dataset.map(tokenize_data, batched=True)\n",
    "#         tokenized_test = test_dataset.map(tokenize_data, batched=True)\n",
    "\n",
    "#         logger.info(\"Datasets tokenized successfully.\")\n",
    "#         return {\n",
    "#             'train': tokenized_train,\n",
    "#             'val': tokenized_val,\n",
    "#             'test': tokenized_test\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    get_data_pre_config = config.get_data_preprocessing_config()\n",
    "    data_pre_process = DataPreprocessing(config=get_data_pre_config)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    data_pre_process.text_process()\n",
    "    data_pre_process.mapping_labels_func()\n",
    "    data_pre_process.tokenize_text()\n",
    "    \n",
    "    # Save the preprocessed data\n",
    "    data_pre_process.save_data()  # No need to pass the path\n",
    "    \n",
    "    # Split the data into train, validation, and test sets\n",
    "    splits = data_pre_process.train_val_test_split()\n",
    "    \n",
    "    # Convert splits to tokenized datasets\n",
    "    tokenized_datasets = data_pre_process.convert_to_tokenized_datasets(splits)\n",
    "    \n",
    "    # Access the tokenized datasets\n",
    "    train_dataset = tokenized_datasets['train']\n",
    "    val_dataset = tokenized_datasets['val']\n",
    "    test_dataset = tokenized_datasets['test']\n",
    "    \n",
    "    logger.info(\"Tokenized datasets created successfully.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during data preprocessing: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataPreprocessingConfig:\n",
    "    root_dir: Path\n",
    "    training_data_path: Path\n",
    "    training_data_file: Path\n",
    "    training_cleansed_data: Path\n",
    "    datasets_dir: Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "\n",
    "    def get_data_preprocessing_config(self) -> DataPreprocessingConfig:\n",
    "        config = self.config.feature_engineering\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_preprocessing_config = DataPreprocessingConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            training_data_path=config.training_data_path,\n",
    "            training_data_file=config.training_data_file,\n",
    "            training_cleansed_data=config.training_cleansed_data,\n",
    "            datasets_dir=config.datasets_dir\n",
    "        )\n",
    "\n",
    "        return data_preprocessing_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        \"\"\"\n",
    "        Custom PyTorch Dataset for sentiment analysis.\n",
    "\n",
    "        Args:\n",
    "            encodings (dict): Tokenized encodings (e.g., input_ids, attention_mask).\n",
    "            labels (list): List of labels corresponding to the encodings.\n",
    "        \"\"\"\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "class DataPreprocessing:\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initializes the DataPreprocessing class.\n",
    "\n",
    "        Args:\n",
    "            config: Configuration object containing paths and settings.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.nlp = spacy.load('en_core_web_sm')  # Load spaCy model\n",
    "        self.stop_words = set(stopwords.words('english'))  # Load stopwords\n",
    "        self.mapping_labels = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "        # self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "        # Load the dataset during initialization\n",
    "        self.data = self._load_data()\n",
    "\n",
    "        # Set the default output file path\n",
    "        self.output_file_path = Path(self.config.training_cleansed_data) / 'cleaned_tweets.csv'\n",
    "        self.datasets_dir = Path(self.config.datasets_dir)\n",
    "\n",
    "    def _load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads the dataset from the path specified in the config.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The loaded dataset.\n",
    "        \"\"\"\n",
    "        # Check if the file exists\n",
    "        if not Path(self.config.training_data_file).exists():\n",
    "            raise FileNotFoundError(f\"File not found: {self.config.training_data_file}\")\n",
    "\n",
    "        # Load the dataset\n",
    "        data = pd.read_csv(self.config.training_data_file)\n",
    "        logger.info(f\"Dataset loaded from {self.config.training_data_file}\")\n",
    "        return data\n",
    "\n",
    "    def text_process(self) -> None:\n",
    "        \"\"\"\n",
    "        Preprocesses the text column in the dataset by:\n",
    "        1. Converting to lowercase\n",
    "        2. Removing punctuation\n",
    "        3. Tokenizing and lemmatizing\n",
    "        4. Removing stopwords\n",
    "        \"\"\"\n",
    "        # Apply preprocessing to the text column\n",
    "        self.data['cleaned_text'] = self.data['text'].apply(self._process_single_text)\n",
    "        logger.info(\"Text preprocessing completed.\")\n",
    "\n",
    "    def _process_single_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Helper method to preprocess a single text string.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to preprocess.\n",
    "\n",
    "        Returns:\n",
    "            str: The preprocessed text.\n",
    "        \"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # Tokenize and lemmatize\n",
    "        doc = self.nlp(text)\n",
    "        tokens = [token.lemma_ for token in doc if token.text not in self.stop_words]\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def mapping_labels_func(self) -> None:\n",
    "        \"\"\"\n",
    "        Maps the sentiment labels to numerical values.\n",
    "        \"\"\"\n",
    "        self.data['label'] = self.data['airline_sentiment'].map(self.mapping_labels)\n",
    "        logger.info(\"Labels mapped to numerical values.\")\n",
    "\n",
    "    def tokenize_text(self) -> None:\n",
    "        \"\"\"\n",
    "        Tokenizes the cleaned text column using BERT tokenizer.\n",
    "        \"\"\"\n",
    "        # Tokenize the text\n",
    "        self.data['tokenized'] = self.data['cleaned_text'].apply(\n",
    "            lambda x: self.tokenizer(x, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "        )\n",
    "        logger.info(\"Text tokenization completed.\")\n",
    "\n",
    "    def save_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Saves the preprocessed data to the default output file.\n",
    "        \"\"\"\n",
    "        # Save the DataFrame to CSV\n",
    "        self.data.to_csv(self.output_file_path, index=False)\n",
    "        logger.info(f\"Preprocessed data saved to {self.output_file_path}\")\n",
    "\n",
    "    def train_val_test_split(self, test_size: float = 0.3, val_size: float = 0.5, random_state: int = 42) -> dict:\n",
    "        \"\"\"\n",
    "        Splits the dataset into training, validation, and test sets.\n",
    "\n",
    "        Args:\n",
    "            test_size (float): Proportion of the dataset to include in the test split. Defaults to 0.3.\n",
    "            val_size (float): Proportion of the temp split to include in the validation split. Defaults to 0.5.\n",
    "            random_state (int): Random seed for reproducibility. Defaults to 42.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing the splits:\n",
    "            {\n",
    "                'train': {'texts': train_texts, 'labels': train_labels},\n",
    "                'val': {'texts': val_texts, 'labels': val_labels},\n",
    "                'test': {'texts': test_texts, 'labels': test_labels}\n",
    "            }\n",
    "        \"\"\"\n",
    "        # Split into train and temp (val + test)\n",
    "        train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "            self.data['cleaned_text'].to_list(), self.data['label'].to_list(), test_size=test_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "        # Split temp into validation and test\n",
    "        val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "            temp_texts, temp_labels, test_size=val_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "        logger.info(\"Data split into train, validation, and test sets.\")\n",
    "        return {\n",
    "            'train': {'texts': train_texts, 'labels': train_labels},\n",
    "            'val': {'texts': val_texts, 'labels': val_labels},\n",
    "            'test': {'texts': test_texts, 'labels': test_labels}\n",
    "        }\n",
    "\n",
    "    def convert_to_tokenized_datasets(self, splits: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Converts the train, validation, and test splits into PyTorch datasets.\n",
    "\n",
    "        Args:\n",
    "            splits (dict): A dictionary containing the splits:\n",
    "                {\n",
    "                    'train': {'texts': train_texts, 'labels': train_labels},\n",
    "                    'val': {'texts': val_texts, 'labels': val_labels},\n",
    "                    'test': {'texts': test_texts, 'labels': test_labels}\n",
    "                }\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the PyTorch datasets:\n",
    "                {\n",
    "                    'train': train_dataset,\n",
    "                    'val': val_dataset,\n",
    "                    'test': test_dataset\n",
    "                }\n",
    "        \"\"\"\n",
    "        # Tokenize the texts\n",
    "        train_encodings = self.tokenizer(splits['train']['texts'], truncation=True, padding=True, max_length=128)\n",
    "        val_encodings = self.tokenizer(splits['val']['texts'], truncation=True, padding=True, max_length=128)\n",
    "        test_encodings = self.tokenizer(splits['test']['texts'], truncation=True, padding=True, max_length=128)\n",
    "\n",
    "        # Create PyTorch datasets\n",
    "        train_dataset = SentimentDataset(train_encodings, splits['train']['labels'])\n",
    "        val_dataset = SentimentDataset(val_encodings, splits['val']['labels'])\n",
    "        test_dataset = SentimentDataset(test_encodings, splits['test']['labels'])\n",
    "\n",
    "        logger.info(\"PyTorch datasets created successfully.\")\n",
    "        return {\n",
    "            'train': train_dataset,\n",
    "            'val': val_dataset,\n",
    "            'test': test_dataset\n",
    "        }\n",
    "    def save_datasets(self, train_dataset, val_dataset, test_dataset):\n",
    "        \"\"\"\n",
    "        Saves the train, validation, and test datasets to the specified directory.\n",
    "\n",
    "        Args:\n",
    "            train_dataset: Training dataset.\n",
    "            val_dataset: Validation dataset.\n",
    "            test_dataset: Test dataset.\n",
    "        \"\"\"\n",
    "        # Create the directory if it doesn't exist\n",
    "        datasets_dir = Path(self.config.datasets_dir)\n",
    "        datasets_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save the datasets\n",
    "        torch.save(train_dataset, datasets_dir / \"train_dataset.pt\")\n",
    "        torch.save(val_dataset, datasets_dir / \"val_dataset.pt\")\n",
    "        torch.save(test_dataset, datasets_dir / \"test_dataset.pt\")\n",
    "\n",
    "        logger.info(f\"Datasets saved to {datasets_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-13 12:07:14,560: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\config\\config.yaml loaded successfully]\n",
      "[2025-03-13 12:07:14,573: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\params.yaml loaded successfully]\n",
      "[2025-03-13 12:07:14,580: INFO: commons: created directory at: artifacts]\n",
      "[2025-03-13 12:07:14,594: INFO: commons: created directory at: artifacts/feature_engineering]\n",
      "[2025-03-13 12:07:16,666: INFO: 2686220794: Dataset loaded from artifacts/feature_engineering/Tweets.csv]\n",
      "[2025-03-13 12:09:10,854: INFO: 2686220794: Text preprocessing completed.]\n",
      "[2025-03-13 12:09:10,873: INFO: 2686220794: Labels mapped to numerical values.]\n",
      "[2025-03-13 12:09:15,663: INFO: 2686220794: Text tokenization completed.]\n",
      "[2025-03-13 12:09:31,817: INFO: 2686220794: Preprocessed data saved to artifacts\\feature_engineering\\cleaned_tweets.csv]\n",
      "[2025-03-13 12:09:31,834: INFO: 2686220794: Data split into train, validation, and test sets.]\n",
      "[2025-03-13 12:09:32,323: INFO: 2686220794: PyTorch datasets created successfully.]\n",
      "[2025-03-13 12:09:32,324: INFO: 1199617285: PyTorch datasets created successfully.]\n",
      "[2025-03-13 12:09:33,547: INFO: 2686220794: Datasets saved to artifacts\\feature_engineering\\datasets]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    get_data_pre_config = config.get_data_preprocessing_config()\n",
    "    data_pre_process = DataPreprocessing(config=get_data_pre_config)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    data_pre_process.text_process()\n",
    "    data_pre_process.mapping_labels_func()\n",
    "    data_pre_process.tokenize_text()\n",
    "    \n",
    "    # Save the preprocessed data\n",
    "    data_pre_process.save_data()  # No need to pass the path\n",
    "    \n",
    "    # Split the data into train, validation, and test sets\n",
    "    splits = data_pre_process.train_val_test_split()\n",
    "    \n",
    "    # Convert splits to PyTorch datasets\n",
    "    tokenized_datasets = data_pre_process.convert_to_tokenized_datasets(splits)\n",
    "    \n",
    "    # Access the PyTorch datasets\n",
    "    train_dataset = tokenized_datasets['train']\n",
    "    val_dataset = tokenized_datasets['val']\n",
    "    test_dataset = tokenized_datasets['test']\n",
    "    logger.info(\"PyTorch datasets created successfully.\")\n",
    "    datasets = data_pre_process.save_datasets(train_dataset, val_dataset, test_dataset)\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during data preprocessing: {e}\")\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
