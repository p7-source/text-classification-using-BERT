{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "02_feature_engineering_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective:\n",
    "    1. Reading the file from the loacation\n",
    "    2. Doing data preprcessing like extracting relevant features, maping values.\n",
    "    3. Let's do 1 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature_engineering: \\\n",
    "  root_dir: artifacts/feature_engineering  \\\n",
    "  training_data_path: artifacts/feature_engineering  \\\n",
    "  training_data_file: artifacts/feature_engineering/tweets.cs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataPreprocessingConfig:\n",
    "    root_dir: Path\n",
    "    training_data_path: Path\n",
    "    training_data_file: str\n",
    "    training_cleansed_data: str\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT\\\\research'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CONFIG_FILE_PATH = Path(\"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT/config/config.yaml\")\n",
    "PARAMS_FILE_PATH = Path(\"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT/params.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.textClassifier.constants import *\n",
    "from src.textClassifier.utils.commons import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "\n",
    "    def get_data_preprocessing_config(self) -> DataPreprocessingConfig:\n",
    "        config = self.config.feature_engineering\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_preprocessing_config = DataPreprocessingConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            training_data_path=config.training_data_path,\n",
    "            training_data_file=config.training_data_file,\n",
    "            training_cleansed_data=config.training_cleansed_data\n",
    "        )\n",
    "\n",
    "        return data_preprocessing_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textClassifier import logger\n",
    "# import zipfile\n",
    "# import os\n",
    "# from textClassifier.utils.commons import get_size\n",
    "# import gdown\n",
    "# import spacy\n",
    "# from nltk.corpus import stopwords\n",
    "# import string\n",
    "\n",
    "\n",
    "\n",
    "# class DataPreprocessing:\n",
    "#     def __init__(self, config: DataPreprocessingConfig):\n",
    "#         self.config = config\n",
    "\n",
    "#     def preprocess_text(self):\n",
    "#         nlp = spacy.load('en_core_web_sm')\n",
    "#         stop_words = set(stopwords.words('english'))\n",
    "#         text = text.lower() # Convert to lowercase\n",
    "\n",
    "#         # Remove punctuation\n",
    "#         text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "#         # Tokenize and lemmatize\n",
    "#         doc = nlp(text)\n",
    "#         tokens = [token.lemma_ for token in doc if token.text not in stop_words]\n",
    "#         return ' '.join(tokens)\n",
    "\n",
    "#     def pass_data_to_text_process(self):\n",
    "#         # self.config.training_data_file\n",
    "#         data['cleaned_text'] = data['text'].apply(preprocess_text)\n",
    "#         return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp = DataPreprocessing(config_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\artifacts\\data_ingestion\\Tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi dude happy'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dp.preprocess_text('hi dude Happy was how are you doing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textClassifier import logger\n",
    "import zipfile\n",
    "import os\n",
    "from textClassifier.utils.commons import get_size\n",
    "import gdown\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class DataPreprocessing:\n",
    "    def __init__(self, config: DataPreprocessingConfig):\n",
    "        self.config = config\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "    def text_process(self, text: str) -> str:\n",
    "        text = text.lower()\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        doc = self.nlp(text)\n",
    "        tokens = [token.lemma_ for token in doc if token.text not in self.stop_words]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def get_data_from_features(self) -> pd.DataFrame:\n",
    "        data = pd.read_csv(self.config.training_data_file)\n",
    "        data['cleaned_text'] = data['text'].apply(self.text_process)\n",
    "        return data\n",
    "    \n",
    "    def save_data(self, data: pd.DataFrame, path_name: str='cleaned_tweets.csv') -> None:\n",
    "        out_path_name = Path(self.config.training_cleansed_data)\n",
    "        data.to_csv(out_path_name, index=False)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textClassifier import logger\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DataPreprocessing:\n",
    "    def __init__(self, config: DataPreprocessingConfig):\n",
    "        self.config = config\n",
    "        self.nlp = spacy.load('en_core_web_sm')  # Load spaCy model\n",
    "        self.stop_words = set(stopwords.words('english'))  # Load stopwords\n",
    "        self.mapping_labels = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def text_process(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocesses the input text by:\n",
    "        1. Converting to lowercase\n",
    "        2. Removing punctuation\n",
    "        3. Tokenizing and lemmatizing\n",
    "        4. Removing stopwords\n",
    "        \"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # Tokenize and lemmatize\n",
    "        doc = self.nlp(text)\n",
    "        tokens = [token.lemma_ for token in doc if token.text not in self.stop_words]\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def get_data_from_features(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads the dataset from the path specified in the config and applies preprocessing to the text column.\n",
    "        \"\"\"\n",
    "        # Check if the file exists\n",
    "        if not Path(self.config.training_data_file).exists():\n",
    "            raise FileNotFoundError(f\"File not found: {self.config.training_data_file}\")\n",
    "\n",
    "        # Load the dataset\n",
    "        data = pd.read_csv(self.config.training_data_file)\n",
    "\n",
    "        # Apply preprocessing to the text column\n",
    "        data['cleaned_text'] = data['text'].apply(self.text_process)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def save_data(self, data: pd.DataFrame, path_name: str = 'cleaned_tweets.csv') -> None:\n",
    "        \"\"\"\n",
    "        Saves the preprocessed data to the specified output file.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): The preprocessed DataFrame to save.\n",
    "            path_name (str): The name of the output file. Defaults to \"cleaned_tweets.csv\".\n",
    "        \"\"\"\n",
    "        # Create the full output file path\n",
    "        out_path_name = Path(self.config.training_cleansed_data) / path_name\n",
    "\n",
    "        # Save the DataFrame to CSV\n",
    "        data.to_csv(out_path_name, index=False)\n",
    "        logger.info(f\"Preprocessed data saved to {out_path_name}\")\n",
    "\n",
    "\n",
    "    def mapping_labels_func(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "        data['label'] = data['airline_sentiment'].map(self.mapping_labels)\n",
    "        logger.info(f\"values are mapped in the {data}\")\n",
    "        return data\n",
    "    \n",
    "    # def tokenizing_func(self, data: pd.DataFrame, text_column: str = 'cleaned_text') -> dict:\n",
    "    #     # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    #     # return tokenizer(data, padding='max_length', truncation=True, max_length=128)\n",
    "    \n",
    "    #     tokenized_inputs = self.tokenizer(\n",
    "    #         data[text_column].tolist(),  # Convert text column to list\n",
    "    #         padding='max_length',        # Pad to max length\n",
    "    #         truncation=True,             # Truncate to max length\n",
    "    #         max_length=128,              # Set max length\n",
    "    #         return_tensors='pt'          # Return PyTorch tensors\n",
    "    #     )\n",
    "    #     return tokenized_inputs\n",
    "\n",
    "    def tokenize_text(self, data: pd.DataFrame, text_column: str = 'cleaned_text') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Tokenizes the text in the specified column using BERT tokenizer.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): The DataFrame containing the text to tokenize.\n",
    "            text_column (str): The name of the column containing the text. Defaults to 'cleaned_text'.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The DataFrame with an additional 'tokenized' column.\n",
    "        \"\"\"\n",
    "        # Tokenize the text\n",
    "        data['tokenized'] = data[text_column].apply(\n",
    "            lambda x: self.tokenizer(x, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "        )\n",
    "        logger.info(f\"Text in column '{text_column}' has been tokenized.\")\n",
    "        return data\n",
    "    \n",
    "    def train_val_test_split(self, data: pd.DataFrame, test_size: float = 0.3, val_size: float = 0.5, random_state: int = 42):\n",
    "        \"\"\"\n",
    "        Splits the dataset into training, validation, and test sets.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): The DataFrame containing the tokenized data and labels.\n",
    "            test_size (float): Proportion of the dataset to include in the test split. Defaults to 0.3.\n",
    "            val_size (float): Proportion of the temp split to include in the validation split. Defaults to 0.5.\n",
    "            random_state (int): Random seed for reproducibility. Defaults to 42.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing the splits:\n",
    "            {\n",
    "                'train': {'texts': train_texts, 'labels': train_labels},\n",
    "                'val': {'texts': val_texts, 'labels': val_labels},\n",
    "                'test': {'texts': test_texts, 'labels': test_labels}\n",
    "            }\n",
    "        \"\"\"\n",
    "        # Split into train and temp (val + test)\n",
    "        train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "            data['tokenized'].to_list(), data['label'].to_list(), test_size=test_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "        # Split temp into validation and test\n",
    "        val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "            temp_texts, temp_labels, test_size=val_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Data split into train, validation, and test sets.\")\n",
    "        return {\n",
    "            'train': {'texts': train_texts, 'labels': train_labels},\n",
    "            'val': {'texts': val_texts, 'labels': val_labels},\n",
    "            'test': {'texts': test_texts, 'labels': test_labels}\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-14 15:32:52,577: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\config\\config.yaml loaded successfully]\n",
      "[2025-02-14 15:32:52,590: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\params.yaml loaded successfully]\n",
      "[2025-02-14 15:32:52,593: INFO: commons: created directory at: artifacts]\n",
      "[2025-02-14 15:32:52,600: INFO: commons: created directory at: artifacts/feature_engineering]\n",
      "[2025-02-14 15:34:45,125: INFO: 2396406771: Preprocessed data saved to artifacts\\feature_engineering\\cleaned_tweets.csv]\n",
      "[2025-02-14 15:34:45,136: INFO: 2396406771: values are mapped in the                  tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
      "0      570306133677760513           neutral                        1.0000   \n",
      "1      570301130888122368          positive                        0.3486   \n",
      "2      570301083672813571           neutral                        0.6837   \n",
      "3      570301031407624196          negative                        1.0000   \n",
      "4      570300817074462722          negative                        1.0000   \n",
      "...                   ...               ...                           ...   \n",
      "14635  569587686496825344          positive                        0.3487   \n",
      "14636  569587371693355008          negative                        1.0000   \n",
      "14637  569587242672398336           neutral                        1.0000   \n",
      "14638  569587188687634433          negative                        1.0000   \n",
      "14639  569587140490866689           neutral                        0.6771   \n",
      "\n",
      "               negativereason  negativereason_confidence         airline  \\\n",
      "0                         NaN                        NaN  Virgin America   \n",
      "1                         NaN                     0.0000  Virgin America   \n",
      "2                         NaN                        NaN  Virgin America   \n",
      "3                  Bad Flight                     0.7033  Virgin America   \n",
      "4                  Can't Tell                     1.0000  Virgin America   \n",
      "...                       ...                        ...             ...   \n",
      "14635                     NaN                     0.0000        American   \n",
      "14636  Customer Service Issue                     1.0000        American   \n",
      "14637                     NaN                        NaN        American   \n",
      "14638  Customer Service Issue                     0.6659        American   \n",
      "14639                     NaN                     0.0000        American   \n",
      "\n",
      "      airline_sentiment_gold             name negativereason_gold  \\\n",
      "0                        NaN          cairdin                 NaN   \n",
      "1                        NaN         jnardino                 NaN   \n",
      "2                        NaN       yvonnalynn                 NaN   \n",
      "3                        NaN         jnardino                 NaN   \n",
      "4                        NaN         jnardino                 NaN   \n",
      "...                      ...              ...                 ...   \n",
      "14635                    NaN  KristenReenders                 NaN   \n",
      "14636                    NaN         itsropes                 NaN   \n",
      "14637                    NaN         sanyabun                 NaN   \n",
      "14638                    NaN       SraJackson                 NaN   \n",
      "14639                    NaN        daviddtwu                 NaN   \n",
      "\n",
      "       retweet_count                                               text  \\\n",
      "0                  0                @VirginAmerica What @dhepburn said.   \n",
      "1                  0  @VirginAmerica plus you've added commercials t...   \n",
      "2                  0  @VirginAmerica I didn't today... Must mean I n...   \n",
      "3                  0  @VirginAmerica it's really aggressive to blast...   \n",
      "4                  0  @VirginAmerica and it's a really big bad thing...   \n",
      "...              ...                                                ...   \n",
      "14635              0  @AmericanAir thank you we got on a different f...   \n",
      "14636              0  @AmericanAir leaving over 20 minutes Late Flig...   \n",
      "14637              0  @AmericanAir Please bring American Airlines to...   \n",
      "14638              0  @AmericanAir you have my money, you change my ...   \n",
      "14639              0  @AmericanAir we have 8 ppl so we need 2 know h...   \n",
      "\n",
      "      tweet_coord              tweet_created tweet_location  \\\n",
      "0             NaN  2015-02-24 11:35:52 -0800            NaN   \n",
      "1             NaN  2015-02-24 11:15:59 -0800            NaN   \n",
      "2             NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
      "3             NaN  2015-02-24 11:15:36 -0800            NaN   \n",
      "4             NaN  2015-02-24 11:14:45 -0800            NaN   \n",
      "...           ...                        ...            ...   \n",
      "14635         NaN  2015-02-22 12:01:01 -0800            NaN   \n",
      "14636         NaN  2015-02-22 11:59:46 -0800          Texas   \n",
      "14637         NaN  2015-02-22 11:59:15 -0800  Nigeria,lagos   \n",
      "14638         NaN  2015-02-22 11:59:02 -0800     New Jersey   \n",
      "14639         NaN  2015-02-22 11:58:51 -0800     dallas, TX   \n",
      "\n",
      "                    user_timezone  \\\n",
      "0      Eastern Time (US & Canada)   \n",
      "1      Pacific Time (US & Canada)   \n",
      "2      Central Time (US & Canada)   \n",
      "3      Pacific Time (US & Canada)   \n",
      "4      Pacific Time (US & Canada)   \n",
      "...                           ...   \n",
      "14635                         NaN   \n",
      "14636                         NaN   \n",
      "14637                         NaN   \n",
      "14638  Eastern Time (US & Canada)   \n",
      "14639                         NaN   \n",
      "\n",
      "                                            cleaned_text  label  \n",
      "0                             virginamerica dhepburn say      1  \n",
      "1      virginamerica plus add commercial experience t...      2  \n",
      "2      virginamerica not today must mean need take an...      1  \n",
      "3      virginamerica really aggressive blast obnoxiou...      0  \n",
      "4                     virginamerica really big bad thing      0  \n",
      "...                                                  ...    ...  \n",
      "14635     americanair thank get different flight chicago      2  \n",
      "14636  americanair leave 20 minute late flight warnin...      0  \n",
      "14637  americanair please bring american airline blac...      1  \n",
      "14638  americanair money change flight not answer pho...      0  \n",
      "14639  americanair 8 ppl need 2 know many seat next f...      1  \n",
      "\n",
      "[14640 rows x 17 columns]]\n",
      "[2025-02-14 15:34:54,168: INFO: 2396406771: Text in column 'cleaned_text' has been tokenized.]\n",
      "[2025-02-14 15:34:54,187: INFO: 2396406771: Data split into train, validation, and test sets.]\n",
      "Train set size: 10248\n",
      "Validation set size: 2196\n",
      "Test set size: 2196\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming ConfigurationManager is defined elsewhere\n",
    "    config_manager = ConfigurationManager()\n",
    "\n",
    "    # Get the data preprocessing config\n",
    "    get_data_pre_config = config_manager.get_data_preprocessing_config()\n",
    "\n",
    "    # Initialize DataPreprocessing with the config\n",
    "    data_processing = DataPreprocessing(config=get_data_pre_config)\n",
    "\n",
    "    # Load and preprocess the data\n",
    "    output = data_processing.get_data_from_features()\n",
    "\n",
    "    # Save the preprocessed data\n",
    "    data_processing.save_data(data=output, path_name='cleaned_tweets.csv')\n",
    "\n",
    "    # Map labels to numerical values\n",
    "    output = data_processing.mapping_labels_func(data=output)\n",
    "\n",
    "    # Tokenize the cleaned text\n",
    "    output = data_processing.tokenize_text(data=output)\n",
    "\n",
    "    # Split the data into train, validation, and test sets\n",
    "    splits = data_processing.train_val_test_split(data=output)\n",
    "\n",
    "    # Access the splits\n",
    "    train_data = splits['train']\n",
    "    val_data = splits['val']\n",
    "    test_data = splits['test']\n",
    "\n",
    "    # Display the sizes of the splits\n",
    "    print(f\"Train set size: {len(train_data['texts'])}\")\n",
    "    print(f\"Validation set size: {len(val_data['texts'])}\")\n",
    "    print(f\"Test set size: {len(test_data['texts'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "def convert_to_dicts(tokenized_texts):\n",
    "    \"\"\"\n",
    "    Converts a list of tokenized texts into a dictionary of input IDs and attention masks.\n",
    "\n",
    "    Args:\n",
    "        tokenized_texts (list): List of tokenized texts (each is a dictionary with 'input_ids' and 'attention_mask').\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing 'input_ids' and 'attention_mask' as lists.\n",
    "    \"\"\"\n",
    "    input_ids = [d['input_ids'].squeeze(0) for d in tokenized_texts]  # Remove batch dimension\n",
    "    attention_masks = [d['attention_mask'].squeeze(0) for d in tokenized_texts]  # Remove batch dimension\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_masks}\n",
    "\n",
    "\n",
    "def create_datasets(train_texts, train_labels, val_texts, val_labels, test_texts, test_labels):\n",
    "    \"\"\"\n",
    "    Creates SentimentDataset objects for train, validation, and test splits.\n",
    "\n",
    "    Args:\n",
    "        train_texts (list): List of tokenized texts for the training set.\n",
    "        train_labels (list): List of labels for the training set.\n",
    "        val_texts (list): List of tokenized texts for the validation set.\n",
    "        val_labels (list): List of labels for the validation set.\n",
    "        test_texts (list): List of tokenized texts for the test set.\n",
    "        test_labels (list): List of labels for the test set.\n",
    "\n",
    "    Returns:\n",
    "        train_dataset, val_dataset, test_dataset: SentimentDataset objects for each split.\n",
    "    \"\"\"\n",
    "    # Convert tokenized texts to encodings\n",
    "    train_encodings = convert_to_dicts(train_texts)\n",
    "    val_encodings = convert_to_dicts(val_texts)\n",
    "    test_encodings = convert_to_dicts(test_texts)\n",
    "\n",
    "    # Create SentimentDataset objects\n",
    "    train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "    val_dataset = SentimentDataset(val_encodings, val_labels)\n",
    "    test_dataset = SentimentDataset(test_encodings, test_labels)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-14 15:50:20,078: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\config\\config.yaml loaded successfully]\n",
      "[2025-02-14 15:50:20,082: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\params.yaml loaded successfully]\n",
      "[2025-02-14 15:50:20,084: INFO: commons: created directory at: artifacts]\n",
      "[2025-02-14 15:50:20,086: INFO: commons: created directory at: artifacts/feature_engineering]\n",
      "[2025-02-14 15:52:16,736: INFO: 2396406771: Preprocessed data saved to artifacts\\feature_engineering\\cleaned_tweets.csv]\n",
      "[2025-02-14 15:52:16,745: INFO: 2396406771: values are mapped in the                  tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
      "0      570306133677760513           neutral                        1.0000   \n",
      "1      570301130888122368          positive                        0.3486   \n",
      "2      570301083672813571           neutral                        0.6837   \n",
      "3      570301031407624196          negative                        1.0000   \n",
      "4      570300817074462722          negative                        1.0000   \n",
      "...                   ...               ...                           ...   \n",
      "14635  569587686496825344          positive                        0.3487   \n",
      "14636  569587371693355008          negative                        1.0000   \n",
      "14637  569587242672398336           neutral                        1.0000   \n",
      "14638  569587188687634433          negative                        1.0000   \n",
      "14639  569587140490866689           neutral                        0.6771   \n",
      "\n",
      "               negativereason  negativereason_confidence         airline  \\\n",
      "0                         NaN                        NaN  Virgin America   \n",
      "1                         NaN                     0.0000  Virgin America   \n",
      "2                         NaN                        NaN  Virgin America   \n",
      "3                  Bad Flight                     0.7033  Virgin America   \n",
      "4                  Can't Tell                     1.0000  Virgin America   \n",
      "...                       ...                        ...             ...   \n",
      "14635                     NaN                     0.0000        American   \n",
      "14636  Customer Service Issue                     1.0000        American   \n",
      "14637                     NaN                        NaN        American   \n",
      "14638  Customer Service Issue                     0.6659        American   \n",
      "14639                     NaN                     0.0000        American   \n",
      "\n",
      "      airline_sentiment_gold             name negativereason_gold  \\\n",
      "0                        NaN          cairdin                 NaN   \n",
      "1                        NaN         jnardino                 NaN   \n",
      "2                        NaN       yvonnalynn                 NaN   \n",
      "3                        NaN         jnardino                 NaN   \n",
      "4                        NaN         jnardino                 NaN   \n",
      "...                      ...              ...                 ...   \n",
      "14635                    NaN  KristenReenders                 NaN   \n",
      "14636                    NaN         itsropes                 NaN   \n",
      "14637                    NaN         sanyabun                 NaN   \n",
      "14638                    NaN       SraJackson                 NaN   \n",
      "14639                    NaN        daviddtwu                 NaN   \n",
      "\n",
      "       retweet_count                                               text  \\\n",
      "0                  0                @VirginAmerica What @dhepburn said.   \n",
      "1                  0  @VirginAmerica plus you've added commercials t...   \n",
      "2                  0  @VirginAmerica I didn't today... Must mean I n...   \n",
      "3                  0  @VirginAmerica it's really aggressive to blast...   \n",
      "4                  0  @VirginAmerica and it's a really big bad thing...   \n",
      "...              ...                                                ...   \n",
      "14635              0  @AmericanAir thank you we got on a different f...   \n",
      "14636              0  @AmericanAir leaving over 20 minutes Late Flig...   \n",
      "14637              0  @AmericanAir Please bring American Airlines to...   \n",
      "14638              0  @AmericanAir you have my money, you change my ...   \n",
      "14639              0  @AmericanAir we have 8 ppl so we need 2 know h...   \n",
      "\n",
      "      tweet_coord              tweet_created tweet_location  \\\n",
      "0             NaN  2015-02-24 11:35:52 -0800            NaN   \n",
      "1             NaN  2015-02-24 11:15:59 -0800            NaN   \n",
      "2             NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
      "3             NaN  2015-02-24 11:15:36 -0800            NaN   \n",
      "4             NaN  2015-02-24 11:14:45 -0800            NaN   \n",
      "...           ...                        ...            ...   \n",
      "14635         NaN  2015-02-22 12:01:01 -0800            NaN   \n",
      "14636         NaN  2015-02-22 11:59:46 -0800          Texas   \n",
      "14637         NaN  2015-02-22 11:59:15 -0800  Nigeria,lagos   \n",
      "14638         NaN  2015-02-22 11:59:02 -0800     New Jersey   \n",
      "14639         NaN  2015-02-22 11:58:51 -0800     dallas, TX   \n",
      "\n",
      "                    user_timezone  \\\n",
      "0      Eastern Time (US & Canada)   \n",
      "1      Pacific Time (US & Canada)   \n",
      "2      Central Time (US & Canada)   \n",
      "3      Pacific Time (US & Canada)   \n",
      "4      Pacific Time (US & Canada)   \n",
      "...                           ...   \n",
      "14635                         NaN   \n",
      "14636                         NaN   \n",
      "14637                         NaN   \n",
      "14638  Eastern Time (US & Canada)   \n",
      "14639                         NaN   \n",
      "\n",
      "                                            cleaned_text  label  \n",
      "0                             virginamerica dhepburn say      1  \n",
      "1      virginamerica plus add commercial experience t...      2  \n",
      "2      virginamerica not today must mean need take an...      1  \n",
      "3      virginamerica really aggressive blast obnoxiou...      0  \n",
      "4                     virginamerica really big bad thing      0  \n",
      "...                                                  ...    ...  \n",
      "14635     americanair thank get different flight chicago      2  \n",
      "14636  americanair leave 20 minute late flight warnin...      0  \n",
      "14637  americanair please bring american airline blac...      1  \n",
      "14638  americanair money change flight not answer pho...      0  \n",
      "14639  americanair 8 ppl need 2 know many seat next f...      1  \n",
      "\n",
      "[14640 rows x 17 columns]]\n",
      "[2025-02-14 15:52:25,797: INFO: 2396406771: Text in column 'cleaned_text' has been tokenized.]\n",
      "[2025-02-14 15:52:25,805: INFO: 2396406771: Data split into train, validation, and test sets.]\n",
      "{'input_ids': tensor([  101,  6892, 16558,  5657,  3462,  3770,  2620,  2620,  1055,  2595,\n",
      "        24798,  2102,  3501, 24316,  2051,  3902,  2681,  3309, 13114,  2570,\n",
      "         3742,  6724,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(1)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prass\\AppData\\Local\\Temp\\ipykernel_17580\\2803380171.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming ConfigurationManager is defined elsewhere\n",
    "    config_manager = ConfigurationManager()\n",
    "\n",
    "    # Get the data preprocessing config\n",
    "    get_data_pre_config = config_manager.get_data_preprocessing_config()\n",
    "\n",
    "    # Initialize DataPreprocessing with the config\n",
    "    data_processing = DataPreprocessing(config=get_data_pre_config)\n",
    "\n",
    "    # Load and preprocess the data\n",
    "    output = data_processing.get_data_from_features()\n",
    "\n",
    "    # Save the preprocessed data\n",
    "    data_processing.save_data(data=output, path_name='cleaned_tweets.csv')\n",
    "\n",
    "    # Map labels to numerical values\n",
    "    output = data_processing.mapping_labels_func(data=output)\n",
    "\n",
    "    # Tokenize the cleaned text\n",
    "    output = data_processing.tokenize_text(data=output)\n",
    "\n",
    "    # Split the data into train, validation, and test sets\n",
    "    splits = data_processing.train_val_test_split(data=output)\n",
    "\n",
    "    # Access the splits\n",
    "    train_data = splits['train']\n",
    "    val_data = splits['val']\n",
    "    test_data = splits['test']\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset, val_dataset, test_dataset = create_datasets(\n",
    "        train_data['texts'], train_data['labels'],\n",
    "        val_data['texts'], val_data['labels'],\n",
    "        test_data['texts'], test_data['labels']\n",
    "    )\n",
    "\n",
    "    # Example of accessing data\n",
    "    print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map labels to integers\n",
    "# label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "# data['label'] = data['airline_sentiment'].map(label_mapping)\n",
    "\n",
    "# from transformers import BertTokenizer\n",
    "\n",
    "# # Tokenization\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Tokenize the data\n",
    "# def tokenize_function(text):\n",
    "#     return tokenizer(text, padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "# data['tokenized'] = data['cleaned_text'].apply(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-12 15:56:11,364: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\config\\config.yaml loaded successfully]\n",
      "[2025-02-12 15:56:11,368: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\params.yaml loaded successfully]\n",
      "[2025-02-12 15:56:11,370: INFO: commons: created directory at: artifacts]\n",
      "[2025-02-12 15:56:11,373: INFO: commons: created directory at: artifacts/feature_engineering]\n",
      "[2025-02-12 15:58:20,522: INFO: 751830446: Preprocessed data saved to artifacts\\feature_engineering\\cleaned_tweets.csv]\n",
      "[2025-02-12 15:58:20,533: INFO: 751830446: values are mapped in the                  tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
      "0      570306133677760513           neutral                        1.0000   \n",
      "1      570301130888122368          positive                        0.3486   \n",
      "2      570301083672813571           neutral                        0.6837   \n",
      "3      570301031407624196          negative                        1.0000   \n",
      "4      570300817074462722          negative                        1.0000   \n",
      "...                   ...               ...                           ...   \n",
      "14635  569587686496825344          positive                        0.3487   \n",
      "14636  569587371693355008          negative                        1.0000   \n",
      "14637  569587242672398336           neutral                        1.0000   \n",
      "14638  569587188687634433          negative                        1.0000   \n",
      "14639  569587140490866689           neutral                        0.6771   \n",
      "\n",
      "               negativereason  negativereason_confidence         airline  \\\n",
      "0                         NaN                        NaN  Virgin America   \n",
      "1                         NaN                     0.0000  Virgin America   \n",
      "2                         NaN                        NaN  Virgin America   \n",
      "3                  Bad Flight                     0.7033  Virgin America   \n",
      "4                  Can't Tell                     1.0000  Virgin America   \n",
      "...                       ...                        ...             ...   \n",
      "14635                     NaN                     0.0000        American   \n",
      "14636  Customer Service Issue                     1.0000        American   \n",
      "14637                     NaN                        NaN        American   \n",
      "14638  Customer Service Issue                     0.6659        American   \n",
      "14639                     NaN                     0.0000        American   \n",
      "\n",
      "      airline_sentiment_gold             name negativereason_gold  \\\n",
      "0                        NaN          cairdin                 NaN   \n",
      "1                        NaN         jnardino                 NaN   \n",
      "2                        NaN       yvonnalynn                 NaN   \n",
      "3                        NaN         jnardino                 NaN   \n",
      "4                        NaN         jnardino                 NaN   \n",
      "...                      ...              ...                 ...   \n",
      "14635                    NaN  KristenReenders                 NaN   \n",
      "14636                    NaN         itsropes                 NaN   \n",
      "14637                    NaN         sanyabun                 NaN   \n",
      "14638                    NaN       SraJackson                 NaN   \n",
      "14639                    NaN        daviddtwu                 NaN   \n",
      "\n",
      "       retweet_count                                               text  \\\n",
      "0                  0                @VirginAmerica What @dhepburn said.   \n",
      "1                  0  @VirginAmerica plus you've added commercials t...   \n",
      "2                  0  @VirginAmerica I didn't today... Must mean I n...   \n",
      "3                  0  @VirginAmerica it's really aggressive to blast...   \n",
      "4                  0  @VirginAmerica and it's a really big bad thing...   \n",
      "...              ...                                                ...   \n",
      "14635              0  @AmericanAir thank you we got on a different f...   \n",
      "14636              0  @AmericanAir leaving over 20 minutes Late Flig...   \n",
      "14637              0  @AmericanAir Please bring American Airlines to...   \n",
      "14638              0  @AmericanAir you have my money, you change my ...   \n",
      "14639              0  @AmericanAir we have 8 ppl so we need 2 know h...   \n",
      "\n",
      "      tweet_coord              tweet_created tweet_location  \\\n",
      "0             NaN  2015-02-24 11:35:52 -0800            NaN   \n",
      "1             NaN  2015-02-24 11:15:59 -0800            NaN   \n",
      "2             NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
      "3             NaN  2015-02-24 11:15:36 -0800            NaN   \n",
      "4             NaN  2015-02-24 11:14:45 -0800            NaN   \n",
      "...           ...                        ...            ...   \n",
      "14635         NaN  2015-02-22 12:01:01 -0800            NaN   \n",
      "14636         NaN  2015-02-22 11:59:46 -0800          Texas   \n",
      "14637         NaN  2015-02-22 11:59:15 -0800  Nigeria,lagos   \n",
      "14638         NaN  2015-02-22 11:59:02 -0800     New Jersey   \n",
      "14639         NaN  2015-02-22 11:58:51 -0800     dallas, TX   \n",
      "\n",
      "                    user_timezone  \\\n",
      "0      Eastern Time (US & Canada)   \n",
      "1      Pacific Time (US & Canada)   \n",
      "2      Central Time (US & Canada)   \n",
      "3      Pacific Time (US & Canada)   \n",
      "4      Pacific Time (US & Canada)   \n",
      "...                           ...   \n",
      "14635                         NaN   \n",
      "14636                         NaN   \n",
      "14637                         NaN   \n",
      "14638  Eastern Time (US & Canada)   \n",
      "14639                         NaN   \n",
      "\n",
      "                                            cleaned_text  label  \n",
      "0                             virginamerica dhepburn say      1  \n",
      "1      virginamerica plus add commercial experience t...      2  \n",
      "2      virginamerica not today must mean need take an...      1  \n",
      "3      virginamerica really aggressive blast obnoxiou...      0  \n",
      "4                     virginamerica really big bad thing      0  \n",
      "...                                                  ...    ...  \n",
      "14635     americanair thank get different flight chicago      2  \n",
      "14636  americanair leave 20 minute late flight warnin...      0  \n",
      "14637  americanair please bring american airline blac...      1  \n",
      "14638  americanair money change flight not answer pho...      0  \n",
      "14639  americanair 8 ppl need 2 know many seat next f...      1  \n",
      "\n",
      "[14640 rows x 17 columns]]\n",
      "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
      "0  570306133677760513           neutral                        1.0000   \n",
      "1  570301130888122368          positive                        0.3486   \n",
      "2  570301083672813571           neutral                        0.6837   \n",
      "3  570301031407624196          negative                        1.0000   \n",
      "4  570300817074462722          negative                        1.0000   \n",
      "\n",
      "  negativereason  negativereason_confidence         airline  \\\n",
      "0            NaN                        NaN  Virgin America   \n",
      "1            NaN                     0.0000  Virgin America   \n",
      "2            NaN                        NaN  Virgin America   \n",
      "3     Bad Flight                     0.7033  Virgin America   \n",
      "4     Can't Tell                     1.0000  Virgin America   \n",
      "\n",
      "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
      "0                    NaN     cairdin                 NaN              0   \n",
      "1                    NaN    jnardino                 NaN              0   \n",
      "2                    NaN  yvonnalynn                 NaN              0   \n",
      "3                    NaN    jnardino                 NaN              0   \n",
      "4                    NaN    jnardino                 NaN              0   \n",
      "\n",
      "                                                text tweet_coord  \\\n",
      "0                @VirginAmerica What @dhepburn said.         NaN   \n",
      "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
      "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
      "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
      "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
      "\n",
      "               tweet_created tweet_location               user_timezone  \\\n",
      "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)   \n",
      "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)   \n",
      "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)   \n",
      "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)   \n",
      "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)   \n",
      "\n",
      "                                        cleaned_text  label  \n",
      "0                         virginamerica dhepburn say      1  \n",
      "1  virginamerica plus add commercial experience t...      2  \n",
      "2  virginamerica not today must mean need take an...      1  \n",
      "3  virginamerica really aggressive blast obnoxiou...      0  \n",
      "4                 virginamerica really big bad thing      0  \n",
      "{'input_ids': tensor([[  101,  6261, 14074, 14735, 28144, 13699,  8022,  2360,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  6261, 14074, 14735,  4606,  5587,  3293,  3325, 26997,  2100,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  6261, 14074, 14735,  2025,  2651,  2442,  2812,  2342,  2202,\n",
      "          2178,  4440,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  6261, 14074, 14735,  2428,  9376,  8479, 27885,  3630, 25171,\n",
      "          4024,  4113,  2227, 23713,  2210, 28667, 22957,  2063,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  6261, 14074, 14735,  2428,  2502,  2919,  2518,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "config_manager = ConfigurationManager()\n",
    "\n",
    "# Get the data preprocessing config\n",
    "get_data_pre_config = config_manager.get_data_preprocessing_config()\n",
    "\n",
    "# Initialize DataPreprocessing with the config\n",
    "data_processing = DataPreprocessing(config=get_data_pre_config)\n",
    "\n",
    "# Load and preprocess the data\n",
    "output = data_processing.get_data_from_features()\n",
    "\n",
    "# Save the preprocessed data\n",
    "data_processing.save_data(data=output, path_name='cleaned_tweets.csv')\n",
    "\n",
    "# mapping = data_processing.mapping_labels_func()\n",
    "output = data_processing.mapping_labels_func(data = output)\n",
    "print(output.head())\n",
    "tokenized_data = data_processing.tokenizing_func(data=output)\n",
    "print(tokenized_data[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def train_test_split_func(tokenized_data, labels, test_size=0.3, val_size=0.5, random_state=42):\n",
    "    # Split into train and temp (val + test)\n",
    "    train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "        tokenized_data['input_ids'], labels, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Split temp into validation and test\n",
    "    val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "            temp_texts, temp_labels, test_size=val_size, random_state=random_state\n",
    "        )\n",
    "        \n",
    "    return train_texts, val_texts, test_texts, train_labels, val_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
