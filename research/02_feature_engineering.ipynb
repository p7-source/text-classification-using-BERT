{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "02_feature_engineering_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective:\n",
    "    1. Reading the file from the loacation\n",
    "    2. Doing data preprcessing like extracting relevant features, maping values.\n",
    "    3. Let's do 1 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature_engineering: \\\n",
    "  root_dir: artifacts/feature_engineering  \\\n",
    "  training_data_path: artifacts/feature_engineering  \\\n",
    "  training_data_file: artifacts/feature_engineering/tweets.cs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataPreprocessingConfig:\n",
    "    root_dir: Path\n",
    "    training_data_path: Path\n",
    "    training_data_file: str\n",
    "    training_cleansed_data: str\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CONFIG_FILE_PATH = Path(\"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT/config/config.yaml\")\n",
    "PARAMS_FILE_PATH = Path(\"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT/params.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.textClassifier.constants import *\n",
    "from src.textClassifier.utils.commons import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "\n",
    "    def get_data_preprocessing_config(self) -> DataPreprocessingConfig:\n",
    "        config = self.config.feature_engineering\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_preprocessing_config = DataPreprocessingConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            training_data_path=config.training_data_path,\n",
    "            training_data_file=config.training_data_file,\n",
    "            training_cleansed_data=config.training_cleansed_data\n",
    "        )\n",
    "\n",
    "        return data_preprocessing_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textClassifier import logger\n",
    "# import zipfile\n",
    "# import os\n",
    "# from textClassifier.utils.commons import get_size\n",
    "# import gdown\n",
    "# import spacy\n",
    "# from nltk.corpus import stopwords\n",
    "# import string\n",
    "\n",
    "\n",
    "\n",
    "# class DataPreprocessing:\n",
    "#     def __init__(self, config: DataPreprocessingConfig):\n",
    "#         self.config = config\n",
    "\n",
    "#     def preprocess_text(self):\n",
    "#         nlp = spacy.load('en_core_web_sm')\n",
    "#         stop_words = set(stopwords.words('english'))\n",
    "#         text = text.lower() # Convert to lowercase\n",
    "\n",
    "#         # Remove punctuation\n",
    "#         text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "#         # Tokenize and lemmatize\n",
    "#         doc = nlp(text)\n",
    "#         tokens = [token.lemma_ for token in doc if token.text not in stop_words]\n",
    "#         return ' '.join(tokens)\n",
    "\n",
    "#     def pass_data_to_text_process(self):\n",
    "#         # self.config.training_data_file\n",
    "#         data['cleaned_text'] = data['text'].apply(preprocess_text)\n",
    "#         return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp = DataPreprocessing(config_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\artifacts\\data_ingestion\\Tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi dude happy'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dp.preprocess_text('hi dude Happy was how are you doing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textClassifier import logger\n",
    "import zipfile\n",
    "import os\n",
    "from textClassifier.utils.commons import get_size\n",
    "import gdown\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class DataPreprocessing:\n",
    "    def __init__(self, config: DataPreprocessingConfig):\n",
    "        self.config = config\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "    def text_process(self, text: str) -> str:\n",
    "        text = text.lower()\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        doc = self.nlp(text)\n",
    "        tokens = [token.lemma_ for token in doc if token.text not in self.stop_words]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def get_data_from_features(self) -> pd.DataFrame:\n",
    "        data = pd.read_csv(self.config.training_data_file)\n",
    "        data['cleaned_text'] = data['text'].apply(self.text_process)\n",
    "        return data\n",
    "    \n",
    "    def save_data(self, data: pd.DataFrame, path_name: str='cleaned_tweets.csv') -> None:\n",
    "        out_path_name = Path(self.config.training_cleansed_data)\n",
    "        data.to_csv(out_path_name, index=False)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-08 15:30:36,292: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\config\\config.yaml loaded successfully]\n",
      "[2025-02-08 15:30:36,297: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\params.yaml loaded successfully]\n",
      "[2025-02-08 15:30:36,299: INFO: commons: created directory at: artifacts]\n",
      "[2025-02-08 15:30:36,301: INFO: commons: created directory at: artifacts/feature_engineering]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "save_data() missing 1 required positional argument: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m data_processing \u001b[38;5;241m=\u001b[39m DataPreprocessing(config \u001b[38;5;241m=\u001b[39m get_data_pre_config)\n\u001b[0;32m      4\u001b[0m output \u001b[38;5;241m=\u001b[39m data_processing\u001b[38;5;241m.\u001b[39mget_data_from_features()\n\u001b[1;32m----> 5\u001b[0m save_data \u001b[38;5;241m=\u001b[39m \u001b[43mdata_processing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: save_data() missing 1 required positional argument: 'data'"
     ]
    }
   ],
   "source": [
    "config_manager = ConfigurationManager()\n",
    "get_data_pre_config = config_manager.get_data_preprocessing_config()\n",
    "data_processing = DataPreprocessing(config = get_data_pre_config)\n",
    "output = data_processing.get_data_from_features()\n",
    "save_data = data_processing.save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
      "0  570306133677760513           neutral                        1.0000   \n",
      "1  570301130888122368          positive                        0.3486   \n",
      "2  570301083672813571           neutral                        0.6837   \n",
      "3  570301031407624196          negative                        1.0000   \n",
      "4  570300817074462722          negative                        1.0000   \n",
      "\n",
      "  negativereason  negativereason_confidence         airline  \\\n",
      "0            NaN                        NaN  Virgin America   \n",
      "1            NaN                     0.0000  Virgin America   \n",
      "2            NaN                        NaN  Virgin America   \n",
      "3     Bad Flight                     0.7033  Virgin America   \n",
      "4     Can't Tell                     1.0000  Virgin America   \n",
      "\n",
      "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
      "0                    NaN     cairdin                 NaN              0   \n",
      "1                    NaN    jnardino                 NaN              0   \n",
      "2                    NaN  yvonnalynn                 NaN              0   \n",
      "3                    NaN    jnardino                 NaN              0   \n",
      "4                    NaN    jnardino                 NaN              0   \n",
      "\n",
      "                                                text tweet_coord  \\\n",
      "0                @VirginAmerica What @dhepburn said.         NaN   \n",
      "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
      "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
      "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
      "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
      "\n",
      "               tweet_created tweet_location               user_timezone  \\\n",
      "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)   \n",
      "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)   \n",
      "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)   \n",
      "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)   \n",
      "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0                         virginamerica dhepburn say  \n",
      "1  virginamerica plus add commercial experience t...  \n",
      "2  virginamerica not today must mean need take an...  \n",
      "3  virginamerica really aggressive blast obnoxiou...  \n",
      "4                 virginamerica really big bad thing  \n"
     ]
    }
   ],
   "source": [
    "print(output.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textClassifier import logger\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from pathlib import Path\n",
    "\n",
    "class DataPreprocessing:\n",
    "    def __init__(self, config: DataPreprocessingConfig):\n",
    "        self.config = config\n",
    "        self.nlp = spacy.load('en_core_web_sm')  # Load spaCy model\n",
    "        self.stop_words = set(stopwords.words('english'))  # Load stopwords\n",
    "\n",
    "    def text_process(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocesses the input text by:\n",
    "        1. Converting to lowercase\n",
    "        2. Removing punctuation\n",
    "        3. Tokenizing and lemmatizing\n",
    "        4. Removing stopwords\n",
    "        \"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # Tokenize and lemmatize\n",
    "        doc = self.nlp(text)\n",
    "        tokens = [token.lemma_ for token in doc if token.text not in self.stop_words]\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def get_data_from_features(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads the dataset from the path specified in the config and applies preprocessing to the text column.\n",
    "        \"\"\"\n",
    "        # Check if the file exists\n",
    "        if not Path(self.config.training_data_file).exists():\n",
    "            raise FileNotFoundError(f\"File not found: {self.config.training_data_file}\")\n",
    "\n",
    "        # Load the dataset\n",
    "        data = pd.read_csv(self.config.training_data_file)\n",
    "\n",
    "        # Apply preprocessing to the text column\n",
    "        data['cleaned_text'] = data['text'].apply(self.text_process)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def save_data(self, data: pd.DataFrame, path_name: str = 'cleaned_tweets.csv') -> None:\n",
    "        \"\"\"\n",
    "        Saves the preprocessed data to the specified output file.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): The preprocessed DataFrame to save.\n",
    "            path_name (str): The name of the output file. Defaults to \"cleaned_tweets.csv\".\n",
    "        \"\"\"\n",
    "        # Create the full output file path\n",
    "        out_path_name = Path(self.config.training_cleansed_data) / path_name\n",
    "\n",
    "        # Save the DataFrame to CSV\n",
    "        data.to_csv(out_path_name, index=False)\n",
    "        logger.info(f\"Preprocessed data saved to {out_path_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-08 15:41:41,083: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\config\\config.yaml loaded successfully]\n",
      "[2025-02-08 15:41:41,087: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\params.yaml loaded successfully]\n",
      "[2025-02-08 15:41:41,090: INFO: commons: created directory at: artifacts]\n",
      "[2025-02-08 15:41:41,094: INFO: commons: created directory at: artifacts/feature_engineering]\n",
      "[2025-02-08 15:43:36,407: INFO: 1592871266: Preprocessed data saved to artifacts\\feature_engineering\\cleaned_tweets.csv]\n"
     ]
    }
   ],
   "source": [
    "config_manager = ConfigurationManager()\n",
    "\n",
    "# Get the data preprocessing config\n",
    "get_data_pre_config = config_manager.get_data_preprocessing_config()\n",
    "\n",
    "# Initialize DataPreprocessing with the config\n",
    "data_processing = DataPreprocessing(config=get_data_pre_config)\n",
    "\n",
    "# Load and preprocess the data\n",
    "output = data_processing.get_data_from_features()\n",
    "\n",
    "# Save the preprocessed data\n",
    "data_processing.save_data(data=output, path_name='cleaned_tweets.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
