{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows-SSD\n",
      " Volume Serial Number is 5824-84FE\n",
      "\n",
      " Directory of c:\\Users\\prass\\OneDrive\\Desktop\\practise\\new_env\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/03/2025  10:10 PM    <DIR>          .\n",
      "02/03/2025  06:58 PM    <DIR>          ..\n",
      "02/03/2025  10:02 PM    <DIR>          .github\n",
      "02/17/2025  03:23 PM    <DIR>          text-classification-using-BERT\n",
      "               0 File(s)              0 bytes\n",
      "               4 Dir(s)  256,057,806,848 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    datasets_dir: Path  # Directory where datasets are saved\n",
    "    base_model: Path\n",
    "    output_dir: Path  # Directory to save training outputs\n",
    "    model_save_path: Path  # Directory to save the trained model\n",
    "    num_train_epochs: int  # Number of training epochs\n",
    "    per_device_train_batch_size: int  # Training batch size per device\n",
    "    per_device_eval_batch_size: int  # Evaluation batch size per device\n",
    "    warmup_steps: int  # Number of warmup steps\n",
    "    weight_decay: float  # Weight decay rate\n",
    "    max_steps: int  # Maximum number of training steps\n",
    "    save_steps: int  # Save model every `save_steps` steps\n",
    "    logging_steps: int  # Log metrics every `logging_steps` steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CONFIG_FILE_PATH = Path(\"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT/config/config.yaml\")\n",
    "PARAMS_FILE_PATH = Path(\"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT/params.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\config\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "print(CONFIG_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'usr\\\\bin\\\\spam'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "# from pathlib import Path\n",
    "# path = os.path.join('usr', 'bin', 'spam')\n",
    "# print(Path(path))\n",
    "import os\n",
    "# from pathlib import Path\n",
    "os.path.join('usr', 'bin', 'spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textClassifier.constants import *\n",
    "from textClassifier.utils.commons import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from pathlib import Path\n",
    "\n",
    "# class ConfigurationManager:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         config_filepath = CONFIG_FILE_PATH,\n",
    "#         params_filepath = PARAMS_FILE_PATH\n",
    "#     ):\n",
    "#         self.config = read_yaml(config_filepath)\n",
    "#         self.params = read_yaml(params_filepath)\n",
    "\n",
    "#         create_directories([self.config.artifacts_root])\n",
    "#         # Debug: Print the config and params objects\n",
    "#         print(\"Config:\", self.config)\n",
    "#         print(\"Params:\", self.params)\n",
    "\n",
    "#     def get_training_config(self) -> TrainingConfig:\n",
    "#         \"\"\"\n",
    "#         Returns the configuration for model training.\n",
    "#         \"\"\"\n",
    "#         training = self.config.feature_engineering\n",
    "#         print(\"i am printing training\", training)\n",
    "#         params = self.params\n",
    "#         training_data = os.path.join(self.config.artifacts_root, 'feature_engineering')\n",
    "#         model_save_path = os.path.join(self.config.artifacts_root, 'training')\n",
    "\n",
    "#         create_directories([training.root_dir, training.model_save_path])\n",
    "\n",
    "#         training_config = TrainingConfig(\n",
    "#             datasets_dir=Path(training_data),\n",
    "#             output_dir=Path(training.root_dir),\n",
    "#             # model_save_path=Path(training.model_save_path),\n",
    "#             params_num_train_epochs=params.num_train_epochs,\n",
    "#             params_per_device_train_batch_size=params.per_device_train_batch_size,\n",
    "#             params_per_device_eval_batch_size=params.per_device_eval_batch_size,\n",
    "#             params_warmup_steps=params.warmup_steps,\n",
    "#             params_weight_decay=params.weight_decay,\n",
    "#             params_max_steps=params.max_steps,\n",
    "#             params_save_steps=params.save_steps,\n",
    "#             params_logging_steps=params.logging_steps\n",
    "            \n",
    "#         )\n",
    "\n",
    "#         return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This below code is for BERT but we are using distllBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# #  this is bert model or bert-based-uncased but we downgraded to distillBert for resources bottleneks\n",
    "# from transformers import (\n",
    "#     BertForSequenceClassification,\n",
    "#     Trainer,\n",
    "#     TrainingArguments\n",
    "# )\n",
    "\n",
    "# from pathlib import Path\n",
    "# import torch\n",
    "# from textClassifier import logger\n",
    "\n",
    "\n",
    "# class ModelTraining:\n",
    "#     def __init__(self, config: TrainingConfig):\n",
    "#         \"\"\"\n",
    "#         Initializes the ModelTraining class.\n",
    "\n",
    "#         Args:\n",
    "#             config (ModelTrainingConfig): Configuration for model training.\n",
    "#         \"\"\"\n",
    "#         self.config = config\n",
    "\n",
    "#     def load_datasets(self):\n",
    "#         \"\"\"\n",
    "#         Loads the train, validation, and test datasets from the specified directory.\n",
    "\n",
    "#         Returns:\n",
    "#             train_dataset, val_dataset, test_dataset: Loaded datasets.\n",
    "#         \"\"\"\n",
    "#         datasets_dir = Path(self.config.feature_engineering)\n",
    "#         train_dataset = torch.load(datasets_dir / \"train_dataset.pt\")\n",
    "#         val_dataset = torch.load(datasets_dir / \"val_dataset.pt\")\n",
    "#         test_dataset = torch.load(datasets_dir / \"test_dataset.pt\")\n",
    "\n",
    "#         logger.info(f\"Datasets loaded from {datasets_dir}\")\n",
    "#         return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "#     def train(self):\n",
    "#         \"\"\"\n",
    "#         Trains the model using the loaded datasets.\n",
    "#         \"\"\"\n",
    "#         # Load datasets\n",
    "#         train_dataset, val_dataset, _ = self.load_datasets()\n",
    "\n",
    "#         # Initialize the model\n",
    "#         model = BertForSequenceClassification.from_pretrained(\n",
    "#             'bert-base-uncased', num_labels=3\n",
    "#         )\n",
    "\n",
    "#         # Set up training arguments\n",
    "#         training_args = TrainingArguments(\n",
    "#             output_dir=self.config.output_dir,\n",
    "#             num_train_epochs=self.config.num_train_epochs,\n",
    "#             per_device_train_batch_size=self.config.per_device_train_batch_size,\n",
    "#             per_device_eval_batch_size=self.config.per_device_eval_batch_size,\n",
    "#             warmup_steps=self.config.warmup_steps,\n",
    "#             weight_decay=self.config.weight_decay,\n",
    "#             max_steps=self.config.max_steps,\n",
    "#             save_steps=self.config.save_steps,\n",
    "#             logging_steps=self.config.logging_steps,\n",
    "#             load_best_model_at_end=True\n",
    "#         )\n",
    "\n",
    "#         # Initialize the Trainer\n",
    "#         trainer = Trainer(\n",
    "#             model=model,\n",
    "#             args=training_args,\n",
    "#             train_dataset=train_dataset,\n",
    "#             eval_dataset=val_dataset\n",
    "#         )\n",
    "\n",
    "#         # Start training\n",
    "#         trainer.train()\n",
    "\n",
    "#         # Save the trained model\n",
    "#         self.save_model(trainer)\n",
    "\n",
    "#     def save_model(self, trainer):\n",
    "#         \"\"\"\n",
    "#         Saves the trained model and tokenizer to the specified directory.\n",
    "\n",
    "#         Args:\n",
    "#             trainer (Trainer): The Trainer object containing the trained model.\n",
    "#         \"\"\"\n",
    "#         save_path = Path(self.config.model_save_path)\n",
    "#         save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#         trainer.save_model(save_path)\n",
    "#         logger.info(f\"Model saved to {save_path}\")\n",
    "\n",
    "#     def evaluate(self):\n",
    "#         \"\"\"\n",
    "#         Evaluates the model on the test dataset.\n",
    "#         \"\"\"\n",
    "#         # Load the test dataset\n",
    "#         _, _, test_dataset = self.load_datasets()\n",
    "\n",
    "#         # Load the trained model\n",
    "#         model = BertForSequenceClassification.from_pretrained(self.config.model_save_path)\n",
    "\n",
    "#         # Set up training arguments for evaluation\n",
    "#         training_args = TrainingArguments(\n",
    "#             output_dir=self.config.output_dir,\n",
    "#             per_device_eval_batch_size=self.config.per_device_eval_batch_size,\n",
    "#             logging_dir=self.config.logging_dir\n",
    "#         )\n",
    "\n",
    "#         # Initialize the Trainer for evaluation\n",
    "#         trainer = Trainer(\n",
    "#             model=model,\n",
    "#             args=training_args,\n",
    "#             # train_dataset=train_dataset, # The dataset used for training the model\n",
    "#             # eval_dataset=val_dataset\n",
    "#         )\n",
    "\n",
    "#         # Evaluate the model\n",
    "#         results = trainer.evaluate(test_dataset)\n",
    "#         logger.info(\"Evaluation Results:\")\n",
    "#         logger.info(f\"  - Loss: {results['eval_loss']:.4f}\")\n",
    "#         logger.info(f\"  - Runtime: {results['eval_runtime']:.2f} seconds\")\n",
    "#         logger.info(f\"  - Samples per Second: {results['eval_samples_per_second']:.2f}\")\n",
    "#         logger.info(f\"  - Steps per Second: {results['eval_steps_per_second']:.2f}\")\n",
    "#         logger.info(f\"  - Epoch: {results['epoch']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConfigurationManager:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         config_filepath = CONFIG_FILE_PATH,\n",
    "#         params_filepath = PARAMS_FILE_PATH\n",
    "#     ):\n",
    "#         self.config = read_yaml(config_filepath)\n",
    "#         self.params = read_yaml(params_filepath)\n",
    "\n",
    "#         create_directories([self.config.artifacts_root])\n",
    "\n",
    "#     def get_training_config(self) -> TrainingConfig:\n",
    "#         \"\"\"\n",
    "#         Returns the configuration for model training.\n",
    "#         \"\"\"\n",
    "#         training = self.config.training\n",
    "#         params = self.params\n",
    "        \n",
    "\n",
    "\n",
    "#         # Create directories if they don't exist\n",
    "#         create_directories([\n",
    "#             Path(training.root_dir)])\n",
    "\n",
    "#         training_config = TrainingConfig(\n",
    "#             datasets_dir=Path(self.config.feature_engineering.datasets_dir),  # Update this to point to feature_engineering\n",
    "#             base_model_path=Path(self.config.artifacts.prepare_model),\n",
    "#             output_dir=Path(training.root_dir),\n",
    "#             model_save_path=Path(training.model_save_path),\n",
    "#             num_train_epochs=params.num_train_epochs,\n",
    "#             per_device_train_batch_size=params.per_device_train_batch_size,\n",
    "#             per_device_eval_batch_size=params.per_device_eval_batch_size,\n",
    "#             warmup_steps=params.warmup_steps,\n",
    "#             weight_decay=params.weight_decay,\n",
    "#             max_steps=params.max_steps,\n",
    "#             save_steps=params.save_steps,\n",
    "#             logging_steps=params.logging_steps\n",
    "#         )\n",
    "\n",
    "#         return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT\\\\research'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual Training starts here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CONFIG_FILE_PATH = Path(\"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT/config/config.yaml\")\n",
    "PARAMS_FILE_PATH = Path(\"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT/params.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textClassifier.constants import *\n",
    "from textClassifier.utils.commons import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    datasets_dir: Path  # Directory where datasets are saved\n",
    "    output_dir: Path  # Directory to save training outputs\n",
    "    base_model_path: Path\n",
    "    model_save_path: Path  # Directory to save the trained model\n",
    "    num_train_epochs: int  # Number of training epochs\n",
    "    per_device_train_batch_size: int  # Training batch size per device\n",
    "    per_device_eval_batch_size: int  # Evaluation batch size per device\n",
    "    warmup_steps: int  # Number of warmup steps\n",
    "    weight_decay: float  # Weight decay rate\n",
    "    max_steps: int  # Maximum number of training steps\n",
    "    save_steps: int  # Save model every `save_steps` steps\n",
    "    logging_steps: int  # Log metrics every `logging_steps` steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        \"\"\"\n",
    "        Returns the configuration for model training.\n",
    "        \"\"\"\n",
    "        training = self.config.training\n",
    "        params = self.params\n",
    "        training_data = os.path.join(self.config.feature_engineering.datasets_dir, \"datasets\")\n",
    "        base_model = self.config.prepare_model\n",
    "        base_model = os.path.join(self.config.prepare_model.base_model_path, \"prepare_model\")\n",
    "    \n",
    "\n",
    "        # Create directories if they don't exist\n",
    "        create_directories([\n",
    "            Path(training.root_dir),\n",
    "            Path(training.model_save_path)\n",
    "        ])\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            datasets_dir=Path(training_data),  # Update this to point to feature_engineering\n",
    "            base_model_path=Path(base_model),\n",
    "            output_dir=Path(training.root_dir),\n",
    "            model_save_path=Path(training.model_save_path),\n",
    "            num_train_epochs=params.num_train_epochs,\n",
    "            per_device_train_batch_size=params.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=params.per_device_eval_batch_size,\n",
    "            warmup_steps=params.warmup_steps,\n",
    "            weight_decay=params.weight_decay,\n",
    "            max_steps=params.max_steps,\n",
    "            save_steps=params.save_steps,\n",
    "            logging_steps=params.logging_steps\n",
    "        )\n",
    "\n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments, \n",
    "    DataCollatorWithPadding,\n",
    "    AutoTokenizer\n",
    ")\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from textClassifier import logger\n",
    "\n",
    "\n",
    "class ModelTraining:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        \"\"\"\n",
    "        Initializes the ModelTraining class.\n",
    "\n",
    "        Args:\n",
    "            config (ModelTrainingConfig): Configuration for model training.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "    def load_datasets(self):\n",
    "        \"\"\"\n",
    "        Loads the train, validation, and test datasets from the specified directory.\n",
    "\n",
    "        Returns:\n",
    "            train_dataset, val_dataset, test_dataset: Loaded datasets.\n",
    "        \"\"\"\n",
    "        # C:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\artifacts\\feature_engineering\\datasets\n",
    "        # datasets = Path(self.config.datasets_dir)\n",
    "        datasets = Path(\"artifacts/feature_engineering/datasets\")\n",
    "        print(self.config.datasets_dir)\n",
    "        print(datasets)\n",
    "        train_dataset = torch.load(datasets / \"train_dataset.pt\", weights_only=False)\n",
    "        val_dataset = torch.load(datasets / \"val_dataset.pt\", weights_only=False)\n",
    "        test_dataset = torch.load(datasets / \"test_dataset.pt\", weights_only=False)\n",
    "        # train_dataset = torch.load(datasets_dir / \"train_dataset.pt\", weights_only=False)\n",
    "        # val_dataset = torch.load(datasets_dir / \"val_dataset.pt\", weights_only=False)\n",
    "        # test_dataset = torch.load(datasets_dir / \"test_dataset.pt\", weights_only=False)\n",
    "\n",
    "        logger.info(f\"Datasets loaded from {datasets}\")\n",
    "        return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the model using the loaded datasets.\n",
    "        \"\"\"\n",
    "        # Load datasets\n",
    "        train_dataset, val_dataset, _ = self.load_datasets()\n",
    "\n",
    "        # Initialize the model\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"distilbert-base-uncased\", num_labels=3\n",
    "            )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "        \n",
    "\n",
    "        #model = BertForSequenceClassification.from_pretrained(\n",
    "        #    'bert-base-uncased', num_labels=3\n",
    "        #)\n",
    "\n",
    "        # Set up training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config.output_dir,\n",
    "            num_train_epochs=self.config.num_train_epochs,\n",
    "            per_device_train_batch_size=self.config.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=self.config.per_device_eval_batch_size,\n",
    "            warmup_steps=self.config.warmup_steps,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            max_steps=self.config.max_steps,\n",
    "            save_steps=self.config.save_steps,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            # load_best_model_at_end=True\n",
    "        )\n",
    "\n",
    "        # Initialize the Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator\n",
    "        )\n",
    "\n",
    "        # Start training\n",
    "        trainer.train()\n",
    "\n",
    "        # Save the trained model\n",
    "        self.save_model(trainer)\n",
    "\n",
    "    def save_model(self, trainer):\n",
    "        \"\"\"\n",
    "        Saves the trained model and tokenizer to the specified directory.\n",
    "\n",
    "        Args:\n",
    "            trainer (Trainer): The Trainer object containing the trained model.\n",
    "        \"\"\"\n",
    "        save_path = Path(self.config.model_save_path)\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        trainer.save_model(save_path)\n",
    "        logger.info(f\"Model saved to {save_path}\")\n",
    "\n",
    "        # # Save the tokenizer\n",
    "        # tokenizer = trainer.tokenizer\n",
    "        # if tokenizer is not None:\n",
    "        #     tokenizer.save_pretrained(save_path)\n",
    "        #     logger.info(f\"Tokenizer saved to {save_path}\")\n",
    "        # else:\n",
    "        #     logger.warning(\"Tokenizer not found in the trainer object. Only the model was saved.\")\n",
    "\n",
    "        # Save the processing class (e.g., tokenizer)\n",
    "        processing_class = getattr(trainer, \"processing_class\", None)\n",
    "        if processing_class is not None:\n",
    "            processing_class.save_pretrained(save_path)\n",
    "            logger.info(f\"Processing class (e.g., tokenizer) saved to {save_path}\")\n",
    "        else:\n",
    "            logger.warning(\"Processing class not found. Only the model was saved.\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluates the model on the test dataset.\n",
    "        \"\"\"\n",
    "        # Load the test dataset\n",
    "        _, _, test_dataset = self.load_datasets()\n",
    "\n",
    "        # Load the trained model\n",
    "        # model = AutoModelForSequenceClassification.from_pretrained(self.config.model_save_path)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(self.config.model_save_path)\n",
    "\n",
    "\n",
    "        # Set up training arguments for evaluation\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config.output_dir,\n",
    "            per_device_eval_batch_size=self.config.per_device_eval_batch_size\n",
    "        )\n",
    "\n",
    "        # Initialize the Trainer for evaluation\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args\n",
    "            \n",
    "        )\n",
    "\n",
    "        # Evaluate the model\n",
    "        results = trainer.evaluate(test_dataset)\n",
    "\n",
    "        logger.info(\"Evaluation Results:\")\n",
    "        logger.info(\"Evaluation Results:\", results)\n",
    "        logger.info(f\"  - Loss: {results['eval_loss']:.4f}\")\n",
    "        logger.info(f\"  - Runtime: {results['eval_runtime']:.2f} seconds\")\n",
    "        logger.info(f\"  - Samples per Second: {results['eval_samples_per_second']:.2f}\")\n",
    "        logger.info(f\"  - Steps per Second: {results['eval_steps_per_second']:.2f}\")\n",
    "        logger.info(f\"  - Epoch: {results.get('epoch', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments, \n",
    "    DataCollatorWithPadding,\n",
    "    AutoTokenizer\n",
    ")\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from textClassifier import logger\n",
    "\n",
    "\n",
    "class ModelTraining:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        \"\"\"\n",
    "        Initializes the ModelTraining class.\n",
    "\n",
    "        Args:\n",
    "            config (TrainingConfig): Configuration for model training.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "    def load_datasets(self):\n",
    "        \"\"\"\n",
    "        Loads the train, validation, and test datasets from the specified directory.\n",
    "        \"\"\"\n",
    "        # Explicitly set the correct path\n",
    "        datasets = Path(\"artifacts/feature_engineering/datasets\")\n",
    "        \n",
    "        # Debug: Print the datasets directory\n",
    "        print(\"Datasets Directory:\", datasets)\n",
    "\n",
    "        # Load datasets\n",
    "        train_dataset = torch.load(datasets / \"train_dataset.pt\", weights_only=False)\n",
    "        val_dataset = torch.load(datasets / \"val_dataset.pt\", weights_only=False)\n",
    "        test_dataset = torch.load(datasets / \"test_dataset.pt\", weights_only=False)\n",
    "\n",
    "        logger.info(f\"Datasets loaded from {datasets}\")\n",
    "        return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the model using the loaded datasets.\n",
    "        \"\"\"\n",
    "        # Load datasets\n",
    "        train_dataset, val_dataset, _ = self.load_datasets()\n",
    "\n",
    "        # Load the base model and tokenizer from the artifacts/prepare_model folder\n",
    "        # base_model_path = Path(self.config.base_model_path)\n",
    "        base_model_path = Path(\"artifacts/prepare_model\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(base_model_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "        # Set up training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config.output_dir,\n",
    "            num_train_epochs=self.config.num_train_epochs,\n",
    "            per_device_train_batch_size=self.config.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=self.config.per_device_eval_batch_size,\n",
    "            warmup_steps=self.config.warmup_steps,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            max_steps=self.config.max_steps,\n",
    "            save_steps=self.config.save_steps,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "        )\n",
    "\n",
    "        # Initialize the Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator\n",
    "        )\n",
    "\n",
    "        # Start training\n",
    "        trainer.train()\n",
    "\n",
    "        # Save the trained model\n",
    "        self.save_model(trainer)\n",
    "\n",
    "    def save_model(self, trainer):\n",
    "        \"\"\"\n",
    "        Saves the trained model and tokenizer to the specified directory.\n",
    "\n",
    "        Args:\n",
    "            trainer (Trainer): The Trainer object containing the trained model.\n",
    "        \"\"\"\n",
    "        save_path = Path(self.config.model_save_path)\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save the model\n",
    "        trainer.save_model(save_path)\n",
    "        logger.info(f\"Model saved to {save_path}\")\n",
    "\n",
    "        # Save the tokenizer\n",
    "        tokenizer = trainer.tokenizer\n",
    "        if tokenizer is not None:\n",
    "            tokenizer.save_pretrained(save_path)\n",
    "            logger.info(f\"Tokenizer saved to {save_path}\")\n",
    "        else:\n",
    "            logger.warning(\"Tokenizer not found in the trainer object. Only the model was saved.\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluates the model on the test dataset.\n",
    "        \"\"\"\n",
    "        # Load the test dataset\n",
    "        _, _, test_dataset = self.load_datasets()\n",
    "\n",
    "        # Load the trained model from the model_save_path\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(self.config.model_save_path)\n",
    "\n",
    "        # Set up training arguments for evaluation\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config.output_dir,\n",
    "            per_device_eval_batch_size=self.config.per_device_eval_batch_size\n",
    "        )\n",
    "\n",
    "        # Initialize the Trainer for evaluation\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args\n",
    "        )\n",
    "\n",
    "        # Evaluate the model\n",
    "        results = trainer.evaluate(test_dataset)\n",
    "\n",
    "        logger.info(\"Evaluation Results:\")\n",
    "        logger.info(f\"  - Loss: {results['eval_loss']:.4f}\")\n",
    "        logger.info(f\"  - Runtime: {results['eval_runtime']:.2f} seconds\")\n",
    "        logger.info(f\"  - Samples per Second: {results['eval_samples_per_second']:.2f}\")\n",
    "        logger.info(f\"  - Steps per Second: {results['eval_steps_per_second']:.2f}\")\n",
    "        logger.info(f\"  - Epoch: {results.get('epoch', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-14 01:01:37,324: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\config\\config.yaml loaded successfully]\n",
      "[2025-03-14 01:01:37,334: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\params.yaml loaded successfully]\n",
      "[2025-03-14 01:01:37,337: INFO: commons: created directory at: artifacts]\n",
      "[2025-03-14 01:01:37,339: INFO: commons: created directory at: artifacts\\training]\n",
      "[2025-03-14 01:01:37,341: INFO: commons: created directory at: artifacts\\training\\trained_model]\n",
      "Datasets Directory: artifacts\\feature_engineering\\datasets\n",
      "[2025-03-14 01:01:37,558: INFO: 1819293319: Datasets loaded from artifacts\\feature_engineering\\datasets]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:18<00:00,  1.74s/it]\n",
      "  0%|          | 0/1098 [00:00<?, ?it/s]\n",
      "  0%|          | 3/1098 [00:00<01:03, 17.21it/s]\n",
      "  0%|          | 5/1098 [00:00<01:21, 13.39it/s]\n",
      "  1%|          | 7/1098 [00:00<01:35, 11.38it/s]\n",
      "  1%|          | 9/1098 [00:00<01:38, 11.00it/s]\n",
      "  1%|          | 11/1098 [00:00<01:45, 10.30it/s]\n",
      "  1%|          | 13/1098 [00:01<01:46, 10.17it/s]\n",
      "  1%|â–         | 15/1098 [00:01<01:48,  9.95it/s]\n",
      "  2%|â–         | 17/1098 [00:01<01:52,  9.62it/s]\n",
      "  2%|â–         | 18/1098 [00:01<01:54,  9.43it/s]\n",
      "  2%|â–         | 19/1098 [00:01<01:55,  9.30it/s]\n",
      "  2%|â–         | 20/1098 [00:01<01:54,  9.41it/s]\n",
      "  2%|â–         | 21/1098 [00:02<01:53,  9.49it/s]\n",
      "  2%|â–         | 22/1098 [00:02<01:54,  9.41it/s]\n",
      "  2%|â–         | 23/1098 [00:02<01:55,  9.33it/s]\n",
      "  2%|â–         | 25/1098 [00:02<01:52,  9.50it/s]\n",
      "  2%|â–         | 26/1098 [00:02<01:52,  9.51it/s]\n",
      "  2%|â–         | 27/1098 [00:02<01:54,  9.38it/s]\n",
      "  3%|â–Ž         | 28/1098 [00:02<01:54,  9.32it/s]\n",
      "  3%|â–Ž         | 29/1098 [00:02<01:52,  9.47it/s]\n",
      "  3%|â–Ž         | 30/1098 [00:03<01:54,  9.36it/s]\n",
      "  3%|â–Ž         | 31/1098 [00:03<02:09,  8.23it/s]\n",
      "  3%|â–Ž         | 32/1098 [00:03<02:05,  8.49it/s]\n",
      "  3%|â–Ž         | 33/1098 [00:03<02:00,  8.83it/s]\n",
      "  3%|â–Ž         | 34/1098 [00:03<02:02,  8.70it/s]\n",
      "  3%|â–Ž         | 35/1098 [00:03<02:02,  8.66it/s]\n",
      "  3%|â–Ž         | 36/1098 [00:03<02:00,  8.81it/s]\n",
      "  3%|â–Ž         | 37/1098 [00:03<01:57,  9.00it/s]\n",
      "  3%|â–Ž         | 38/1098 [00:03<01:56,  9.07it/s]\n",
      "  4%|â–Ž         | 39/1098 [00:04<01:54,  9.22it/s]\n",
      "  4%|â–Ž         | 40/1098 [00:04<01:55,  9.18it/s]\n",
      "  4%|â–Ž         | 41/1098 [00:04<01:54,  9.22it/s]\n",
      "  4%|â–         | 42/1098 [00:04<01:53,  9.34it/s]\n",
      "  4%|â–         | 43/1098 [00:04<01:51,  9.44it/s]\n",
      "  4%|â–         | 44/1098 [00:04<01:50,  9.54it/s]\n",
      "  4%|â–         | 45/1098 [00:04<01:50,  9.52it/s]\n",
      "  4%|â–         | 47/1098 [00:04<01:44, 10.06it/s]\n",
      "  4%|â–         | 48/1098 [00:04<01:46,  9.88it/s]\n",
      "  4%|â–         | 49/1098 [00:05<01:46,  9.84it/s]\n",
      "  5%|â–         | 50/1098 [00:05<01:46,  9.81it/s]\n",
      "  5%|â–         | 51/1098 [00:05<01:47,  9.72it/s]\n",
      "  5%|â–         | 52/1098 [00:05<01:46,  9.79it/s]\n",
      "  5%|â–         | 53/1098 [00:05<01:47,  9.73it/s]\n",
      "  5%|â–         | 54/1098 [00:05<01:46,  9.76it/s]\n",
      "  5%|â–Œ         | 55/1098 [00:05<01:49,  9.55it/s]\n",
      "  5%|â–Œ         | 56/1098 [00:05<01:47,  9.67it/s]\n",
      "  5%|â–Œ         | 58/1098 [00:05<01:41, 10.25it/s]\n",
      "  5%|â–Œ         | 60/1098 [00:06<01:39, 10.39it/s]\n",
      "  6%|â–Œ         | 62/1098 [00:06<01:37, 10.61it/s]\n",
      "  6%|â–Œ         | 64/1098 [00:06<01:38, 10.55it/s]\n",
      "  6%|â–Œ         | 66/1098 [00:06<01:36, 10.70it/s]\n",
      "  6%|â–Œ         | 68/1098 [00:06<01:37, 10.61it/s]\n",
      "  6%|â–‹         | 70/1098 [00:07<01:36, 10.67it/s]\n",
      "  7%|â–‹         | 72/1098 [00:07<01:38, 10.44it/s]\n",
      "  7%|â–‹         | 74/1098 [00:07<01:38, 10.43it/s]\n",
      "  7%|â–‹         | 76/1098 [00:07<01:36, 10.64it/s]\n",
      "  7%|â–‹         | 78/1098 [00:07<01:34, 10.83it/s]\n",
      "  7%|â–‹         | 80/1098 [00:08<01:35, 10.63it/s]\n",
      "  7%|â–‹         | 82/1098 [00:08<01:34, 10.80it/s]\n",
      "  8%|â–Š         | 84/1098 [00:08<01:36, 10.51it/s]\n",
      "  8%|â–Š         | 86/1098 [00:08<01:34, 10.66it/s]\n",
      "  8%|â–Š         | 88/1098 [00:08<01:33, 10.84it/s]\n",
      "  8%|â–Š         | 90/1098 [00:08<01:33, 10.81it/s]\n",
      "  8%|â–Š         | 92/1098 [00:09<01:33, 10.70it/s]\n",
      "  9%|â–Š         | 94/1098 [00:09<01:33, 10.71it/s]\n",
      "  9%|â–Š         | 96/1098 [00:09<01:34, 10.62it/s]\n",
      "  9%|â–‰         | 98/1098 [00:09<01:34, 10.58it/s]\n",
      "  9%|â–‰         | 100/1098 [00:09<01:34, 10.55it/s]\n",
      "  9%|â–‰         | 102/1098 [00:10<01:35, 10.43it/s]\n",
      "  9%|â–‰         | 104/1098 [00:10<01:33, 10.60it/s]\n",
      " 10%|â–‰         | 106/1098 [00:10<01:34, 10.47it/s]\n",
      " 10%|â–‰         | 108/1098 [00:10<01:35, 10.37it/s]\n",
      " 10%|â–ˆ         | 110/1098 [00:10<01:34, 10.43it/s]\n",
      " 10%|â–ˆ         | 112/1098 [00:11<01:36, 10.23it/s]\n",
      " 10%|â–ˆ         | 114/1098 [00:11<01:37, 10.12it/s]\n",
      " 11%|â–ˆ         | 116/1098 [00:11<01:34, 10.34it/s]\n",
      " 11%|â–ˆ         | 118/1098 [00:11<01:33, 10.44it/s]\n",
      " 11%|â–ˆ         | 120/1098 [00:11<01:31, 10.71it/s]\n",
      " 11%|â–ˆ         | 122/1098 [00:12<01:29, 10.88it/s]\n",
      " 11%|â–ˆâ–        | 124/1098 [00:12<01:31, 10.70it/s]\n",
      " 11%|â–ˆâ–        | 126/1098 [00:12<01:31, 10.64it/s]\n",
      " 12%|â–ˆâ–        | 128/1098 [00:12<01:29, 10.90it/s]\n",
      " 12%|â–ˆâ–        | 130/1098 [00:12<01:30, 10.74it/s]\n",
      " 12%|â–ˆâ–        | 132/1098 [00:12<01:29, 10.79it/s]\n",
      " 12%|â–ˆâ–        | 134/1098 [00:13<01:32, 10.46it/s]\n",
      " 12%|â–ˆâ–        | 136/1098 [00:13<01:30, 10.57it/s]\n",
      " 13%|â–ˆâ–Ž        | 138/1098 [00:13<01:31, 10.54it/s]\n",
      " 13%|â–ˆâ–Ž        | 140/1098 [00:13<01:30, 10.58it/s]\n",
      " 13%|â–ˆâ–Ž        | 142/1098 [00:13<01:28, 10.74it/s]\n",
      " 13%|â–ˆâ–Ž        | 144/1098 [00:14<01:31, 10.37it/s]\n",
      " 13%|â–ˆâ–Ž        | 146/1098 [00:14<01:33, 10.18it/s]\n",
      " 13%|â–ˆâ–Ž        | 148/1098 [00:14<01:32, 10.28it/s]\n",
      " 14%|â–ˆâ–Ž        | 150/1098 [00:14<01:32, 10.27it/s]\n",
      " 14%|â–ˆâ–        | 152/1098 [00:14<01:30, 10.44it/s]\n",
      " 14%|â–ˆâ–        | 154/1098 [00:15<01:27, 10.76it/s]\n",
      " 14%|â–ˆâ–        | 156/1098 [00:15<01:27, 10.79it/s]\n",
      " 14%|â–ˆâ–        | 158/1098 [00:15<01:29, 10.55it/s]\n",
      " 15%|â–ˆâ–        | 160/1098 [00:15<01:27, 10.68it/s]\n",
      " 15%|â–ˆâ–        | 162/1098 [00:15<01:28, 10.63it/s]\n",
      " 15%|â–ˆâ–        | 164/1098 [00:15<01:27, 10.64it/s]\n",
      " 15%|â–ˆâ–Œ        | 166/1098 [00:16<01:29, 10.46it/s]\n",
      " 15%|â–ˆâ–Œ        | 168/1098 [00:16<01:29, 10.44it/s]\n",
      " 15%|â–ˆâ–Œ        | 170/1098 [00:16<01:32, 10.08it/s]\n",
      " 16%|â–ˆâ–Œ        | 172/1098 [00:16<01:31, 10.12it/s]\n",
      " 16%|â–ˆâ–Œ        | 174/1098 [00:16<01:29, 10.31it/s]\n",
      " 16%|â–ˆâ–Œ        | 176/1098 [00:17<01:27, 10.50it/s]\n",
      " 16%|â–ˆâ–Œ        | 178/1098 [00:17<01:27, 10.57it/s]\n",
      " 16%|â–ˆâ–‹        | 180/1098 [00:17<01:27, 10.54it/s]\n",
      " 17%|â–ˆâ–‹        | 182/1098 [00:17<01:23, 11.01it/s]\n",
      " 17%|â–ˆâ–‹        | 184/1098 [00:17<01:22, 11.05it/s]\n",
      " 17%|â–ˆâ–‹        | 186/1098 [00:18<01:24, 10.76it/s]\n",
      " 17%|â–ˆâ–‹        | 188/1098 [00:18<01:22, 11.00it/s]\n",
      " 17%|â–ˆâ–‹        | 190/1098 [00:18<01:23, 10.93it/s]\n",
      " 17%|â–ˆâ–‹        | 192/1098 [00:18<01:24, 10.75it/s]\n",
      " 18%|â–ˆâ–Š        | 194/1098 [00:18<01:23, 10.82it/s]\n",
      " 18%|â–ˆâ–Š        | 196/1098 [00:18<01:23, 10.84it/s]\n",
      " 18%|â–ˆâ–Š        | 198/1098 [00:19<01:22, 10.86it/s]\n",
      " 18%|â–ˆâ–Š        | 200/1098 [00:19<01:23, 10.73it/s]\n",
      " 18%|â–ˆâ–Š        | 202/1098 [00:19<01:22, 10.80it/s]\n",
      " 19%|â–ˆâ–Š        | 204/1098 [00:19<01:23, 10.75it/s]\n",
      " 19%|â–ˆâ–‰        | 206/1098 [00:19<01:22, 10.75it/s]\n",
      " 19%|â–ˆâ–‰        | 208/1098 [00:20<01:23, 10.65it/s]\n",
      " 19%|â–ˆâ–‰        | 210/1098 [00:20<01:22, 10.78it/s]\n",
      " 19%|â–ˆâ–‰        | 212/1098 [00:20<01:22, 10.69it/s]\n",
      " 19%|â–ˆâ–‰        | 214/1098 [00:20<01:21, 10.80it/s]\n",
      " 20%|â–ˆâ–‰        | 216/1098 [00:20<01:21, 10.86it/s]\n",
      " 20%|â–ˆâ–‰        | 218/1098 [00:21<01:22, 10.66it/s]\n",
      " 20%|â–ˆâ–ˆ        | 220/1098 [00:21<01:21, 10.73it/s]\n",
      " 20%|â–ˆâ–ˆ        | 222/1098 [00:21<01:21, 10.70it/s]\n",
      " 20%|â–ˆâ–ˆ        | 224/1098 [00:21<01:20, 10.81it/s]\n",
      " 21%|â–ˆâ–ˆ        | 226/1098 [00:21<01:20, 10.82it/s]\n",
      " 21%|â–ˆâ–ˆ        | 228/1098 [00:21<01:19, 10.93it/s]\n",
      " 21%|â–ˆâ–ˆ        | 230/1098 [00:22<01:18, 10.99it/s]\n",
      " 21%|â–ˆâ–ˆ        | 232/1098 [00:22<01:20, 10.75it/s]\n",
      " 21%|â–ˆâ–ˆâ–       | 234/1098 [00:22<01:19, 10.86it/s]\n",
      " 21%|â–ˆâ–ˆâ–       | 236/1098 [00:22<01:19, 10.79it/s]\n",
      " 22%|â–ˆâ–ˆâ–       | 238/1098 [00:22<01:20, 10.69it/s]\n",
      " 22%|â–ˆâ–ˆâ–       | 240/1098 [00:23<01:19, 10.76it/s]\n",
      " 22%|â–ˆâ–ˆâ–       | 242/1098 [00:23<01:19, 10.79it/s]\n",
      " 22%|â–ˆâ–ˆâ–       | 244/1098 [00:23<01:20, 10.56it/s]\n",
      " 22%|â–ˆâ–ˆâ–       | 246/1098 [00:23<01:20, 10.57it/s]\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 248/1098 [00:23<01:19, 10.67it/s]\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 250/1098 [00:24<01:18, 10.80it/s]\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 252/1098 [00:24<01:17, 10.89it/s]\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 254/1098 [00:24<01:17, 10.84it/s]\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 256/1098 [00:24<01:16, 11.03it/s]\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 258/1098 [00:24<01:18, 10.76it/s]\n",
      " 24%|â–ˆâ–ˆâ–Ž       | 260/1098 [00:24<01:18, 10.69it/s]\n",
      " 24%|â–ˆâ–ˆâ–       | 262/1098 [00:25<01:17, 10.72it/s]\n",
      " 24%|â–ˆâ–ˆâ–       | 264/1098 [00:25<01:18, 10.66it/s]\n",
      " 24%|â–ˆâ–ˆâ–       | 266/1098 [00:25<01:18, 10.65it/s]\n",
      " 24%|â–ˆâ–ˆâ–       | 268/1098 [00:25<01:17, 10.69it/s]\n",
      " 25%|â–ˆâ–ˆâ–       | 270/1098 [00:25<01:17, 10.71it/s]\n",
      " 25%|â–ˆâ–ˆâ–       | 272/1098 [00:26<01:15, 10.89it/s]\n",
      " 25%|â–ˆâ–ˆâ–       | 274/1098 [00:26<01:15, 10.96it/s]\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 276/1098 [00:26<01:15, 10.83it/s]\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 278/1098 [00:26<01:15, 10.90it/s]\n",
      " 26%|â–ˆâ–ˆâ–Œ       | 280/1098 [00:26<01:15, 10.85it/s]\n",
      " 26%|â–ˆâ–ˆâ–Œ       | 282/1098 [00:26<01:14, 10.90it/s]\n",
      " 26%|â–ˆâ–ˆâ–Œ       | 284/1098 [00:27<01:14, 10.89it/s]\n",
      " 26%|â–ˆâ–ˆâ–Œ       | 286/1098 [00:27<01:13, 11.10it/s]\n",
      " 26%|â–ˆâ–ˆâ–Œ       | 288/1098 [00:27<01:16, 10.59it/s]\n",
      " 26%|â–ˆâ–ˆâ–‹       | 290/1098 [00:27<01:16, 10.49it/s]\n",
      " 27%|â–ˆâ–ˆâ–‹       | 292/1098 [00:27<01:15, 10.62it/s]\n",
      " 27%|â–ˆâ–ˆâ–‹       | 294/1098 [00:28<01:15, 10.67it/s]\n",
      " 27%|â–ˆâ–ˆâ–‹       | 296/1098 [00:28<01:14, 10.71it/s]\n",
      " 27%|â–ˆâ–ˆâ–‹       | 298/1098 [00:28<01:14, 10.76it/s]\n",
      " 27%|â–ˆâ–ˆâ–‹       | 300/1098 [00:28<01:17, 10.24it/s]\n",
      " 28%|â–ˆâ–ˆâ–Š       | 302/1098 [00:28<01:17, 10.33it/s]\n",
      " 28%|â–ˆâ–ˆâ–Š       | 304/1098 [00:29<01:14, 10.66it/s]\n",
      " 28%|â–ˆâ–ˆâ–Š       | 306/1098 [00:29<01:13, 10.78it/s]\n",
      " 28%|â–ˆâ–ˆâ–Š       | 308/1098 [00:29<01:14, 10.67it/s]\n",
      " 28%|â–ˆâ–ˆâ–Š       | 310/1098 [00:29<01:15, 10.38it/s]\n",
      " 28%|â–ˆâ–ˆâ–Š       | 312/1098 [00:29<01:16, 10.30it/s]\n",
      " 29%|â–ˆâ–ˆâ–Š       | 314/1098 [00:30<01:15, 10.44it/s]\n",
      " 29%|â–ˆâ–ˆâ–‰       | 316/1098 [00:30<01:13, 10.64it/s]\n",
      " 29%|â–ˆâ–ˆâ–‰       | 318/1098 [00:30<01:10, 11.05it/s]\n",
      " 29%|â–ˆâ–ˆâ–‰       | 320/1098 [00:30<01:12, 10.80it/s]\n",
      " 29%|â–ˆâ–ˆâ–‰       | 322/1098 [00:30<01:12, 10.68it/s]\n",
      " 30%|â–ˆâ–ˆâ–‰       | 324/1098 [00:30<01:13, 10.56it/s]\n",
      " 30%|â–ˆâ–ˆâ–‰       | 326/1098 [00:31<01:13, 10.55it/s]\n",
      " 30%|â–ˆâ–ˆâ–‰       | 328/1098 [00:31<01:12, 10.63it/s]\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 330/1098 [00:31<01:11, 10.76it/s]\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 332/1098 [00:31<01:12, 10.55it/s]\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 334/1098 [00:31<01:12, 10.61it/s]\n",
      " 31%|â–ˆâ–ˆâ–ˆ       | 336/1098 [00:32<01:09, 10.92it/s]\n",
      " 31%|â–ˆâ–ˆâ–ˆ       | 338/1098 [00:32<01:09, 11.00it/s]\n",
      " 31%|â–ˆâ–ˆâ–ˆ       | 340/1098 [00:32<01:10, 10.78it/s]\n",
      " 31%|â–ˆâ–ˆâ–ˆ       | 342/1098 [00:32<01:10, 10.77it/s]\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–      | 344/1098 [00:32<01:10, 10.65it/s]\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 346/1098 [00:32<01:10, 10.72it/s]\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 348/1098 [00:33<01:10, 10.60it/s]\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 350/1098 [00:33<01:08, 10.87it/s]\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 352/1098 [00:33<01:10, 10.64it/s]\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 354/1098 [00:33<01:08, 10.86it/s]\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 356/1098 [00:33<01:09, 10.69it/s]\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 358/1098 [00:34<01:07, 10.93it/s]\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 360/1098 [00:34<01:07, 11.00it/s]\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 362/1098 [00:34<01:08, 10.82it/s]\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 364/1098 [00:34<01:07, 10.86it/s]\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 366/1098 [00:34<01:07, 10.85it/s]\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 368/1098 [00:35<01:07, 10.74it/s]\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 370/1098 [00:35<01:06, 10.94it/s]\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 372/1098 [00:35<01:07, 10.74it/s]\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 374/1098 [00:35<01:06, 10.86it/s]\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 376/1098 [00:35<01:07, 10.74it/s]\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 378/1098 [00:35<01:07, 10.74it/s]\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–      | 380/1098 [00:36<01:07, 10.58it/s]\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–      | 382/1098 [00:36<01:08, 10.48it/s]\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–      | 384/1098 [00:36<01:08, 10.46it/s]\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 386/1098 [00:36<01:08, 10.41it/s]\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 388/1098 [00:36<01:06, 10.62it/s]\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 390/1098 [00:37<01:05, 10.82it/s]\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 392/1098 [00:37<01:06, 10.66it/s]\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 394/1098 [00:37<01:05, 10.78it/s]\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 396/1098 [00:37<01:06, 10.53it/s]\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 398/1098 [00:37<01:05, 10.66it/s]\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–‹      | 400/1098 [00:38<01:05, 10.66it/s]\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 402/1098 [00:38<01:05, 10.62it/s]\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 404/1098 [00:38<01:06, 10.36it/s]\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 406/1098 [00:38<01:04, 10.66it/s]\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 408/1098 [00:38<01:05, 10.49it/s]\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 410/1098 [00:38<01:04, 10.74it/s]\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 412/1098 [00:39<01:02, 10.96it/s]\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 414/1098 [00:39<01:02, 11.00it/s]\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 416/1098 [00:39<01:03, 10.74it/s]\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 418/1098 [00:39<01:04, 10.62it/s]\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 420/1098 [00:39<01:03, 10.73it/s]\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 422/1098 [00:40<01:12,  9.34it/s]\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–Š      | 423/1098 [00:40<01:27,  7.71it/s]\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–Š      | 424/1098 [00:40<01:25,  7.87it/s]\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–Š      | 425/1098 [00:40<01:23,  8.02it/s]\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 426/1098 [00:40<01:28,  7.59it/s]\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 427/1098 [00:40<01:25,  7.85it/s]\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 428/1098 [00:41<01:26,  7.73it/s]\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 429/1098 [00:41<01:26,  7.74it/s]\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 430/1098 [00:41<01:21,  8.21it/s]\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 431/1098 [00:41<01:17,  8.65it/s]\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 432/1098 [00:41<01:14,  8.88it/s]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 434/1098 [00:41<01:09,  9.58it/s]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 436/1098 [00:41<01:06,  9.96it/s]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 437/1098 [00:41<01:07,  9.85it/s]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 438/1098 [00:42<01:07,  9.78it/s]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 439/1098 [00:42<01:08,  9.60it/s]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 440/1098 [00:42<01:08,  9.55it/s]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 441/1098 [00:42<01:08,  9.52it/s]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 442/1098 [00:42<01:11,  9.15it/s]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 443/1098 [00:42<01:13,  8.92it/s]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 444/1098 [00:42<01:14,  8.76it/s]\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 445/1098 [00:42<01:14,  8.73it/s]\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 446/1098 [00:42<01:14,  8.75it/s]\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 447/1098 [00:43<01:12,  8.95it/s]\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 448/1098 [00:43<01:11,  9.06it/s]\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 449/1098 [00:43<01:09,  9.28it/s]\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 450/1098 [00:43<01:10,  9.22it/s]\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 451/1098 [00:43<01:10,  9.17it/s]\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 452/1098 [00:43<01:10,  9.16it/s]\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 453/1098 [00:43<01:13,  8.79it/s]\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 454/1098 [00:43<01:14,  8.66it/s]\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 455/1098 [00:43<01:15,  8.52it/s]\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 456/1098 [00:44<01:15,  8.54it/s]\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 457/1098 [00:44<01:13,  8.68it/s]\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 458/1098 [00:44<01:12,  8.87it/s]\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 459/1098 [00:44<01:09,  9.18it/s]\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 460/1098 [00:44<01:09,  9.20it/s]\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 461/1098 [00:44<01:09,  9.18it/s]\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 462/1098 [00:44<01:09,  9.21it/s]\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 463/1098 [00:44<01:12,  8.81it/s]\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 464/1098 [00:44<01:13,  8.63it/s]\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 465/1098 [00:45<01:14,  8.55it/s]\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 466/1098 [00:45<01:11,  8.86it/s]\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 467/1098 [00:45<01:11,  8.86it/s]\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 468/1098 [00:45<01:11,  8.86it/s]\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 469/1098 [00:45<01:09,  9.07it/s]\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 470/1098 [00:45<01:10,  8.96it/s]\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 471/1098 [00:45<01:10,  8.89it/s]\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 472/1098 [00:45<01:11,  8.78it/s]\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 473/1098 [00:45<01:10,  8.80it/s]\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 474/1098 [00:46<01:10,  8.84it/s]\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 475/1098 [00:46<01:09,  8.96it/s]\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 476/1098 [00:46<01:08,  9.07it/s]\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 477/1098 [00:46<01:09,  8.91it/s]\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 478/1098 [00:46<01:08,  9.07it/s]\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 479/1098 [00:46<01:07,  9.12it/s]\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 480/1098 [00:46<01:08,  9.05it/s]\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 481/1098 [00:46<01:07,  9.18it/s]\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 482/1098 [00:46<01:07,  9.19it/s]\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 483/1098 [00:47<01:07,  9.04it/s]\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 484/1098 [00:47<01:06,  9.23it/s]\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 485/1098 [00:47<01:08,  8.98it/s]\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 486/1098 [00:47<01:08,  8.96it/s]\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 487/1098 [00:47<01:09,  8.84it/s]\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 488/1098 [00:47<01:07,  9.08it/s]\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 489/1098 [00:47<01:07,  9.07it/s]\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 490/1098 [00:47<01:06,  9.11it/s]\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 491/1098 [00:47<01:07,  9.02it/s]\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 492/1098 [00:48<01:08,  8.91it/s]\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 493/1098 [00:48<01:14,  8.10it/s]\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 494/1098 [00:48<01:11,  8.44it/s]\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 495/1098 [00:48<01:11,  8.48it/s]\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 496/1098 [00:48<01:09,  8.70it/s]\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 497/1098 [00:48<01:08,  8.77it/s]\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 498/1098 [00:48<01:07,  8.89it/s]\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 499/1098 [00:48<01:06,  9.06it/s]\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 500/1098 [00:49<01:06,  9.02it/s]\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 501/1098 [00:49<01:06,  8.94it/s]\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 502/1098 [00:49<01:05,  9.10it/s]\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 503/1098 [00:49<01:04,  9.27it/s]\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 504/1098 [00:49<01:05,  9.12it/s]\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 505/1098 [00:49<01:05,  9.02it/s]\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 506/1098 [00:49<01:05,  8.98it/s]\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 507/1098 [00:49<01:06,  8.92it/s]\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 508/1098 [00:49<01:05,  8.96it/s]\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 509/1098 [00:50<01:04,  9.14it/s]\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 510/1098 [00:50<01:04,  9.11it/s]\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 512/1098 [00:50<01:00,  9.76it/s]\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 513/1098 [00:50<01:01,  9.58it/s]\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 515/1098 [00:50<00:59,  9.80it/s]\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 516/1098 [00:50<01:01,  9.42it/s]\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 517/1098 [00:50<01:02,  9.29it/s]\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 518/1098 [00:50<01:02,  9.30it/s]\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 519/1098 [00:51<01:02,  9.31it/s]\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 520/1098 [00:51<01:02,  9.22it/s]\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 521/1098 [00:51<01:02,  9.18it/s]\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 522/1098 [00:51<01:03,  9.05it/s]\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 523/1098 [00:51<01:04,  8.92it/s]\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 524/1098 [00:51<01:04,  8.87it/s]\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 525/1098 [00:51<01:02,  9.10it/s]\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 526/1098 [00:51<01:02,  9.14it/s]\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 527/1098 [00:51<01:01,  9.22it/s]\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 528/1098 [00:52<01:02,  9.10it/s]\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 529/1098 [00:52<01:01,  9.21it/s]\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 531/1098 [00:52<00:58,  9.75it/s]\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 532/1098 [00:52<00:58,  9.66it/s]\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 533/1098 [00:52<00:58,  9.65it/s]\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 534/1098 [00:52<01:01,  9.24it/s]\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 535/1098 [00:52<01:02,  9.02it/s]\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 536/1098 [00:52<01:02,  9.02it/s]\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 537/1098 [00:53<01:01,  9.12it/s]\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 538/1098 [00:53<01:01,  9.13it/s]\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 539/1098 [00:53<01:01,  9.11it/s]\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 540/1098 [00:53<01:03,  8.85it/s]\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 541/1098 [00:53<01:03,  8.74it/s]\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 542/1098 [00:53<01:03,  8.81it/s]\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 543/1098 [00:53<01:02,  8.88it/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 544/1098 [00:53<01:00,  9.13it/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 545/1098 [00:53<00:59,  9.23it/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 547/1098 [00:54<00:56,  9.72it/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 548/1098 [00:54<00:56,  9.70it/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 550/1098 [00:54<00:53, 10.23it/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 552/1098 [00:54<00:51, 10.59it/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 554/1098 [00:54<00:52, 10.38it/s]\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 556/1098 [00:54<00:50, 10.71it/s]\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 558/1098 [00:55<00:50, 10.65it/s]\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 560/1098 [00:55<00:50, 10.60it/s]\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 562/1098 [00:55<00:50, 10.51it/s]\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 564/1098 [00:55<00:50, 10.64it/s]\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 566/1098 [00:55<00:49, 10.71it/s]\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 568/1098 [00:56<00:50, 10.40it/s]\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 570/1098 [00:56<00:50, 10.38it/s]\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 572/1098 [00:56<00:50, 10.32it/s]\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 574/1098 [00:56<00:50, 10.47it/s]\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 576/1098 [00:56<00:53,  9.78it/s]\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 578/1098 [00:57<00:52,  9.95it/s]\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 580/1098 [00:57<00:50, 10.28it/s]\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 582/1098 [00:57<00:49, 10.39it/s]\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 584/1098 [00:57<00:49, 10.35it/s]\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 586/1098 [00:57<00:48, 10.57it/s]\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 588/1098 [00:58<00:48, 10.58it/s]\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 590/1098 [00:58<00:47, 10.65it/s]\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 592/1098 [00:58<00:46, 10.81it/s]\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 594/1098 [00:58<00:48, 10.48it/s]\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 596/1098 [00:58<00:47, 10.58it/s]\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 598/1098 [00:58<00:45, 10.90it/s]\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 600/1098 [00:59<00:45, 10.89it/s]\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 602/1098 [00:59<00:45, 10.89it/s]\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 604/1098 [00:59<00:46, 10.73it/s]\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 606/1098 [00:59<00:46, 10.66it/s]\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 608/1098 [00:59<00:45, 10.74it/s]\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 610/1098 [01:00<00:47, 10.35it/s]\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 612/1098 [01:00<00:45, 10.66it/s]\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 614/1098 [01:00<00:47, 10.25it/s]\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 616/1098 [01:00<00:46, 10.38it/s]\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 618/1098 [01:00<00:45, 10.50it/s]\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 620/1098 [01:01<00:44, 10.66it/s]\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 622/1098 [01:01<00:45, 10.53it/s]\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 624/1098 [01:01<00:44, 10.65it/s]\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 626/1098 [01:01<00:44, 10.60it/s]\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 628/1098 [01:01<00:43, 10.70it/s]\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 630/1098 [01:01<00:43, 10.85it/s]\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 632/1098 [01:02<00:42, 10.85it/s]\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 634/1098 [01:02<00:43, 10.78it/s]\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 636/1098 [01:02<00:42, 10.98it/s]\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 638/1098 [01:02<00:42, 10.92it/s]\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 640/1098 [01:02<00:41, 10.95it/s]\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 642/1098 [01:03<00:42, 10.79it/s]\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 644/1098 [01:03<00:41, 10.85it/s]\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 646/1098 [01:03<00:43, 10.50it/s]\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 648/1098 [01:03<00:44, 10.15it/s]\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 650/1098 [01:03<00:45,  9.93it/s]\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 651/1098 [01:03<00:45,  9.76it/s]\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 652/1098 [01:04<00:45,  9.70it/s]\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 653/1098 [01:04<00:46,  9.49it/s]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 654/1098 [01:04<00:47,  9.27it/s]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 655/1098 [01:04<00:48,  9.18it/s]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 656/1098 [01:04<00:49,  8.98it/s]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 657/1098 [01:04<00:49,  8.91it/s]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 658/1098 [01:04<00:49,  8.95it/s]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 659/1098 [01:04<00:49,  8.83it/s]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 660/1098 [01:05<00:49,  8.78it/s]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 661/1098 [01:05<00:49,  8.80it/s]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 662/1098 [01:05<00:51,  8.46it/s]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 663/1098 [01:05<00:55,  7.87it/s]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 664/1098 [01:05<00:56,  7.62it/s]\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 665/1098 [01:05<00:54,  8.02it/s]\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 666/1098 [01:05<00:52,  8.30it/s]\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 667/1098 [01:05<00:50,  8.58it/s]\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 668/1098 [01:05<00:50,  8.56it/s]\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 669/1098 [01:06<00:49,  8.74it/s]\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 670/1098 [01:06<00:47,  8.95it/s]\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 671/1098 [01:06<00:48,  8.89it/s]\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 672/1098 [01:06<00:47,  8.89it/s]\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 674/1098 [01:06<00:44,  9.50it/s]\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 675/1098 [01:06<00:47,  8.97it/s]\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 676/1098 [01:06<00:47,  8.96it/s]\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 677/1098 [01:06<00:46,  9.01it/s]\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 678/1098 [01:07<00:45,  9.22it/s]\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 679/1098 [01:07<00:44,  9.34it/s]\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 681/1098 [01:07<00:41, 10.01it/s]\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 682/1098 [01:07<00:41,  9.98it/s]\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 684/1098 [01:07<00:37, 10.91it/s]\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 686/1098 [01:07<00:36, 11.29it/s]\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 688/1098 [01:07<00:35, 11.52it/s]\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 690/1098 [01:08<00:36, 11.12it/s]\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 692/1098 [01:08<00:37, 10.96it/s]\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 694/1098 [01:08<00:42,  9.50it/s]\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 695/1098 [01:08<00:45,  8.95it/s]\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 696/1098 [01:08<00:45,  8.90it/s]\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 697/1098 [01:08<00:45,  8.81it/s]\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 698/1098 [01:09<00:45,  8.73it/s]\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 699/1098 [01:09<00:46,  8.56it/s]\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 700/1098 [01:09<00:46,  8.59it/s]\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 701/1098 [01:09<00:46,  8.63it/s]\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 702/1098 [01:09<00:44,  8.81it/s]\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 703/1098 [01:09<00:46,  8.55it/s]\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 704/1098 [01:09<00:47,  8.35it/s]\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 705/1098 [01:09<00:47,  8.20it/s]\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 706/1098 [01:10<00:46,  8.38it/s]\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 707/1098 [01:10<00:46,  8.39it/s]\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 708/1098 [01:10<00:47,  8.24it/s]\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 709/1098 [01:10<00:46,  8.33it/s]\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 710/1098 [01:10<00:46,  8.40it/s]\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 711/1098 [01:10<00:44,  8.61it/s]\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 713/1098 [01:10<00:41,  9.17it/s]\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 714/1098 [01:10<00:41,  9.18it/s]\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 715/1098 [01:11<00:41,  9.31it/s]\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 716/1098 [01:11<00:40,  9.46it/s]\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 717/1098 [01:11<00:39,  9.54it/s]\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 718/1098 [01:11<00:39,  9.59it/s]\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 719/1098 [01:11<00:39,  9.48it/s]\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 720/1098 [01:11<00:40,  9.45it/s]\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 721/1098 [01:11<00:39,  9.54it/s]\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 723/1098 [01:11<00:35, 10.50it/s]\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 725/1098 [01:11<00:33, 11.27it/s]\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 727/1098 [01:12<00:32, 11.42it/s]\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 729/1098 [01:12<00:32, 11.27it/s]\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 731/1098 [01:12<00:32, 11.24it/s]\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 733/1098 [01:12<00:32, 11.18it/s]\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 735/1098 [01:12<00:33, 10.96it/s]\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 737/1098 [01:13<00:32, 11.06it/s]\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 739/1098 [01:13<00:33, 10.78it/s]\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 741/1098 [01:13<00:33, 10.68it/s]\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 743/1098 [01:13<00:33, 10.60it/s]\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 745/1098 [01:13<00:33, 10.59it/s]\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 747/1098 [01:14<00:32, 10.71it/s]\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 749/1098 [01:14<00:32, 10.89it/s]\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 751/1098 [01:14<00:34, 10.18it/s]\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 753/1098 [01:14<00:34, 10.10it/s]\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 755/1098 [01:14<00:32, 10.41it/s]\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 757/1098 [01:14<00:32, 10.56it/s]\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 759/1098 [01:15<00:32, 10.54it/s]\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 761/1098 [01:15<00:32, 10.47it/s]\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 763/1098 [01:15<00:32, 10.36it/s]\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 765/1098 [01:15<00:31, 10.50it/s]\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 767/1098 [01:15<00:32, 10.11it/s]\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 769/1098 [01:16<00:32, 10.23it/s]\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 771/1098 [01:16<00:39,  8.34it/s]\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 772/1098 [01:16<00:38,  8.51it/s]\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 773/1098 [01:16<00:37,  8.63it/s]\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 774/1098 [01:16<00:36,  8.90it/s]\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 775/1098 [01:16<00:35,  9.07it/s]\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 777/1098 [01:17<00:33,  9.56it/s]\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 778/1098 [01:17<00:33,  9.64it/s]\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 779/1098 [01:17<00:32,  9.71it/s]\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 780/1098 [01:17<00:33,  9.61it/s]\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 782/1098 [01:17<00:31, 10.14it/s]\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 784/1098 [01:17<00:29, 10.53it/s]\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 786/1098 [01:17<00:29, 10.53it/s]\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 788/1098 [01:18<00:29, 10.60it/s]\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 790/1098 [01:18<00:29, 10.56it/s]\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 792/1098 [01:18<00:28, 10.67it/s]\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 794/1098 [01:18<00:27, 10.90it/s]\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 796/1098 [01:18<00:27, 11.00it/s]\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 798/1098 [01:19<00:27, 10.88it/s]\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 800/1098 [01:19<00:27, 10.81it/s]\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 802/1098 [01:19<00:27, 10.94it/s]\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 804/1098 [01:19<00:27, 10.77it/s]\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 806/1098 [01:19<00:27, 10.80it/s]\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 808/1098 [01:20<00:27, 10.66it/s]\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 810/1098 [01:20<00:26, 10.85it/s]\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 812/1098 [01:20<00:26, 10.88it/s]\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 814/1098 [01:20<00:26, 10.67it/s]\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 816/1098 [01:20<00:26, 10.72it/s]\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 818/1098 [01:20<00:25, 10.85it/s]\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 820/1098 [01:21<00:25, 10.78it/s]\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 822/1098 [01:21<00:25, 10.62it/s]\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 824/1098 [01:21<00:25, 10.67it/s]\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 826/1098 [01:21<00:25, 10.85it/s]\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 828/1098 [01:21<00:25, 10.68it/s]\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 830/1098 [01:22<00:24, 10.82it/s]\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 832/1098 [01:22<00:24, 10.82it/s]\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 834/1098 [01:22<00:24, 10.76it/s]\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 836/1098 [01:22<00:24, 10.85it/s]\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 838/1098 [01:22<00:24, 10.78it/s]\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 840/1098 [01:22<00:24, 10.73it/s]\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 842/1098 [01:23<00:25, 10.03it/s]\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 844/1098 [01:23<00:24, 10.19it/s]\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 846/1098 [01:23<00:24, 10.46it/s]\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 848/1098 [01:23<00:24, 10.39it/s]\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 850/1098 [01:23<00:23, 10.57it/s]\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 852/1098 [01:24<00:23, 10.63it/s]\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 854/1098 [01:24<00:22, 10.63it/s]\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 856/1098 [01:24<00:22, 10.64it/s]\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 858/1098 [01:24<00:22, 10.66it/s]\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 860/1098 [01:24<00:22, 10.57it/s]\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 862/1098 [01:25<00:22, 10.52it/s]\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 864/1098 [01:25<00:21, 10.67it/s]\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 866/1098 [01:25<00:21, 10.85it/s]\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 868/1098 [01:25<00:21, 10.64it/s]\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 870/1098 [01:25<00:21, 10.79it/s]\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 872/1098 [01:26<00:21, 10.64it/s]\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 874/1098 [01:26<00:20, 10.76it/s]\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 876/1098 [01:26<00:20, 10.70it/s]\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 878/1098 [01:26<00:20, 10.74it/s]\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 880/1098 [01:26<00:20, 10.57it/s]\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 882/1098 [01:26<00:20, 10.42it/s]\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 884/1098 [01:27<00:20, 10.55it/s]\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 886/1098 [01:27<00:19, 10.71it/s]\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 888/1098 [01:27<00:19, 10.55it/s]\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 890/1098 [01:27<00:19, 10.72it/s]\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 892/1098 [01:27<00:19, 10.72it/s]\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 894/1098 [01:28<00:18, 10.77it/s]\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 896/1098 [01:28<00:18, 10.94it/s]\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 898/1098 [01:28<00:18, 10.78it/s]\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 900/1098 [01:28<00:18, 10.87it/s]\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 902/1098 [01:28<00:18, 10.66it/s]\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 904/1098 [01:29<00:18, 10.65it/s]\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 906/1098 [01:29<00:17, 10.72it/s]\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 908/1098 [01:29<00:17, 10.88it/s]\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 910/1098 [01:29<00:17, 10.64it/s]\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 912/1098 [01:29<00:17, 10.54it/s]\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 914/1098 [01:29<00:17, 10.79it/s]\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 916/1098 [01:30<00:16, 10.91it/s]\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 918/1098 [01:30<00:16, 10.94it/s]\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 920/1098 [01:30<00:16, 10.83it/s]\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 922/1098 [01:30<00:16, 10.84it/s]\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 924/1098 [01:30<00:16, 10.50it/s]\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 926/1098 [01:31<00:16, 10.46it/s]\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 928/1098 [01:31<00:16, 10.17it/s]\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 930/1098 [01:31<00:17,  9.60it/s]\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 931/1098 [01:31<00:17,  9.62it/s]\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 933/1098 [01:31<00:16, 10.15it/s]\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 935/1098 [01:31<00:15, 10.26it/s]\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 937/1098 [01:32<00:15, 10.35it/s]\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 939/1098 [01:32<00:15, 10.42it/s]\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 941/1098 [01:32<00:15, 10.44it/s]\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 943/1098 [01:32<00:14, 10.70it/s]\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 945/1098 [01:32<00:14, 10.51it/s]\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 947/1098 [01:33<00:14, 10.52it/s]\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 949/1098 [01:33<00:13, 10.79it/s]\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 951/1098 [01:33<00:13, 10.91it/s]\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 953/1098 [01:33<00:13, 10.95it/s]\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 955/1098 [01:33<00:13, 10.76it/s]\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 957/1098 [01:34<00:13, 10.52it/s]\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 959/1098 [01:34<00:13, 10.53it/s]\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 961/1098 [01:34<00:12, 10.65it/s]\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 963/1098 [01:34<00:12, 10.63it/s]\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 965/1098 [01:34<00:12, 10.76it/s]\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 967/1098 [01:34<00:12, 10.84it/s]\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 969/1098 [01:35<00:12, 10.69it/s]\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 971/1098 [01:35<00:12, 10.57it/s]\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 973/1098 [01:35<00:11, 10.70it/s]\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 975/1098 [01:35<00:11, 10.81it/s]\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 977/1098 [01:35<00:11, 10.84it/s]\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 979/1098 [01:36<00:11, 10.63it/s]\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 981/1098 [01:36<00:10, 10.74it/s]\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 983/1098 [01:36<00:10, 10.94it/s]\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 985/1098 [01:36<00:10, 10.85it/s]\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 987/1098 [01:36<00:10, 10.87it/s]\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 989/1098 [01:37<00:10, 10.83it/s]\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 991/1098 [01:37<00:09, 10.79it/s]\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 993/1098 [01:37<00:09, 10.83it/s]\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 995/1098 [01:37<00:09, 10.84it/s]\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 997/1098 [01:37<00:09, 10.66it/s]\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 999/1098 [01:37<00:09, 10.81it/s]\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1001/1098 [01:38<00:09, 10.75it/s]\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1003/1098 [01:38<00:08, 10.64it/s]\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1005/1098 [01:38<00:08, 10.82it/s]\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1007/1098 [01:38<00:08, 10.68it/s]\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1009/1098 [01:38<00:08, 10.17it/s]\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1011/1098 [01:39<00:08, 10.35it/s]\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1013/1098 [01:39<00:08, 10.37it/s]\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1015/1098 [01:39<00:08, 10.29it/s]\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1017/1098 [01:39<00:07, 10.61it/s]\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1019/1098 [01:39<00:07, 10.56it/s]\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1021/1098 [01:40<00:07, 10.74it/s]\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1023/1098 [01:40<00:07, 10.56it/s]\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1025/1098 [01:40<00:06, 10.73it/s]\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1027/1098 [01:40<00:06, 10.56it/s]\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1029/1098 [01:40<00:06, 10.59it/s]\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1031/1098 [01:40<00:06, 10.41it/s]\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1033/1098 [01:41<00:06,  9.97it/s]\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1035/1098 [01:41<00:06,  9.77it/s]\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1036/1098 [01:41<00:06,  9.79it/s]\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1038/1098 [01:41<00:05, 10.10it/s]\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1040/1098 [01:41<00:05, 10.33it/s]\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1042/1098 [01:42<00:05, 10.34it/s]\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1044/1098 [01:42<00:05, 10.55it/s]\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1046/1098 [01:42<00:04, 10.51it/s]\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1048/1098 [01:42<00:04, 10.43it/s]\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1050/1098 [01:42<00:04, 10.78it/s]\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1052/1098 [01:43<00:04, 10.78it/s]\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1054/1098 [01:43<00:04, 10.71it/s]\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1056/1098 [01:43<00:03, 10.57it/s]\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1058/1098 [01:43<00:03, 10.52it/s]\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1060/1098 [01:43<00:03, 10.79it/s]\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1062/1098 [01:43<00:03, 10.61it/s]\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1064/1098 [01:44<00:03, 10.71it/s]\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1066/1098 [01:44<00:02, 10.89it/s]\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1068/1098 [01:44<00:02, 10.67it/s]\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1070/1098 [01:44<00:02, 10.56it/s]\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1072/1098 [01:44<00:02, 10.57it/s]\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1074/1098 [01:45<00:02, 10.42it/s]\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1076/1098 [01:45<00:02, 10.29it/s]\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1078/1098 [01:45<00:01, 10.33it/s]\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1080/1098 [01:45<00:01, 10.44it/s]\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1082/1098 [01:45<00:01, 10.32it/s]\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1084/1098 [01:46<00:01,  9.62it/s]\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1085/1098 [01:46<00:01,  9.63it/s]\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1086/1098 [01:46<00:01,  9.58it/s]\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1088/1098 [01:46<00:01,  9.83it/s]\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1089/1098 [01:46<00:00,  9.58it/s]\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1090/1098 [01:46<00:00,  9.50it/s]\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1091/1098 [01:46<00:00,  9.61it/s]\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1092/1098 [01:46<00:00,  9.52it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1093/1098 [01:47<00:00,  9.47it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1095/1098 [01:47<00:00, 10.00it/s]\n",
      "                                               t/s]\n",
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9710053205490112, 'eval_runtime': 107.6341, 'eval_samples_per_second': 20.402, 'eval_steps_per_second': 10.201, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:07<00:00,  1.74s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1098/1098 [01:47<00:00, 10.24it/s]\n",
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 127.4863, 'train_samples_per_second': 0.157, 'train_steps_per_second': 0.078, 'train_loss': 1.107685375213623, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:07<00:00, 12.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-14 01:03:48,249: INFO: 1819293319: Model saved to artifacts\\training\\trained_model]\n",
      "[2025-03-14 01:03:48,269: INFO: 1819293319: Tokenizer saved to artifacts\\training\\trained_model]\n",
      "Datasets Directory: artifacts\\feature_engineering\\datasets\n",
      "[2025-03-14 01:03:48,614: INFO: 1819293319: Datasets loaded from artifacts\\feature_engineering\\datasets]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1098/1098 [02:02<00:00,  8.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-14 01:05:51,319: INFO: 1819293319: Evaluation Results:]\n",
      "[2025-03-14 01:05:51,319: INFO: 1819293319:   - Loss: 0.9562]\n",
      "[2025-03-14 01:05:51,321: INFO: 1819293319:   - Runtime: 122.45 seconds]\n",
      "[2025-03-14 01:05:51,322: INFO: 1819293319:   - Samples per Second: 17.93]\n",
      "[2025-03-14 01:05:51,324: INFO: 1819293319:   - Steps per Second: 8.97]\n",
      "[2025-03-14 01:05:51,325: INFO: 1819293319:   - Epoch: N/A]\n"
     ]
    }
   ],
   "source": [
    "from accelerate import PartialState\n",
    "accelerator_state_kwargs = {\"enabled\": True, \"use_configured_state\": False}\n",
    "\n",
    "# Initialize PartialState\n",
    "\n",
    "# from textClassifier.components import sentimentsDataset\n",
    "if __name__ == \"__main__\":\n",
    "    # import mlflow\n",
    "    # mlflow.set_tracking_uri(None)\n",
    "    # Initialize ConfigurationManager\n",
    "    config_manager = ConfigurationManager()\n",
    "    # Get the model training config\n",
    "    training_config = config_manager.get_training_config()\n",
    "\n",
    "    # Initialize ModelTraining\n",
    "    model_training = ModelTraining(config=training_config)\n",
    "\n",
    "    # Train the model\n",
    "    model_training.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    model_training.evaluate()\n",
    "    \n",
    "    partial_state = PartialState()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textClassifier.components import SentimentDataset  # Import the SentimentDataset class\n",
    "# import torch\n",
    "# C:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\artifacts\\feature_engineering\\datasets\\train_dataset.pt\n",
    "\n",
    "# # Load your data and create datasets\n",
    "# train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "# val_dataset = SentimentDataset(val_encodings, val_labels)\n",
    "# test_dataset = SentimentDataset(test_encodings, test_labels)\n",
    "\n",
    "# # Save the datasets\n",
    "# torch.save(train_dataset, \"artifacts/feature_engineering/train_dataset.pt\")\n",
    "# torch.save(val_dataset, \"artifacts/feature_engineering/val_dataset.pt\")\n",
    "# torch.save(test_dataset, \"artifacts/feature_engineering/test_dataset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# class SentimentDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, encodings, labels):\n",
    "#         self.encodings = encodings\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "#         item['labels'] = torch.tensor(self.labels[idx])\n",
    "#         return item\n",
    "\n",
    "# def convert_to_dicts(tokenized_texts):\n",
    "#     \"\"\"\n",
    "#     Converts a list of tokenized texts into a dictionary of input IDs and attention masks.\n",
    "\n",
    "#     Args:\n",
    "#         tokenized_texts (list): List of tokenized texts (each is a dictionary with 'input_ids' and 'attention_mask').\n",
    "\n",
    "#     Returns:\n",
    "#         dict: A dictionary containing 'input_ids' and 'attention_mask' as lists.\n",
    "#     \"\"\"\n",
    "#     input_ids = [d['input_ids'].squeeze(0) for d in tokenized_texts]  # Remove batch dimension\n",
    "#     attention_masks = [d['attention_mask'].squeeze(0) for d in tokenized_texts]  # Remove batch dimension\n",
    "#     return {'input_ids': input_ids, 'attention_mask': attention_masks}\n",
    "\n",
    "\n",
    "# def create_datasets(train_texts, train_labels, val_texts, val_labels, test_texts, test_labels):\n",
    "#     \"\"\"\n",
    "#     Creates SentimentDataset objects for train, validation, and test splits.\n",
    "\n",
    "#     Args:\n",
    "#         train_texts (list): List of tokenized texts for the training set.\n",
    "#         train_labels (list): List of labels for the training set.\n",
    "#         val_texts (list): List of tokenized texts for the validation set.\n",
    "#         val_labels (list): List of labels for the validation set.\n",
    "#         test_texts (list): List of tokenized texts for the test set.\n",
    "#         test_labels (list): List of labels for the test set.\n",
    "\n",
    "#     Returns:\n",
    "#         train_dataset, val_dataset, test_dataset: SentimentDataset objects for each split.\n",
    "#     \"\"\"\n",
    "#     # Convert tokenized texts to encodings\n",
    "#     train_encodings = convert_to_dicts(train_texts)\n",
    "#     val_encodings = convert_to_dicts(val_texts)\n",
    "#     test_encodings = convert_to_dicts(test_texts)\n",
    "\n",
    "#     # Create SentimentDataset objects\n",
    "#     train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "#     val_dataset = SentimentDataset(val_encodings, val_labels)\n",
    "#     test_dataset = SentimentDataset(test_encodings, test_labels)\n",
    "\n",
    "#     return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# def save_datasets(self, train_dataset, val_dataset, test_dataset):\n",
    "#         \"\"\"\n",
    "#         Saves the train, validation, and test datasets to the specified directory.\n",
    "\n",
    "#         Args:\n",
    "#             train_dataset: Training dataset.\n",
    "#             val_dataset: Validation dataset.\n",
    "#             test_dataset: Test dataset.\n",
    "#         \"\"\"\n",
    "#         # Create the directory if it doesn't exist\n",
    "#         datasets_dir = Path(self.config.datasets_dir)\n",
    "#         datasets_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#         # Save the datasets\n",
    "#         torch.save(train_dataset, datasets_dir / \"train_dataset.pt\")\n",
    "#         torch.save(val_dataset, datasets_dir / \"val_dataset.pt\")\n",
    "#         torch.save(test_dataset, datasets_dir / \"test_dataset.pt\")\n",
    "\n",
    "#         logger.info(f\"Datasets saved to {datasets_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "class SentimentInference:\n",
    "    def __init__(self, model_path: str):\n",
    "        \"\"\"\n",
    "        Initializes the SentimentInference class.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): Path to the trained model and tokenizer.\n",
    "        \"\"\"\n",
    "        self.model_path = Path(model_path)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        self.sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "    def predict_sentiment(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Predicts the sentiment of the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text for sentiment prediction.\n",
    "\n",
    "        Returns:\n",
    "            str: Predicted sentiment ('negative', 'neutral', 'positive').\n",
    "        \"\"\"\n",
    "        # Tokenize the input text\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        \n",
    "        # Get model predictions\n",
    "        outputs = self.model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "        \n",
    "        # Map the predicted class to sentiment\n",
    "        return self.sentiment_map[predicted_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows-SSD\n",
      " Volume Serial Number is 5824-84FE\n",
      "\n",
      " Directory of c:\\Users\\prass\\OneDrive\\Desktop\\practise\\new_env\\text-classification-using-BERT\n",
      "\n",
      "02/17/2025  03:23 PM    <DIR>          .\n",
      "02/03/2025  10:10 PM    <DIR>          ..\n",
      "02/03/2025  10:15 PM    <DIR>          .github\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/05/2025  11:14 AM             3,597 .gitignore\n",
      "03/04/2025  09:54 PM    <DIR>          artifacts\n",
      "02/03/2025  10:15 PM    <DIR>          config\n",
      "02/03/2025  10:15 PM                 0 dvc.yaml\n",
      "02/03/2025  09:43 PM             1,087 LICENSE\n",
      "02/03/2025  10:50 PM    <DIR>          logs\n",
      "02/05/2025  11:31 AM               440 main.py\n",
      "03/04/2025  08:59 PM               170 params.yaml\n",
      "02/03/2025  09:43 PM                32 README.md\n",
      "02/03/2025  10:26 PM               248 requirements.txt\n",
      "03/04/2025  09:08 PM    <DIR>          research\n",
      "02/03/2025  10:16 PM               825 setup.py\n",
      "02/03/2025  10:28 PM    <DIR>          src\n",
      "02/07/2025  10:25 AM             1,344 template.py\n",
      "02/05/2025  11:31 AM    <DIR>          templates\n",
      "03/03/2025  04:07 PM    <DIR>          wandb\n",
      "               9 File(s)          7,743 bytes\n",
      "              10 Dir(s)  251,240,882,176 bytes free\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\\\\tokenizer_config.json',\n",
       " 'C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\\\\special_tokens_map.json',\n",
       " 'C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\\\\vocab.txt',\n",
       " 'C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\\\\added_tokens.json',\n",
       " 'C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\\\\tokenizer.json')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# # Load the model and tokenizer\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# # Save the model and tokenizer\n",
    "# model.save_pretrained(\"C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\")\n",
    "# tokenizer.save_pretrained(\"C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "class SentimentInference:\n",
    "    def __init__(self, model_path: str):\n",
    "        \"\"\"\n",
    "        Initializes the SentimentInference class.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): Path to the trained model and tokenizer.\n",
    "        \"\"\"\n",
    "        self.model_path = Path(model_path)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        self.sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "    def predict_sentiment(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Predicts the sentiment of the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text for sentiment prediction.\n",
    "\n",
    "        Returns:\n",
    "            str: Predicted sentiment ('negative', 'neutral', 'positive').\n",
    "        \"\"\"\n",
    "        # Tokenize the input text\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        \n",
    "        # Get model predictions\n",
    "        outputs = self.model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "        \n",
    "        # Map the predicted class to sentiment\n",
    "        return self.sentiment_map[predicted_class]\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Provide the path to your trained model and tokenizer\n",
    "#     model_path = \"C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\"\n",
    "    \n",
    "#     # Initialize the SentimentInference class\n",
    "#     pred = SentimentInference(model_path)\n",
    "    \n",
    "#     # Test the prediction\n",
    "#     test = pred.predict_sentiment('I hate flying with airlines')\n",
    "#     print(f\"Predicted Sentiment: {test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_sentiment(text):\n",
    "#     '''Function to predict the sentiment of a given text using a pre-trained BERT model.\n",
    "#     Args: the input text for sentiment prediction.\n",
    "#     Returns: the predicted sentiment ('negative', 'neutral', 'positive').\n",
    "#     '''\n",
    "\n",
    "#     inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "#     outputs = model(**inputs)\n",
    "#     predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "#     predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "#     sentiment = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "#     return sentiment[predicted_class]\n",
    "\n",
    "# # Example prediction\n",
    "# example_text = \"I hate flying with this airline!\"\n",
    "# predicted_sentiment = predict_sentiment(example_text)\n",
    "# print(f\"Predicted Sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerate version: 0.26.0\n",
      "Transformers version: 4.49.0\n"
     ]
    }
   ],
   "source": [
    "# import accelerate\n",
    "# import transformers\n",
    "\n",
    "# print(\"Accelerate version:\", accelerate.__version__)\n",
    "# print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is for reference to basic web page to predict the predictions\n",
    "# from flask import Flask, request, jsonify, render_template_string\n",
    "\n",
    "# # Initialize the Flask app\n",
    "# app = Flask(__name__)\n",
    "\n",
    "# # Dummy SentimentInference class for demonstration\n",
    "# class SentimentInference:\n",
    "#     def __init__(self, model_path):\n",
    "#         self.model_path = model_path\n",
    "\n",
    "#     def predict_sentiment(self, text):\n",
    "#         Dummy sentiment prediction logic\n",
    "#         if \"good\" in text.lower():\n",
    "#             return \"Positive\"\n",
    "#         elif \"bad\" in text.lower():\n",
    "#             return \"Negative\"\n",
    "#         else:\n",
    "#             return \"Neutral\"\n",
    "#         # try:\n",
    "#         #     sentiment = sentiment_inference.predict_sentiment(text)\n",
    "#         #     return jsonify({\"sentiment\": sentiment})\n",
    "#         # except Exception as e:\n",
    "#         #     return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "# # Initialize the SentimentInference class\n",
    "# model_path = \"C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\"\n",
    "# sentiment_inference = SentimentInference(model_path)\n",
    "\n",
    "# # Home page route\n",
    "# @app.route(\"/\", methods=[\"GET\"])\n",
    "# def home():\n",
    "#     return \"\"\"\n",
    "#     <h1>Welcome to Sentiment Analysis</h1>\n",
    "#     <p>Click <a href=\"/predict\">here</a> to go to the prediction page.</p>\n",
    "#     \"\"\"\n",
    "\n",
    "# # Predict page route\n",
    "# @app.route(\"/predict\", methods=[\"GET\", \"POST\"])\n",
    "# def predict():\n",
    "#     if request.method == \"GET\":\n",
    "#         # Render a basic HTML form for text input\n",
    "#         return render_template_string('''\n",
    "#         <h1>Sentiment Prediction</h1>\n",
    "#         <form method=\"POST\">\n",
    "#             <label for=\"text\">Enter your text:</label><br>\n",
    "#             <textarea id=\"text\" name=\"text\" rows=\"4\" cols=\"50\"></textarea><br><br>\n",
    "#             <input type=\"submit\" value=\"Predict\">\n",
    "#         </form>\n",
    "#         ''')\n",
    "#     elif request.method == \"POST\":\n",
    "#         # Get the input text from the form\n",
    "#         text = request.form.get(\"text\")\n",
    "        \n",
    "#         # Validate the input\n",
    "#         if not text:\n",
    "#             return jsonify({\"error\": \"No text provided\"}), 400\n",
    "        \n",
    "#         # Predict the sentiment\n",
    "#         try:\n",
    "#             sentiment = sentiment_inference.predict_sentiment(text)\n",
    "#             return jsonify({\"sentiment\": sentiment})\n",
    "#         except Exception as e:\n",
    "#             return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Run the Flask app\n",
    "#     app.run(host=\"0.0.0.0\", port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference pipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n",
      "[2025-03-10 11:10:27,310: INFO: _internal: \u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.1.117:5000]\n",
      "[2025-03-10 11:10:27,312: INFO: _internal: \u001b[33mPress CTRL+C to quit\u001b[0m]\n",
      "[2025-03-10 11:10:32,601: INFO: _internal: 127.0.0.1 - - [10/Mar/2025 11:10:32] \"GET / HTTP/1.1\" 200 -]\n",
      "[2025-03-10 11:10:34,995: INFO: _internal: 127.0.0.1 - - [10/Mar/2025 11:10:34] \"GET /predict HTTP/1.1\" 200 -]\n",
      "[2025-03-10 11:10:42,950: INFO: _internal: 127.0.0.1 - - [10/Mar/2025 11:10:42] \"POST /predict HTTP/1.1\" 200 -]\n",
      "[2025-03-10 11:10:48,498: INFO: _internal: 127.0.0.1 - - [10/Mar/2025 11:10:48] \"GET / HTTP/1.1\" 200 -]\n",
      "[2025-03-10 11:10:51,202: INFO: _internal: 127.0.0.1 - - [10/Mar/2025 11:10:51] \"GET /predict HTTP/1.1\" 200 -]\n",
      "[2025-03-10 11:11:16,212: INFO: _internal: 127.0.0.1 - - [10/Mar/2025 11:11:16] \"POST /predict HTTP/1.1\" 200 -]\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, render_template_string\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize the Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Your SentimentInference class\n",
    "class SentimentInference:\n",
    "    def __init__(self, model_path: str):\n",
    "        \"\"\"\n",
    "        Initializes the SentimentInference class.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): Path to the trained model and tokenizer.\n",
    "        \"\"\"\n",
    "        self.model_path = Path(model_path)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        self.sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "    def predict_sentiment(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Predicts the sentiment of the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text for sentiment prediction.\n",
    "\n",
    "        Returns:\n",
    "            str: Predicted sentiment ('negative', 'neutral', 'positive').\n",
    "        \"\"\"\n",
    "        # Tokenize the input text\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        \n",
    "        # Get model predictions\n",
    "        outputs = self.model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "        \n",
    "        # Map the predicted class to sentiment\n",
    "        return self.sentiment_map[predicted_class]\n",
    "\n",
    "# Initialize the SentimentInference class\n",
    "model_path = \"C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/trained_model\"\n",
    "sentiment_inference = SentimentInference(model_path)\n",
    "\n",
    "# Home page route\n",
    "@app.route(\"/\", methods=[\"GET\"])\n",
    "def home():\n",
    "    return \"\"\"\n",
    "    <h1>Welcome to Sentiment Analysis / Test Classification</h1>\n",
    "    <p>Click <a href=\"/predict\">here</a> to go to the prediction page.</p>\n",
    "    \"\"\"\n",
    "\n",
    "# Predict page route\n",
    "@app.route(\"/predict\", methods=[\"GET\", \"POST\"])\n",
    "def predict():\n",
    "    if request.method == \"GET\":\n",
    "        # Render a basic HTML form for text input\n",
    "        return render_template_string('''\n",
    "        <h1>Sentiment Prediction</h1>\n",
    "        <form method=\"POST\">\n",
    "            <label for=\"text\">Enter your text:</label><br>\n",
    "            <textarea id=\"text\" name=\"text\" rows=\"4\" cols=\"50\"></textarea><br><br>\n",
    "            <input type=\"submit\" value=\"Predict\">\n",
    "        </form>\n",
    "        ''')\n",
    "    elif request.method == \"POST\":\n",
    "        # Get the input text from the form\n",
    "        text = request.form.get(\"text\")\n",
    "        \n",
    "        # Validate the input\n",
    "        if not text:\n",
    "            return jsonify({\"error\": \"No text provided\"}), 400\n",
    "        \n",
    "        # Predict the sentiment\n",
    "        try:\n",
    "            sentiment = sentiment_inference.predict_sentiment(text)\n",
    "            return jsonify({\"sentiment\": sentiment})\n",
    "        except Exception as e:\n",
    "            return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the Flask app\n",
    "    app.run(host=\"0.0.0.0\", port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to test the website for prediction\n",
    "# import requests\n",
    "\n",
    "# url = \"http://127.0.0.1:5000/predict\"\n",
    "# data = {\"text\": \"This is a sample text for sentiment analysis.\"}\n",
    "# response = requests.post(url, json=data)\n",
    "\n",
    "# print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('feature_engineering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT\\\\artifacts\\\\prepare_model'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.chdir('prepare_model')\n",
    "# %pwd\n",
    "# %ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataclasses import dataclass\n",
    "\n",
    "# @dataclass\n",
    "# class TrainingConfig:\n",
    "#     datasets_dir: str = \"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT\\\\artifacts\\\\feature_engineering\"  # Directory containing train_dataset.pt, val_dataset.pt, test_dataset.pt\n",
    "#     output_dir: str = \"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT\\\\artifacts\\\\training\"      # Directory to save training outputs\n",
    "#     model_save_path: str = \"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT\\\\artifacts\\\\training\\\\trained_model\"  # Directory to save the trained model\n",
    "#     num_train_epochs: int = 1              # Number of training epochs\n",
    "#     per_device_train_batch_size: int = 16   # Batch size for training\n",
    "#     per_device_eval_batch_size: int = 16    # Batch size for evaluation\n",
    "#     warmup_steps: int = 500                 # Number of warmup steps\n",
    "#     weight_decay: float = 0.01              # Weight decay\n",
    "#     max_steps: int = 1000                   # Maximum number of training steps\n",
    "#     save_steps: int = 500                   # Save model every `save_steps`\n",
    "#     logging_dir: str = \"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT\\\\logs\"       # Directory for logs\n",
    "\n",
    "# # Initialize TrainingConfig\n",
    "# config = TrainingConfig()\n",
    "\n",
    "# # Initialize ModelTraining\n",
    "# model_training = ModelTraining(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# %os._exit(00)\n",
    "# import accelerate\n",
    "\n",
    "# accelerate.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
