{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows-SSD\n",
      " Volume Serial Number is 5824-84FE\n",
      "\n",
      " Directory of c:\\Users\\prass\\OneDrive\\Desktop\\practise\\new_env\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/03/2025  10:10 PM    <DIR>          .\n",
      "02/03/2025  06:58 PM    <DIR>          ..\n",
      "02/03/2025  10:02 PM    <DIR>          .github\n",
      "02/17/2025  03:23 PM    <DIR>          text-classification-using-BERT\n",
      "               0 File(s)              0 bytes\n",
      "               4 Dir(s)  256,057,806,848 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('text-classification-using-BERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    datasets_dir: Path  # Directory where datasets are saved\n",
    "    output_dir: Path  # Directory to save training outputs\n",
    "    model_save_path: Path  # Directory to save the trained model\n",
    "    num_train_epochs: int  # Number of training epochs\n",
    "    per_device_train_batch_size: int  # Training batch size per device\n",
    "    per_device_eval_batch_size: int  # Evaluation batch size per device\n",
    "    warmup_steps: int  # Number of warmup steps\n",
    "    weight_decay: float  # Weight decay rate\n",
    "    max_steps: int  # Maximum number of training steps\n",
    "    save_steps: int  # Save model every `save_steps` steps\n",
    "    logging_steps: int  # Log metrics every `logging_steps` steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CONFIG_FILE_PATH = Path(\"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT/config/config.yaml\")\n",
    "PARAMS_FILE_PATH = Path(\"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT/params.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textClassifier.constants import *\n",
    "from textClassifier.utils.commons import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "        # Debug: Print the config and params objects\n",
    "        print(\"Config:\", self.config)\n",
    "        print(\"Params:\", self.params)\n",
    "\n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        \"\"\"\n",
    "        Returns the configuration for model training.\n",
    "        \"\"\"\n",
    "        training = self.config.feature_engineering\n",
    "        print(\"i am printing training\", training)\n",
    "        params = self.params\n",
    "        training_data = os.path.join(self.config.artifacts_root, 'feature_engineering')\n",
    "        model_save_path = os.path.join(self.config.artifacts_root, 'training')\n",
    "\n",
    "        create_directories([training.root_dir, training.model_save_path])\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            datasets_dir=Path(training_data),\n",
    "            output_dir=Path(training.root_dir),\n",
    "            # model_save_path=Path(training.model_save_path),\n",
    "            params_num_train_epochs=params.num_train_epochs,\n",
    "            params_per_device_train_batch_size=params.per_device_train_batch_size,\n",
    "            params_per_device_eval_batch_size=params.per_device_eval_batch_size,\n",
    "            params_warmup_steps=params.warmup_steps,\n",
    "            params_weight_decay=params.weight_decay,\n",
    "            params_max_steps=params.max_steps,\n",
    "            params_save_steps=params.save_steps,\n",
    "            params_logging_steps=params.logging_steps\n",
    "            \n",
    "        )\n",
    "\n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        \"\"\"\n",
    "        Returns the configuration for model training.\n",
    "        \"\"\"\n",
    "        training = self.config.training\n",
    "        params = self.params\n",
    "\n",
    "        # Create directories if they don't exist\n",
    "        create_directories([\n",
    "            Path(training.root_dir),\n",
    "            Path(training.model_save_path)\n",
    "        ])\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            datasets_dir=Path(self.config.feature_engineering.training_cleansed_data),  # Update this to point to feature_engineering\n",
    "            output_dir=Path(training.root_dir),\n",
    "            model_save_path=Path(training.model_save_path),\n",
    "            num_train_epochs=params.num_train_epochs,\n",
    "            per_device_train_batch_size=params.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=params.per_device_eval_batch_size,\n",
    "            warmup_steps=params.warmup_steps,\n",
    "            weight_decay=params.weight_decay,\n",
    "            max_steps=params.max_steps,\n",
    "            save_steps=params.save_steps,\n",
    "            logging_steps=params.logging_steps\n",
    "        )\n",
    "\n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This below code is for BERT but we are using distllBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#  this is bert model or bert-based-uncased but we downgraded to distillBert for resources bottleneks\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from textClassifier import logger\n",
    "\n",
    "\n",
    "class ModelTraining:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        \"\"\"\n",
    "        Initializes the ModelTraining class.\n",
    "\n",
    "        Args:\n",
    "            config (ModelTrainingConfig): Configuration for model training.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "    def load_datasets(self):\n",
    "        \"\"\"\n",
    "        Loads the train, validation, and test datasets from the specified directory.\n",
    "\n",
    "        Returns:\n",
    "            train_dataset, val_dataset, test_dataset: Loaded datasets.\n",
    "        \"\"\"\n",
    "        datasets_dir = Path(self.config.feature_engineering)\n",
    "        train_dataset = torch.load(datasets_dir / \"train_dataset.pt\")\n",
    "        val_dataset = torch.load(datasets_dir / \"val_dataset.pt\")\n",
    "        test_dataset = torch.load(datasets_dir / \"test_dataset.pt\")\n",
    "\n",
    "        logger.info(f\"Datasets loaded from {datasets_dir}\")\n",
    "        return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the model using the loaded datasets.\n",
    "        \"\"\"\n",
    "        # Load datasets\n",
    "        train_dataset, val_dataset, _ = self.load_datasets()\n",
    "\n",
    "        # Initialize the model\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            'bert-base-uncased', num_labels=3\n",
    "        )\n",
    "\n",
    "        # Set up training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config.output_dir,\n",
    "            num_train_epochs=self.config.num_train_epochs,\n",
    "            per_device_train_batch_size=self.config.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=self.config.per_device_eval_batch_size,\n",
    "            warmup_steps=self.config.warmup_steps,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            max_steps=self.config.max_steps,\n",
    "            save_steps=self.config.save_steps,\n",
    "            logging_steps=self.config.logging_steps,\n",
    "            load_best_model_at_end=True\n",
    "        )\n",
    "\n",
    "        # Initialize the Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset\n",
    "        )\n",
    "\n",
    "        # Start training\n",
    "        trainer.train()\n",
    "\n",
    "        # Save the trained model\n",
    "        self.save_model(trainer)\n",
    "\n",
    "    def save_model(self, trainer):\n",
    "        \"\"\"\n",
    "        Saves the trained model and tokenizer to the specified directory.\n",
    "\n",
    "        Args:\n",
    "            trainer (Trainer): The Trainer object containing the trained model.\n",
    "        \"\"\"\n",
    "        save_path = Path(self.config.model_save_path)\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        trainer.save_model(save_path)\n",
    "        logger.info(f\"Model saved to {save_path}\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluates the model on the test dataset.\n",
    "        \"\"\"\n",
    "        # Load the test dataset\n",
    "        _, _, test_dataset = self.load_datasets()\n",
    "\n",
    "        # Load the trained model\n",
    "        model = BertForSequenceClassification.from_pretrained(self.config.model_save_path)\n",
    "\n",
    "        # Set up training arguments for evaluation\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config.output_dir,\n",
    "            per_device_eval_batch_size=self.config.per_device_eval_batch_size,\n",
    "            logging_dir=self.config.logging_dir\n",
    "        )\n",
    "\n",
    "        # Initialize the Trainer for evaluation\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            # train_dataset=train_dataset, # The dataset used for training the model\n",
    "            # eval_dataset=val_dataset\n",
    "        )\n",
    "\n",
    "        # Evaluate the model\n",
    "        results = trainer.evaluate(test_dataset)\n",
    "        logger.info(\"Evaluation Results:\")\n",
    "        logger.info(f\"  - Loss: {results['eval_loss']:.4f}\")\n",
    "        logger.info(f\"  - Runtime: {results['eval_runtime']:.2f} seconds\")\n",
    "        logger.info(f\"  - Samples per Second: {results['eval_samples_per_second']:.2f}\")\n",
    "        logger.info(f\"  - Steps per Second: {results['eval_steps_per_second']:.2f}\")\n",
    "        logger.info(f\"  - Epoch: {results['epoch']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_train_epochs: 1\n",
    "# per_device_train_batch_size: 5\n",
    "# per_device_eval_batch_size: 5\n",
    "# warmup_steps: 10\n",
    "# weight_decay: 0.01\n",
    "# max_steps: 10\n",
    "# save_steps: 2\n",
    "# logging_steps: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual Training starts here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CONFIG_FILE_PATH = Path(\"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT/config/config.yaml\")\n",
    "PARAMS_FILE_PATH = Path(\"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT/params.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textClassifier.constants import *\n",
    "from textClassifier.utils.commons import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from textClassifier import logger\n",
    "\n",
    "\n",
    "class ModelTraining:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        \"\"\"\n",
    "        Initializes the ModelTraining class.\n",
    "\n",
    "        Args:\n",
    "            config (ModelTrainingConfig): Configuration for model training.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "    def load_datasets(self):\n",
    "        \"\"\"\n",
    "        Loads the train, validation, and test datasets from the specified directory.\n",
    "\n",
    "        Returns:\n",
    "            train_dataset, val_dataset, test_dataset: Loaded datasets.\n",
    "        \"\"\"\n",
    "        datasets_dir = Path(self.config.datasets_dir)\n",
    "        train_dataset = torch.load(datasets_dir / \"train_dataset.pt\", weights_only=False)\n",
    "        val_dataset = torch.load(datasets_dir / \"val_dataset.pt\", weights_only=False)\n",
    "        test_dataset = torch.load(datasets_dir / \"test_dataset.pt\", weights_only=False)\n",
    "\n",
    "        logger.info(f\"Datasets loaded from {datasets_dir}\")\n",
    "        return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the model using the loaded datasets.\n",
    "        \"\"\"\n",
    "        # Load datasets\n",
    "        train_dataset, val_dataset, _ = self.load_datasets()\n",
    "\n",
    "        # Initialize the model\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"distilbert-base-uncased\", num_labels=3\n",
    "            )\n",
    "\n",
    "        #model = BertForSequenceClassification.from_pretrained(\n",
    "        #    'bert-base-uncased', num_labels=3\n",
    "        #)\n",
    "\n",
    "        # Set up training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config.output_dir,\n",
    "            num_train_epochs=self.config.num_train_epochs,\n",
    "            per_device_train_batch_size=self.config.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=self.config.per_device_eval_batch_size,\n",
    "            warmup_steps=self.config.warmup_steps,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            max_steps=self.config.max_steps,\n",
    "            save_steps=self.config.save_steps,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            # load_best_model_at_end=True\n",
    "        )\n",
    "\n",
    "        # Initialize the Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset\n",
    "        )\n",
    "\n",
    "        # Start training\n",
    "        trainer.train()\n",
    "\n",
    "        # Save the trained model\n",
    "        self.save_model(trainer)\n",
    "\n",
    "    def save_model(self, trainer):\n",
    "        \"\"\"\n",
    "        Saves the trained model and tokenizer to the specified directory.\n",
    "\n",
    "        Args:\n",
    "            trainer (Trainer): The Trainer object containing the trained model.\n",
    "        \"\"\"\n",
    "        save_path = Path(self.config.model_save_path)\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        trainer.save_model(save_path)\n",
    "        logger.info(f\"Model saved to {save_path}\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluates the model on the test dataset.\n",
    "        \"\"\"\n",
    "        # Load the test dataset\n",
    "        _, _, test_dataset = self.load_datasets()\n",
    "\n",
    "        # Load the trained model\n",
    "        # model = AutoModelForSequenceClassification.from_pretrained(self.config.model_save_path)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(self.config.model_save_path)\n",
    "\n",
    "\n",
    "        # Set up training arguments for evaluation\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config.output_dir,\n",
    "            per_device_eval_batch_size=self.config.per_device_eval_batch_size\n",
    "        )\n",
    "\n",
    "        # Initialize the Trainer for evaluation\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args\n",
    "        )\n",
    "\n",
    "        # Evaluate the model\n",
    "        results = trainer.evaluate(test_dataset)\n",
    "\n",
    "        logger.info(\"Evaluation Results:\")\n",
    "        logger.info(\"Evaluation Results:\", results)\n",
    "        logger.info(f\"  - Loss: {results['eval_loss']:.4f}\")\n",
    "        logger.info(f\"  - Runtime: {results['eval_runtime']:.2f} seconds\")\n",
    "        logger.info(f\"  - Samples per Second: {results['eval_samples_per_second']:.2f}\")\n",
    "        logger.info(f\"  - Steps per Second: {results['eval_steps_per_second']:.2f}\")\n",
    "        logger.info(f\"  - Epoch: {results.get('epoch', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textClassifier.components import SentimentDataset  # Import the SentimentDataset class\n",
    "# import torch\n",
    "\n",
    "# # Load your data and create datasets\n",
    "# train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "# val_dataset = SentimentDataset(val_encodings, val_labels)\n",
    "# test_dataset = SentimentDataset(test_encodings, test_labels)\n",
    "\n",
    "# # Save the datasets\n",
    "# torch.save(train_dataset, \"artifacts/feature_engineering/train_dataset.pt\")\n",
    "# torch.save(val_dataset, \"artifacts/feature_engineering/val_dataset.pt\")\n",
    "# torch.save(test_dataset, \"artifacts/feature_engineering/test_dataset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "def convert_to_dicts(tokenized_texts):\n",
    "    \"\"\"\n",
    "    Converts a list of tokenized texts into a dictionary of input IDs and attention masks.\n",
    "\n",
    "    Args:\n",
    "        tokenized_texts (list): List of tokenized texts (each is a dictionary with 'input_ids' and 'attention_mask').\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing 'input_ids' and 'attention_mask' as lists.\n",
    "    \"\"\"\n",
    "    input_ids = [d['input_ids'].squeeze(0) for d in tokenized_texts]  # Remove batch dimension\n",
    "    attention_masks = [d['attention_mask'].squeeze(0) for d in tokenized_texts]  # Remove batch dimension\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_masks}\n",
    "\n",
    "\n",
    "def create_datasets(train_texts, train_labels, val_texts, val_labels, test_texts, test_labels):\n",
    "    \"\"\"\n",
    "    Creates SentimentDataset objects for train, validation, and test splits.\n",
    "\n",
    "    Args:\n",
    "        train_texts (list): List of tokenized texts for the training set.\n",
    "        train_labels (list): List of labels for the training set.\n",
    "        val_texts (list): List of tokenized texts for the validation set.\n",
    "        val_labels (list): List of labels for the validation set.\n",
    "        test_texts (list): List of tokenized texts for the test set.\n",
    "        test_labels (list): List of labels for the test set.\n",
    "\n",
    "    Returns:\n",
    "        train_dataset, val_dataset, test_dataset: SentimentDataset objects for each split.\n",
    "    \"\"\"\n",
    "    # Convert tokenized texts to encodings\n",
    "    train_encodings = convert_to_dicts(train_texts)\n",
    "    val_encodings = convert_to_dicts(val_texts)\n",
    "    test_encodings = convert_to_dicts(test_texts)\n",
    "\n",
    "    # Create SentimentDataset objects\n",
    "    train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "    val_dataset = SentimentDataset(val_encodings, val_labels)\n",
    "    test_dataset = SentimentDataset(test_encodings, test_labels)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def save_datasets(self, train_dataset, val_dataset, test_dataset):\n",
    "        \"\"\"\n",
    "        Saves the train, validation, and test datasets to the specified directory.\n",
    "\n",
    "        Args:\n",
    "            train_dataset: Training dataset.\n",
    "            val_dataset: Validation dataset.\n",
    "            test_dataset: Test dataset.\n",
    "        \"\"\"\n",
    "        # Create the directory if it doesn't exist\n",
    "        datasets_dir = Path(self.config.datasets_dir)\n",
    "        datasets_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save the datasets\n",
    "        torch.save(train_dataset, datasets_dir / \"train_dataset.pt\")\n",
    "        torch.save(val_dataset, datasets_dir / \"val_dataset.pt\")\n",
    "        torch.save(test_dataset, datasets_dir / \"test_dataset.pt\")\n",
    "\n",
    "        logger.info(f\"Datasets saved to {datasets_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-04 21:54:36,547: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\config\\config.yaml loaded successfully]\n",
      "[2025-03-04 21:54:36,555: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\params.yaml loaded successfully]\n",
      "[2025-03-04 21:54:36,558: INFO: commons: created directory at: artifacts]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-04 21:54:36,559: INFO: commons: created directory at: artifacts\\training]\n",
      "[2025-03-04 21:54:36,563: INFO: commons: created directory at: artifacts\\training\\trained_model]\n",
      "[2025-03-04 21:54:40,234: INFO: 2927273006: Datasets loaded from artifacts\\feature_engineering]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\prass\\AppData\\Local\\Temp\\ipykernel_67756\\2659941139.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 04:31, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.976229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prass\\AppData\\Local\\Temp\\ipykernel_67756\\2659941139.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\prass\\AppData\\Local\\Temp\\ipykernel_67756\\2659941139.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\prass\\AppData\\Local\\Temp\\ipykernel_67756\\2659941139.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\prass\\AppData\\Local\\Temp\\ipykernel_67756\\2659941139.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\prass\\AppData\\Local\\Temp\\ipykernel_67756\\2659941139.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-04 21:59:15,344: INFO: 2927273006: Model saved to artifacts\\training\\trained_model]\n",
      "[2025-03-04 21:59:20,081: INFO: 2927273006: Datasets loaded from artifacts\\feature_engineering]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1098' max='1098' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1098/1098 04:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-04 22:03:26,398: INFO: 2927273006: Evaluation Results:]\n",
      "[2025-03-04 22:03:26,399: INFO: 2927273006: Evaluation Results:]\n",
      "[2025-03-04 22:03:26,400: INFO: 2927273006:   - Loss: 0.9623]\n",
      "[2025-03-04 22:03:26,402: INFO: 2927273006:   - Runtime: 246.15 seconds]\n",
      "[2025-03-04 22:03:26,404: INFO: 2927273006:   - Samples per Second: 8.92]\n",
      "[2025-03-04 22:03:26,406: INFO: 2927273006:   - Steps per Second: 4.46]\n",
      "[2025-03-04 22:03:26,408: INFO: 2927273006:   - Epoch: N/A]\n"
     ]
    }
   ],
   "source": [
    "from accelerate import PartialState\n",
    "accelerator_state_kwargs = {\"enabled\": True, \"use_configured_state\": False}\n",
    "\n",
    "# Initialize PartialState\n",
    "\n",
    "from textClassifier.components import sentimentsDataset\n",
    "if __name__ == \"__main__\":\n",
    "    # import mlflow\n",
    "    # mlflow.set_tracking_uri(None)\n",
    "    # Initialize ConfigurationManager\n",
    "    config_manager = ConfigurationManager()\n",
    "    # Get the model training config\n",
    "    training_config = config_manager.get_training_config()\n",
    "\n",
    "    # Initialize ModelTraining\n",
    "    model_training = ModelTraining(config=training_config)\n",
    "\n",
    "    # Train the model\n",
    "    model_training.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    model_training.evaluate()\n",
    "    \n",
    "    partial_state = PartialState()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "class SentimentInference:\n",
    "    def __init__(self, model_path: str):\n",
    "        \"\"\"\n",
    "        Initializes the SentimentInference class.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): Path to the trained model and tokenizer.\n",
    "        \"\"\"\n",
    "        self.model_path = Path(model_path)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        self.sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "    def predict_sentiment(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Predicts the sentiment of the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text for sentiment prediction.\n",
    "\n",
    "        Returns:\n",
    "            str: Predicted sentiment ('negative', 'neutral', 'positive').\n",
    "        \"\"\"\n",
    "        # Tokenize the input text\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        \n",
    "        # Get model predictions\n",
    "        outputs = self.model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "        \n",
    "        # Map the predicted class to sentiment\n",
    "        return self.sentiment_map[predicted_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows-SSD\n",
      " Volume Serial Number is 5824-84FE\n",
      "\n",
      " Directory of c:\\Users\\prass\\OneDrive\\Desktop\\practise\\new_env\\text-classification-using-BERT\n",
      "\n",
      "02/17/2025  03:23 PM    <DIR>          .\n",
      "02/03/2025  10:10 PM    <DIR>          ..\n",
      "02/03/2025  10:15 PM    <DIR>          .github\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/05/2025  11:14 AM             3,597 .gitignore\n",
      "03/04/2025  09:54 PM    <DIR>          artifacts\n",
      "02/03/2025  10:15 PM    <DIR>          config\n",
      "02/03/2025  10:15 PM                 0 dvc.yaml\n",
      "02/03/2025  09:43 PM             1,087 LICENSE\n",
      "02/03/2025  10:50 PM    <DIR>          logs\n",
      "02/05/2025  11:31 AM               440 main.py\n",
      "03/04/2025  08:59 PM               170 params.yaml\n",
      "02/03/2025  09:43 PM                32 README.md\n",
      "02/03/2025  10:26 PM               248 requirements.txt\n",
      "03/04/2025  09:08 PM    <DIR>          research\n",
      "02/03/2025  10:16 PM               825 setup.py\n",
      "02/03/2025  10:28 PM    <DIR>          src\n",
      "02/07/2025  10:25 AM             1,344 template.py\n",
      "02/05/2025  11:31 AM    <DIR>          templates\n",
      "03/03/2025  04:07 PM    <DIR>          wandb\n",
      "               9 File(s)          7,743 bytes\n",
      "              10 Dir(s)  251,240,882,176 bytes free\n"
     ]
    }
   ],
   "source": [
    "os."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (1975116703.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[74], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    model_path = 'C:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\artifacts\\training'\u001b[0m\n\u001b[1;37m                                                                                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model_path = 'C:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\artifacts\\training'\n",
    "    pred = SentimentInference(model_path)\n",
    "    test = pred.predict_sentiment('I hate flying with airlines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'C:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\artifacts\\training\\trained_model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'C:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\artifacts\\training\\trained_model' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/trained_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Initialize the SentimentInference class\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mSentimentInference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Test the prediction\u001b[39;00m\n\u001b[0;32m     10\u001b[0m test \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mpredict_sentiment(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI hate flying with airlines\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[69], line 15\u001b[0m, in \u001b[0;36mSentimentInference.__init__\u001b[1;34m(self, model_path)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_path \u001b[38;5;241m=\u001b[39m Path(model_path)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_path)\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentiment_map \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneutral\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m'\u001b[39m}\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:963\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    960\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_fast\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2036\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2033\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[0;32m   2034\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[0;32m   2035\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[1;32m-> 2036\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2037\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2038\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2039\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2040\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2041\u001b[0m     )\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2044\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'C:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\artifacts\\training\\trained_model'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'C:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\artifacts\\training\\trained_model' is the correct path to a directory containing all relevant files for a DistilBertTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "if __name__ == \"__main__\":\n",
    "    # Provide the path to your trained model\n",
    "    model_path = Path(\"C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/trained_model\")\n",
    "    \n",
    "    # Initialize the SentimentInference class\n",
    "    pred = SentimentInference(model_path)\n",
    "    \n",
    "    # Test the prediction\n",
    "    test = pred.predict_sentiment('I hate flying with airlines')\n",
    "    print(f\"Predicted Sentiment: {test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\\\\tokenizer_config.json',\n",
       " 'C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\\\\special_tokens_map.json',\n",
       " 'C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\\\\vocab.txt',\n",
       " 'C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\\\\added_tokens.json',\n",
       " 'C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\\\\tokenizer.json')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# # Load the model and tokenizer\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# # Save the model and tokenizer\n",
    "# model.save_pretrained(\"C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\")\n",
    "# tokenizer.save_pretrained(\"C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "class SentimentInference:\n",
    "    def __init__(self, model_path: str):\n",
    "        \"\"\"\n",
    "        Initializes the SentimentInference class.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): Path to the trained model and tokenizer.\n",
    "        \"\"\"\n",
    "        self.model_path = Path(model_path)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        self.sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "    def predict_sentiment(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Predicts the sentiment of the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text for sentiment prediction.\n",
    "\n",
    "        Returns:\n",
    "            str: Predicted sentiment ('negative', 'neutral', 'positive').\n",
    "        \"\"\"\n",
    "        # Tokenize the input text\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        \n",
    "        # Get model predictions\n",
    "        outputs = self.model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "        \n",
    "        # Map the predicted class to sentiment\n",
    "        return self.sentiment_map[predicted_class]\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Provide the path to your trained model and tokenizer\n",
    "#     model_path = \"C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\"\n",
    "    \n",
    "#     # Initialize the SentimentInference class\n",
    "#     pred = SentimentInference(model_path)\n",
    "    \n",
    "#     # Test the prediction\n",
    "#     test = pred.predict_sentiment('I hate flying with airlines')\n",
    "#     print(f\"Predicted Sentiment: {test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_sentiment(text):\n",
    "#     '''Function to predict the sentiment of a given text using a pre-trained BERT model.\n",
    "#     Args: the input text for sentiment prediction.\n",
    "#     Returns: the predicted sentiment ('negative', 'neutral', 'positive').\n",
    "#     '''\n",
    "\n",
    "#     inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "#     outputs = model(**inputs)\n",
    "#     predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "#     predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "#     sentiment = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "#     return sentiment[predicted_class]\n",
    "\n",
    "# # Example prediction\n",
    "# example_text = \"I hate flying with this airline!\"\n",
    "# predicted_sentiment = predict_sentiment(example_text)\n",
    "# print(f\"Predicted Sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerate version: 0.26.0\n",
      "Transformers version: 4.49.0\n"
     ]
    }
   ],
   "source": [
    "# import accelerate\n",
    "# import transformers\n",
    "\n",
    "# print(\"Accelerate version:\", accelerate.__version__)\n",
    "# print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n",
      "[2025-03-09 14:07:14,064: INFO: _internal: \u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.1.117:5000]\n",
      "[2025-03-09 14:07:14,066: INFO: _internal: \u001b[33mPress CTRL+C to quit\u001b[0m]\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS, cross_origin\n",
    "# Initialize the Flask app\n",
    "app = Flask(__name__)\n",
    "# app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Initialize the SentimentInference class\n",
    "# model_path = \"artifacts/trained_model\"\n",
    "model_path = \"C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\"\n",
    "sentiment_inference = SentimentInference(model_path)\n",
    "\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict():\n",
    "    \"\"\"\n",
    "    API endpoint for sentiment prediction.\n",
    "    Expects a JSON payload with a 'text' field.\n",
    "    \"\"\"\n",
    "    # Get the input text from the request\n",
    "    data = request.json\n",
    "    text = data.get(\"text\")\n",
    "    \n",
    "    # Validate the input\n",
    "    if not text:\n",
    "        return jsonify({\"error\": \"No text provided\"}), 400\n",
    "    \n",
    "    # Predict the sentiment\n",
    "    try:\n",
    "        sentiment = sentiment_inference.predict_sentiment(text)\n",
    "        return jsonify({\"sentiment\": sentiment})\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the Flask app\n",
    "    app.run(host=\"0.0.0.0\", port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n",
      "[2025-03-09 14:26:25,822: INFO: _internal: \u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.1.117:5000]\n",
      "[2025-03-09 14:26:25,829: INFO: _internal: \u001b[33mPress CTRL+C to quit\u001b[0m]\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, render_template_string\n",
    "\n",
    "# Initialize the Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Dummy SentimentInference class for demonstration\n",
    "class SentimentInference:\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "\n",
    "    def predict_sentiment(self, text):\n",
    "        # Dummy sentiment prediction logic\n",
    "        # if \"good\" in text.lower():\n",
    "        #     return \"Positive\"\n",
    "        # elif \"bad\" in text.lower():\n",
    "        #     return \"Negative\"\n",
    "        # else:\n",
    "        #     return \"Neutral\"\n",
    "        try:\n",
    "            sentiment = sentiment_inference.predict_sentiment(text)\n",
    "            return jsonify({\"sentiment\": sentiment})\n",
    "        except Exception as e:\n",
    "            return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "# Initialize the SentimentInference class\n",
    "model_path = \"C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\"\n",
    "sentiment_inference = SentimentInference(model_path)\n",
    "\n",
    "# Home page route\n",
    "@app.route(\"/\", methods=[\"GET\"])\n",
    "def home():\n",
    "    return \"\"\"\n",
    "    <h1>Welcome to Sentiment Analysis</h1>\n",
    "    <p>Click <a href=\"/predict\">here</a> to go to the prediction page.</p>\n",
    "    \"\"\"\n",
    "\n",
    "# Predict page route\n",
    "@app.route(\"/predict\", methods=[\"GET\", \"POST\"])\n",
    "def predict():\n",
    "    if request.method == \"GET\":\n",
    "        # Render a basic HTML form for text input\n",
    "        return render_template_string('''\n",
    "        <h1>Sentiment Prediction</h1>\n",
    "        <form method=\"POST\">\n",
    "            <label for=\"text\">Enter your text:</label><br>\n",
    "            <textarea id=\"text\" name=\"text\" rows=\"4\" cols=\"50\"></textarea><br><br>\n",
    "            <input type=\"submit\" value=\"Predict\">\n",
    "        </form>\n",
    "        ''')\n",
    "    elif request.method == \"POST\":\n",
    "        # Get the input text from the form\n",
    "        text = request.form.get(\"text\")\n",
    "        \n",
    "        # Validate the input\n",
    "        if not text:\n",
    "            return jsonify({\"error\": \"No text provided\"}), 400\n",
    "        \n",
    "        # Predict the sentiment\n",
    "        try:\n",
    "            sentiment = sentiment_inference.predict_sentiment(text)\n",
    "            return jsonify({\"sentiment\": sentiment})\n",
    "        except Exception as e:\n",
    "            return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the Flask app\n",
    "    app.run(host=\"0.0.0.0\", port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n",
      "[2025-03-09 14:33:46,912: INFO: _internal: \u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.1.117:5000]\n",
      "[2025-03-09 14:33:46,912: INFO: _internal: \u001b[33mPress CTRL+C to quit\u001b[0m]\n",
      "[2025-03-09 14:34:04,194: INFO: _internal: 127.0.0.1 - - [09/Mar/2025 14:34:04] \"GET / HTTP/1.1\" 200 -]\n",
      "[2025-03-09 14:34:04,499: INFO: _internal: 127.0.0.1 - - [09/Mar/2025 14:34:04] \"GET / HTTP/1.1\" 200 -]\n",
      "[2025-03-09 14:34:07,187: INFO: _internal: 127.0.0.1 - - [09/Mar/2025 14:34:07] \"GET /predict HTTP/1.1\" 200 -]\n",
      "[2025-03-09 14:34:18,667: INFO: _internal: 127.0.0.1 - - [09/Mar/2025 14:34:18] \"POST /predict HTTP/1.1\" 200 -]\n",
      "[2025-03-09 14:34:37,061: INFO: _internal: 127.0.0.1 - - [09/Mar/2025 14:34:37] \"GET / HTTP/1.1\" 200 -]\n",
      "[2025-03-09 14:34:38,810: INFO: _internal: 127.0.0.1 - - [09/Mar/2025 14:34:38] \"GET /predict HTTP/1.1\" 200 -]\n",
      "[2025-03-09 14:34:51,884: INFO: _internal: 127.0.0.1 - - [09/Mar/2025 14:34:51] \"POST /predict HTTP/1.1\" 200 -]\n",
      "[2025-03-09 14:35:17,838: INFO: _internal: 127.0.0.1 - - [09/Mar/2025 14:35:17] \"GET / HTTP/1.1\" 200 -]\n",
      "[2025-03-09 14:35:29,048: INFO: _internal: 127.0.0.1 - - [09/Mar/2025 14:35:29] \"GET /predict HTTP/1.1\" 200 -]\n",
      "[2025-03-09 14:35:44,453: INFO: _internal: 127.0.0.1 - - [09/Mar/2025 14:35:44] \"POST /predict HTTP/1.1\" 200 -]\n",
      "[2025-03-09 14:36:01,655: INFO: _internal: 127.0.0.1 - - [09/Mar/2025 14:36:01] \"GET /predict HTTP/1.1\" 200 -]\n",
      "[2025-03-09 14:36:11,124: INFO: _internal: 127.0.0.1 - - [09/Mar/2025 14:36:11] \"POST /predict HTTP/1.1\" 200 -]\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, render_template_string\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize the Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Your SentimentInference class\n",
    "class SentimentInference:\n",
    "    def __init__(self, model_path: str):\n",
    "        \"\"\"\n",
    "        Initializes the SentimentInference class.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): Path to the trained model and tokenizer.\n",
    "        \"\"\"\n",
    "        self.model_path = Path(model_path)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        self.sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "    def predict_sentiment(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Predicts the sentiment of the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text for sentiment prediction.\n",
    "\n",
    "        Returns:\n",
    "            str: Predicted sentiment ('negative', 'neutral', 'positive').\n",
    "        \"\"\"\n",
    "        # Tokenize the input text\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        \n",
    "        # Get model predictions\n",
    "        outputs = self.model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "        \n",
    "        # Map the predicted class to sentiment\n",
    "        return self.sentiment_map[predicted_class]\n",
    "\n",
    "# Initialize the SentimentInference class\n",
    "model_path = \"C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\"\n",
    "sentiment_inference = SentimentInference(model_path)\n",
    "\n",
    "# Home page route\n",
    "@app.route(\"/\", methods=[\"GET\"])\n",
    "def home():\n",
    "    return \"\"\"\n",
    "    <h1>Welcome to Sentiment Analysis</h1>\n",
    "    <p>Click <a href=\"/predict\">here</a> to go to the prediction page.</p>\n",
    "    \"\"\"\n",
    "\n",
    "# Predict page route\n",
    "@app.route(\"/predict\", methods=[\"GET\", \"POST\"])\n",
    "def predict():\n",
    "    if request.method == \"GET\":\n",
    "        # Render a basic HTML form for text input\n",
    "        return render_template_string('''\n",
    "        <h1>Sentiment Prediction</h1>\n",
    "        <form method=\"POST\">\n",
    "            <label for=\"text\">Enter your text:</label><br>\n",
    "            <textarea id=\"text\" name=\"text\" rows=\"4\" cols=\"50\"></textarea><br><br>\n",
    "            <input type=\"submit\" value=\"Predict\">\n",
    "        </form>\n",
    "        ''')\n",
    "    elif request.method == \"POST\":\n",
    "        # Get the input text from the form\n",
    "        text = request.form.get(\"text\")\n",
    "        \n",
    "        # Validate the input\n",
    "        if not text:\n",
    "            return jsonify({\"error\": \"No text provided\"}), 400\n",
    "        \n",
    "        # Predict the sentiment\n",
    "        try:\n",
    "            sentiment = sentiment_inference.predict_sentiment(text)\n",
    "            return jsonify({\"sentiment\": sentiment})\n",
    "        except Exception as e:\n",
    "            return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the Flask app\n",
    "    app.run(host=\"0.0.0.0\", port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='127.0.0.1', port=5000): Max retries exceeded with url: /predict (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000023395A1EDF0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\urllib3\\connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\urllib3\\connectionpool.py:493\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 493\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\urllib3\\connection.py:445\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[1;32m--> 445\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\http\\client.py:1280\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1280\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\http\\client.py:1040\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1040\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1043\u001b[0m \n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\http\\client.py:980\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 980\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\urllib3\\connection.py:276\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\urllib3\\connection.py:213\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    215\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    217\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.client.connect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport)\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x0000023395A1EDF0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='127.0.0.1', port=5000): Max retries exceeded with url: /predict (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000023395A1EDF0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://127.0.0.1:5000/predict\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a sample text for sentiment analysis.\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m----> 5\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\requests\\adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='127.0.0.1', port=5000): Max retries exceeded with url: /predict (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000023395A1EDF0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://127.0.0.1:5000/predict\"\n",
    "data = {\"text\": \"This is a sample text for sentiment analysis.\"}\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('feature_engineering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT\\\\artifacts\\\\prepare_model'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.chdir('prepare_model')\n",
    "%pwd\n",
    "# %ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    datasets_dir: str = \"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT\\\\artifacts\\\\feature_engineering\"  # Directory containing train_dataset.pt, val_dataset.pt, test_dataset.pt\n",
    "    output_dir: str = \"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT\\\\artifacts\\\\training\"      # Directory to save training outputs\n",
    "    model_save_path: str = \"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT\\\\artifacts\\\\training\\\\trained_model\"  # Directory to save the trained model\n",
    "    num_train_epochs: int = 1              # Number of training epochs\n",
    "    per_device_train_batch_size: int = 16   # Batch size for training\n",
    "    per_device_eval_batch_size: int = 16    # Batch size for evaluation\n",
    "    warmup_steps: int = 500                 # Number of warmup steps\n",
    "    weight_decay: float = 0.01              # Weight decay\n",
    "    max_steps: int = 1000                   # Maximum number of training steps\n",
    "    save_steps: int = 500                   # Save model every `save_steps`\n",
    "    logging_dir: str = \"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT\\\\logs\"       # Directory for logs\n",
    "\n",
    "# Initialize TrainingConfig\n",
    "config = TrainingConfig()\n",
    "\n",
    "# Initialize ModelTraining\n",
    "model_training = ModelTraining(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%os._exit(00)` not found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "%os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.26.0'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import accelerate\n",
    "\n",
    "accelerate.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
