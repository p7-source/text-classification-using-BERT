{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows-SSD\n",
      " Volume Serial Number is 5824-84FE\n",
      "\n",
      " Directory of c:\\Users\\prass\\OneDrive\\Desktop\\practise\\new_env\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/03/2025  10:10 PM    <DIR>          .\n",
      "02/03/2025  06:58 PM    <DIR>          ..\n",
      "02/03/2025  10:02 PM    <DIR>          .github\n",
      "02/17/2025  03:23 PM    <DIR>          text-classification-using-BERT\n",
      "               0 File(s)              0 bytes\n",
      "               4 Dir(s)  256,057,806,848 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('text-classification-using-BERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    datasets_dir: Path  # Directory where datasets are saved\n",
    "    output_dir: Path  # Directory to save training outputs\n",
    "    model_save_path: Path  # Directory to save the trained model\n",
    "    num_train_epochs: int  # Number of training epochs\n",
    "    per_device_train_batch_size: int  # Training batch size per device\n",
    "    per_device_eval_batch_size: int  # Evaluation batch size per device\n",
    "    warmup_steps: int  # Number of warmup steps\n",
    "    weight_decay: float  # Weight decay rate\n",
    "    max_steps: int  # Maximum number of training steps\n",
    "    save_steps: int  # Save model every `save_steps` steps\n",
    "    logging_steps: int  # Log metrics every `logging_steps` steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CONFIG_FILE_PATH = Path(\"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT/config/config.yaml\")\n",
    "PARAMS_FILE_PATH = Path(\"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT/params.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\config\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "print(CONFIG_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'usr\\\\bin\\\\spam'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "# from pathlib import Path\n",
    "# path = os.path.join('usr', 'bin', 'spam')\n",
    "# print(Path(path))\n",
    "import os\n",
    "# from pathlib import Path\n",
    "os.path.join('usr', 'bin', 'spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textClassifier.constants import *\n",
    "from textClassifier.utils.commons import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "        # Debug: Print the config and params objects\n",
    "        print(\"Config:\", self.config)\n",
    "        print(\"Params:\", self.params)\n",
    "\n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        \"\"\"\n",
    "        Returns the configuration for model training.\n",
    "        \"\"\"\n",
    "        training = self.config.feature_engineering\n",
    "        print(\"i am printing training\", training)\n",
    "        params = self.params\n",
    "        training_data = os.path.join(self.config.artifacts_root, 'feature_engineering')\n",
    "        model_save_path = os.path.join(self.config.artifacts_root, 'training')\n",
    "\n",
    "        create_directories([training.root_dir, training.model_save_path])\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            datasets_dir=Path(training_data),\n",
    "            output_dir=Path(training.root_dir),\n",
    "            # model_save_path=Path(training.model_save_path),\n",
    "            params_num_train_epochs=params.num_train_epochs,\n",
    "            params_per_device_train_batch_size=params.per_device_train_batch_size,\n",
    "            params_per_device_eval_batch_size=params.per_device_eval_batch_size,\n",
    "            params_warmup_steps=params.warmup_steps,\n",
    "            params_weight_decay=params.weight_decay,\n",
    "            params_max_steps=params.max_steps,\n",
    "            params_save_steps=params.save_steps,\n",
    "            params_logging_steps=params.logging_steps\n",
    "            \n",
    "        )\n",
    "\n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        \"\"\"\n",
    "        Returns the configuration for model training.\n",
    "        \"\"\"\n",
    "        training = self.config.training\n",
    "        params = self.params\n",
    "\n",
    "        # Create directories if they don't exist\n",
    "        create_directories([\n",
    "            Path(training.root_dir),\n",
    "            Path(training.model_save_path)\n",
    "        ])\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            datasets_dir=Path(self.config.feature_engineering.training_cleansed_data),  # Update this to point to feature_engineering\n",
    "            output_dir=Path(training.root_dir),\n",
    "            model_save_path=Path(training.model_save_path),\n",
    "            num_train_epochs=params.num_train_epochs,\n",
    "            per_device_train_batch_size=params.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=params.per_device_eval_batch_size,\n",
    "            warmup_steps=params.warmup_steps,\n",
    "            weight_decay=params.weight_decay,\n",
    "            max_steps=params.max_steps,\n",
    "            save_steps=params.save_steps,\n",
    "            logging_steps=params.logging_steps\n",
    "        )\n",
    "\n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This below code is for BERT but we are using distllBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# #  this is bert model or bert-based-uncased but we downgraded to distillBert for resources bottleneks\n",
    "# from transformers import (\n",
    "#     BertForSequenceClassification,\n",
    "#     Trainer,\n",
    "#     TrainingArguments\n",
    "# )\n",
    "\n",
    "# from pathlib import Path\n",
    "# import torch\n",
    "# from textClassifier import logger\n",
    "\n",
    "\n",
    "# class ModelTraining:\n",
    "#     def __init__(self, config: TrainingConfig):\n",
    "#         \"\"\"\n",
    "#         Initializes the ModelTraining class.\n",
    "\n",
    "#         Args:\n",
    "#             config (ModelTrainingConfig): Configuration for model training.\n",
    "#         \"\"\"\n",
    "#         self.config = config\n",
    "\n",
    "#     def load_datasets(self):\n",
    "#         \"\"\"\n",
    "#         Loads the train, validation, and test datasets from the specified directory.\n",
    "\n",
    "#         Returns:\n",
    "#             train_dataset, val_dataset, test_dataset: Loaded datasets.\n",
    "#         \"\"\"\n",
    "#         datasets_dir = Path(self.config.feature_engineering)\n",
    "#         train_dataset = torch.load(datasets_dir / \"train_dataset.pt\")\n",
    "#         val_dataset = torch.load(datasets_dir / \"val_dataset.pt\")\n",
    "#         test_dataset = torch.load(datasets_dir / \"test_dataset.pt\")\n",
    "\n",
    "#         logger.info(f\"Datasets loaded from {datasets_dir}\")\n",
    "#         return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "#     def train(self):\n",
    "#         \"\"\"\n",
    "#         Trains the model using the loaded datasets.\n",
    "#         \"\"\"\n",
    "#         # Load datasets\n",
    "#         train_dataset, val_dataset, _ = self.load_datasets()\n",
    "\n",
    "#         # Initialize the model\n",
    "#         model = BertForSequenceClassification.from_pretrained(\n",
    "#             'bert-base-uncased', num_labels=3\n",
    "#         )\n",
    "\n",
    "#         # Set up training arguments\n",
    "#         training_args = TrainingArguments(\n",
    "#             output_dir=self.config.output_dir,\n",
    "#             num_train_epochs=self.config.num_train_epochs,\n",
    "#             per_device_train_batch_size=self.config.per_device_train_batch_size,\n",
    "#             per_device_eval_batch_size=self.config.per_device_eval_batch_size,\n",
    "#             warmup_steps=self.config.warmup_steps,\n",
    "#             weight_decay=self.config.weight_decay,\n",
    "#             max_steps=self.config.max_steps,\n",
    "#             save_steps=self.config.save_steps,\n",
    "#             logging_steps=self.config.logging_steps,\n",
    "#             load_best_model_at_end=True\n",
    "#         )\n",
    "\n",
    "#         # Initialize the Trainer\n",
    "#         trainer = Trainer(\n",
    "#             model=model,\n",
    "#             args=training_args,\n",
    "#             train_dataset=train_dataset,\n",
    "#             eval_dataset=val_dataset\n",
    "#         )\n",
    "\n",
    "#         # Start training\n",
    "#         trainer.train()\n",
    "\n",
    "#         # Save the trained model\n",
    "#         self.save_model(trainer)\n",
    "\n",
    "#     def save_model(self, trainer):\n",
    "#         \"\"\"\n",
    "#         Saves the trained model and tokenizer to the specified directory.\n",
    "\n",
    "#         Args:\n",
    "#             trainer (Trainer): The Trainer object containing the trained model.\n",
    "#         \"\"\"\n",
    "#         save_path = Path(self.config.model_save_path)\n",
    "#         save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#         trainer.save_model(save_path)\n",
    "#         logger.info(f\"Model saved to {save_path}\")\n",
    "\n",
    "#     def evaluate(self):\n",
    "#         \"\"\"\n",
    "#         Evaluates the model on the test dataset.\n",
    "#         \"\"\"\n",
    "#         # Load the test dataset\n",
    "#         _, _, test_dataset = self.load_datasets()\n",
    "\n",
    "#         # Load the trained model\n",
    "#         model = BertForSequenceClassification.from_pretrained(self.config.model_save_path)\n",
    "\n",
    "#         # Set up training arguments for evaluation\n",
    "#         training_args = TrainingArguments(\n",
    "#             output_dir=self.config.output_dir,\n",
    "#             per_device_eval_batch_size=self.config.per_device_eval_batch_size,\n",
    "#             logging_dir=self.config.logging_dir\n",
    "#         )\n",
    "\n",
    "#         # Initialize the Trainer for evaluation\n",
    "#         trainer = Trainer(\n",
    "#             model=model,\n",
    "#             args=training_args,\n",
    "#             # train_dataset=train_dataset, # The dataset used for training the model\n",
    "#             # eval_dataset=val_dataset\n",
    "#         )\n",
    "\n",
    "#         # Evaluate the model\n",
    "#         results = trainer.evaluate(test_dataset)\n",
    "#         logger.info(\"Evaluation Results:\")\n",
    "#         logger.info(f\"  - Loss: {results['eval_loss']:.4f}\")\n",
    "#         logger.info(f\"  - Runtime: {results['eval_runtime']:.2f} seconds\")\n",
    "#         logger.info(f\"  - Samples per Second: {results['eval_samples_per_second']:.2f}\")\n",
    "#         logger.info(f\"  - Steps per Second: {results['eval_steps_per_second']:.2f}\")\n",
    "#         logger.info(f\"  - Epoch: {results['epoch']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual Training starts here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CONFIG_FILE_PATH = Path(\"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT/config/config.yaml\")\n",
    "PARAMS_FILE_PATH = Path(\"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT/params.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textClassifier.constants import *\n",
    "from textClassifier.utils.commons import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments, \n",
    "    DataCollatorWithPadding,\n",
    "    AutoTokenizer\n",
    ")\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from textClassifier import logger\n",
    "\n",
    "\n",
    "class ModelTraining:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        \"\"\"\n",
    "        Initializes the ModelTraining class.\n",
    "\n",
    "        Args:\n",
    "            config (ModelTrainingConfig): Configuration for model training.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "    def load_datasets(self):\n",
    "        \"\"\"\n",
    "        Loads the train, validation, and test datasets from the specified directory.\n",
    "\n",
    "        Returns:\n",
    "            train_dataset, val_dataset, test_dataset: Loaded datasets.\n",
    "        \"\"\"\n",
    "        datasets_dir = Path(self.config.datasets_dir)\n",
    "        train_dataset = torch.load(datasets_dir / \"train_dataset.pt\", weights_only=False)\n",
    "        val_dataset = torch.load(datasets_dir / \"val_dataset.pt\", weights_only=False)\n",
    "        test_dataset = torch.load(datasets_dir / \"test_dataset.pt\", weights_only=False)\n",
    "\n",
    "        logger.info(f\"Datasets loaded from {datasets_dir}\")\n",
    "        return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the model using the loaded datasets.\n",
    "        \"\"\"\n",
    "        # Load datasets\n",
    "        train_dataset, val_dataset, _ = self.load_datasets()\n",
    "\n",
    "        # Initialize the model\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"distilbert-base-uncased\", num_labels=3\n",
    "            )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "        \n",
    "\n",
    "        #model = BertForSequenceClassification.from_pretrained(\n",
    "        #    'bert-base-uncased', num_labels=3\n",
    "        #)\n",
    "\n",
    "        # Set up training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config.output_dir,\n",
    "            num_train_epochs=self.config.num_train_epochs,\n",
    "            per_device_train_batch_size=self.config.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=self.config.per_device_eval_batch_size,\n",
    "            warmup_steps=self.config.warmup_steps,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            max_steps=self.config.max_steps,\n",
    "            save_steps=self.config.save_steps,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            # load_best_model_at_end=True\n",
    "        )\n",
    "\n",
    "        # Initialize the Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator\n",
    "        )\n",
    "\n",
    "        # Start training\n",
    "        trainer.train()\n",
    "\n",
    "        # Save the trained model\n",
    "        self.save_model(trainer)\n",
    "\n",
    "    def save_model(self, trainer):\n",
    "        \"\"\"\n",
    "        Saves the trained model and tokenizer to the specified directory.\n",
    "\n",
    "        Args:\n",
    "            trainer (Trainer): The Trainer object containing the trained model.\n",
    "        \"\"\"\n",
    "        save_path = Path(self.config.model_save_path)\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        trainer.save_model(save_path)\n",
    "        logger.info(f\"Model saved to {save_path}\")\n",
    "\n",
    "        # # Save the tokenizer\n",
    "        # tokenizer = trainer.tokenizer\n",
    "        # if tokenizer is not None:\n",
    "        #     tokenizer.save_pretrained(save_path)\n",
    "        #     logger.info(f\"Tokenizer saved to {save_path}\")\n",
    "        # else:\n",
    "        #     logger.warning(\"Tokenizer not found in the trainer object. Only the model was saved.\")\n",
    "\n",
    "        # Save the processing class (e.g., tokenizer)\n",
    "        processing_class = getattr(trainer, \"processing_class\", None)\n",
    "        if processing_class is not None:\n",
    "            processing_class.save_pretrained(save_path)\n",
    "            logger.info(f\"Processing class (e.g., tokenizer) saved to {save_path}\")\n",
    "        else:\n",
    "            logger.warning(\"Processing class not found. Only the model was saved.\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluates the model on the test dataset.\n",
    "        \"\"\"\n",
    "        # Load the test dataset\n",
    "        _, _, test_dataset = self.load_datasets()\n",
    "\n",
    "        # Load the trained model\n",
    "        # model = AutoModelForSequenceClassification.from_pretrained(self.config.model_save_path)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(self.config.model_save_path)\n",
    "\n",
    "\n",
    "        # Set up training arguments for evaluation\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config.output_dir,\n",
    "            per_device_eval_batch_size=self.config.per_device_eval_batch_size\n",
    "        )\n",
    "\n",
    "        # Initialize the Trainer for evaluation\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args\n",
    "            \n",
    "        )\n",
    "\n",
    "        # Evaluate the model\n",
    "        results = trainer.evaluate(test_dataset)\n",
    "\n",
    "        logger.info(\"Evaluation Results:\")\n",
    "        logger.info(\"Evaluation Results:\", results)\n",
    "        logger.info(f\"  - Loss: {results['eval_loss']:.4f}\")\n",
    "        logger.info(f\"  - Runtime: {results['eval_runtime']:.2f} seconds\")\n",
    "        logger.info(f\"  - Samples per Second: {results['eval_samples_per_second']:.2f}\")\n",
    "        logger.info(f\"  - Steps per Second: {results['eval_steps_per_second']:.2f}\")\n",
    "        logger.info(f\"  - Epoch: {results.get('epoch', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textClassifier.components import SentimentDataset  # Import the SentimentDataset class\n",
    "# import torch\n",
    "\n",
    "# # Load your data and create datasets\n",
    "# train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "# val_dataset = SentimentDataset(val_encodings, val_labels)\n",
    "# test_dataset = SentimentDataset(test_encodings, test_labels)\n",
    "\n",
    "# # Save the datasets\n",
    "# torch.save(train_dataset, \"artifacts/feature_engineering/train_dataset.pt\")\n",
    "# torch.save(val_dataset, \"artifacts/feature_engineering/val_dataset.pt\")\n",
    "# torch.save(test_dataset, \"artifacts/feature_engineering/test_dataset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "def convert_to_dicts(tokenized_texts):\n",
    "    \"\"\"\n",
    "    Converts a list of tokenized texts into a dictionary of input IDs and attention masks.\n",
    "\n",
    "    Args:\n",
    "        tokenized_texts (list): List of tokenized texts (each is a dictionary with 'input_ids' and 'attention_mask').\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing 'input_ids' and 'attention_mask' as lists.\n",
    "    \"\"\"\n",
    "    input_ids = [d['input_ids'].squeeze(0) for d in tokenized_texts]  # Remove batch dimension\n",
    "    attention_masks = [d['attention_mask'].squeeze(0) for d in tokenized_texts]  # Remove batch dimension\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_masks}\n",
    "\n",
    "\n",
    "def create_datasets(train_texts, train_labels, val_texts, val_labels, test_texts, test_labels):\n",
    "    \"\"\"\n",
    "    Creates SentimentDataset objects for train, validation, and test splits.\n",
    "\n",
    "    Args:\n",
    "        train_texts (list): List of tokenized texts for the training set.\n",
    "        train_labels (list): List of labels for the training set.\n",
    "        val_texts (list): List of tokenized texts for the validation set.\n",
    "        val_labels (list): List of labels for the validation set.\n",
    "        test_texts (list): List of tokenized texts for the test set.\n",
    "        test_labels (list): List of labels for the test set.\n",
    "\n",
    "    Returns:\n",
    "        train_dataset, val_dataset, test_dataset: SentimentDataset objects for each split.\n",
    "    \"\"\"\n",
    "    # Convert tokenized texts to encodings\n",
    "    train_encodings = convert_to_dicts(train_texts)\n",
    "    val_encodings = convert_to_dicts(val_texts)\n",
    "    test_encodings = convert_to_dicts(test_texts)\n",
    "\n",
    "    # Create SentimentDataset objects\n",
    "    train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "    val_dataset = SentimentDataset(val_encodings, val_labels)\n",
    "    test_dataset = SentimentDataset(test_encodings, test_labels)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def save_datasets(self, train_dataset, val_dataset, test_dataset):\n",
    "        \"\"\"\n",
    "        Saves the train, validation, and test datasets to the specified directory.\n",
    "\n",
    "        Args:\n",
    "            train_dataset: Training dataset.\n",
    "            val_dataset: Validation dataset.\n",
    "            test_dataset: Test dataset.\n",
    "        \"\"\"\n",
    "        # Create the directory if it doesn't exist\n",
    "        datasets_dir = Path(self.config.datasets_dir)\n",
    "        datasets_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save the datasets\n",
    "        torch.save(train_dataset, datasets_dir / \"train_dataset.pt\")\n",
    "        torch.save(val_dataset, datasets_dir / \"val_dataset.pt\")\n",
    "        torch.save(test_dataset, datasets_dir / \"test_dataset.pt\")\n",
    "\n",
    "        logger.info(f\"Datasets saved to {datasets_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-10 11:00:15,518: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\config\\config.yaml loaded successfully]\n",
      "[2025-03-10 11:00:15,522: INFO: commons: yaml file: c:\\Users\\prass\\OneDrive\\desktop\\practise\\new_env\\text-classification-using-BERT\\params.yaml loaded successfully]\n",
      "[2025-03-10 11:00:15,525: INFO: commons: created directory at: artifacts]\n",
      "[2025-03-10 11:00:15,527: INFO: commons: created directory at: artifacts\\training]\n",
      "[2025-03-10 11:00:15,530: INFO: commons: created directory at: artifacts\\training\\trained_model]\n",
      "[2025-03-10 11:00:25,210: INFO: 2939630643: Datasets loaded from artifacts\\feature_engineering]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\prass\\AppData\\Local\\Temp\\ipykernel_67756\\2939630643.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\prass\\AppData\\Local\\Temp\\ipykernel_67756\\2659941139.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 04:42, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.976229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prass\\AppData\\Local\\Temp\\ipykernel_67756\\2659941139.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\prass\\AppData\\Local\\Temp\\ipykernel_67756\\2659941139.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\prass\\AppData\\Local\\Temp\\ipykernel_67756\\2659941139.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\prass\\AppData\\Local\\Temp\\ipykernel_67756\\2659941139.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\prass\\AppData\\Local\\Temp\\ipykernel_67756\\2659941139.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-10 11:05:14,315: INFO: 2939630643: Model saved to artifacts\\training\\trained_model]\n",
      "[2025-03-10 11:05:14,349: INFO: 2939630643: Processing class (e.g., tokenizer) saved to artifacts\\training\\trained_model]\n",
      "[2025-03-10 11:05:17,142: INFO: 2939630643: Datasets loaded from artifacts\\feature_engineering]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1098' max='1098' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1098/1098 03:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-10 11:09:09,142: INFO: 2939630643: Evaluation Results:]\n",
      "[2025-03-10 11:09:09,142: INFO: 2939630643: Evaluation Results:]\n",
      "[2025-03-10 11:09:09,145: INFO: 2939630643:   - Loss: 0.9623]\n",
      "[2025-03-10 11:09:09,146: INFO: 2939630643:   - Runtime: 231.53 seconds]\n",
      "[2025-03-10 11:09:09,148: INFO: 2939630643:   - Samples per Second: 9.48]\n",
      "[2025-03-10 11:09:09,149: INFO: 2939630643:   - Steps per Second: 4.74]\n",
      "[2025-03-10 11:09:09,152: INFO: 2939630643:   - Epoch: N/A]\n"
     ]
    }
   ],
   "source": [
    "from accelerate import PartialState\n",
    "accelerator_state_kwargs = {\"enabled\": True, \"use_configured_state\": False}\n",
    "\n",
    "# Initialize PartialState\n",
    "\n",
    "from textClassifier.components import sentimentsDataset\n",
    "if __name__ == \"__main__\":\n",
    "    # import mlflow\n",
    "    # mlflow.set_tracking_uri(None)\n",
    "    # Initialize ConfigurationManager\n",
    "    config_manager = ConfigurationManager()\n",
    "    # Get the model training config\n",
    "    training_config = config_manager.get_training_config()\n",
    "\n",
    "    # Initialize ModelTraining\n",
    "    model_training = ModelTraining(config=training_config)\n",
    "\n",
    "    # Train the model\n",
    "    model_training.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    model_training.evaluate()\n",
    "    \n",
    "    partial_state = PartialState()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "class SentimentInference:\n",
    "    def __init__(self, model_path: str):\n",
    "        \"\"\"\n",
    "        Initializes the SentimentInference class.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): Path to the trained model and tokenizer.\n",
    "        \"\"\"\n",
    "        self.model_path = Path(model_path)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        self.sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "    def predict_sentiment(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Predicts the sentiment of the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text for sentiment prediction.\n",
    "\n",
    "        Returns:\n",
    "            str: Predicted sentiment ('negative', 'neutral', 'positive').\n",
    "        \"\"\"\n",
    "        # Tokenize the input text\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        \n",
    "        # Get model predictions\n",
    "        outputs = self.model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "        \n",
    "        # Map the predicted class to sentiment\n",
    "        return self.sentiment_map[predicted_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows-SSD\n",
      " Volume Serial Number is 5824-84FE\n",
      "\n",
      " Directory of c:\\Users\\prass\\OneDrive\\Desktop\\practise\\new_env\\text-classification-using-BERT\n",
      "\n",
      "02/17/2025  03:23 PM    <DIR>          .\n",
      "02/03/2025  10:10 PM    <DIR>          ..\n",
      "02/03/2025  10:15 PM    <DIR>          .github\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/05/2025  11:14 AM             3,597 .gitignore\n",
      "03/04/2025  09:54 PM    <DIR>          artifacts\n",
      "02/03/2025  10:15 PM    <DIR>          config\n",
      "02/03/2025  10:15 PM                 0 dvc.yaml\n",
      "02/03/2025  09:43 PM             1,087 LICENSE\n",
      "02/03/2025  10:50 PM    <DIR>          logs\n",
      "02/05/2025  11:31 AM               440 main.py\n",
      "03/04/2025  08:59 PM               170 params.yaml\n",
      "02/03/2025  09:43 PM                32 README.md\n",
      "02/03/2025  10:26 PM               248 requirements.txt\n",
      "03/04/2025  09:08 PM    <DIR>          research\n",
      "02/03/2025  10:16 PM               825 setup.py\n",
      "02/03/2025  10:28 PM    <DIR>          src\n",
      "02/07/2025  10:25 AM             1,344 template.py\n",
      "02/05/2025  11:31 AM    <DIR>          templates\n",
      "03/03/2025  04:07 PM    <DIR>          wandb\n",
      "               9 File(s)          7,743 bytes\n",
      "              10 Dir(s)  251,240,882,176 bytes free\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\\\\tokenizer_config.json',\n",
       " 'C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\\\\special_tokens_map.json',\n",
       " 'C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\\\\vocab.txt',\n",
       " 'C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\\\\added_tokens.json',\n",
       " 'C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\\\\tokenizer.json')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# # Load the model and tokenizer\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# # Save the model and tokenizer\n",
    "# model.save_pretrained(\"C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\")\n",
    "# tokenizer.save_pretrained(\"C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "class SentimentInference:\n",
    "    def __init__(self, model_path: str):\n",
    "        \"\"\"\n",
    "        Initializes the SentimentInference class.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): Path to the trained model and tokenizer.\n",
    "        \"\"\"\n",
    "        self.model_path = Path(model_path)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        self.sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "    def predict_sentiment(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Predicts the sentiment of the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text for sentiment prediction.\n",
    "\n",
    "        Returns:\n",
    "            str: Predicted sentiment ('negative', 'neutral', 'positive').\n",
    "        \"\"\"\n",
    "        # Tokenize the input text\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        \n",
    "        # Get model predictions\n",
    "        outputs = self.model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "        \n",
    "        # Map the predicted class to sentiment\n",
    "        return self.sentiment_map[predicted_class]\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Provide the path to your trained model and tokenizer\n",
    "#     model_path = \"C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\"\n",
    "    \n",
    "#     # Initialize the SentimentInference class\n",
    "#     pred = SentimentInference(model_path)\n",
    "    \n",
    "#     # Test the prediction\n",
    "#     test = pred.predict_sentiment('I hate flying with airlines')\n",
    "#     print(f\"Predicted Sentiment: {test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_sentiment(text):\n",
    "#     '''Function to predict the sentiment of a given text using a pre-trained BERT model.\n",
    "#     Args: the input text for sentiment prediction.\n",
    "#     Returns: the predicted sentiment ('negative', 'neutral', 'positive').\n",
    "#     '''\n",
    "\n",
    "#     inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "#     outputs = model(**inputs)\n",
    "#     predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "#     predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "#     sentiment = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "#     return sentiment[predicted_class]\n",
    "\n",
    "# # Example prediction\n",
    "# example_text = \"I hate flying with this airline!\"\n",
    "# predicted_sentiment = predict_sentiment(example_text)\n",
    "# print(f\"Predicted Sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerate version: 0.26.0\n",
      "Transformers version: 4.49.0\n"
     ]
    }
   ],
   "source": [
    "# import accelerate\n",
    "# import transformers\n",
    "\n",
    "# print(\"Accelerate version:\", accelerate.__version__)\n",
    "# print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is for reference to basic web page to predict the predictions\n",
    "# from flask import Flask, request, jsonify, render_template_string\n",
    "\n",
    "# # Initialize the Flask app\n",
    "# app = Flask(__name__)\n",
    "\n",
    "# # Dummy SentimentInference class for demonstration\n",
    "# class SentimentInference:\n",
    "#     def __init__(self, model_path):\n",
    "#         self.model_path = model_path\n",
    "\n",
    "#     def predict_sentiment(self, text):\n",
    "#         Dummy sentiment prediction logic\n",
    "#         if \"good\" in text.lower():\n",
    "#             return \"Positive\"\n",
    "#         elif \"bad\" in text.lower():\n",
    "#             return \"Negative\"\n",
    "#         else:\n",
    "#             return \"Neutral\"\n",
    "#         # try:\n",
    "#         #     sentiment = sentiment_inference.predict_sentiment(text)\n",
    "#         #     return jsonify({\"sentiment\": sentiment})\n",
    "#         # except Exception as e:\n",
    "#         #     return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "# # Initialize the SentimentInference class\n",
    "# model_path = \"C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/testing_dir\"\n",
    "# sentiment_inference = SentimentInference(model_path)\n",
    "\n",
    "# # Home page route\n",
    "# @app.route(\"/\", methods=[\"GET\"])\n",
    "# def home():\n",
    "#     return \"\"\"\n",
    "#     <h1>Welcome to Sentiment Analysis</h1>\n",
    "#     <p>Click <a href=\"/predict\">here</a> to go to the prediction page.</p>\n",
    "#     \"\"\"\n",
    "\n",
    "# # Predict page route\n",
    "# @app.route(\"/predict\", methods=[\"GET\", \"POST\"])\n",
    "# def predict():\n",
    "#     if request.method == \"GET\":\n",
    "#         # Render a basic HTML form for text input\n",
    "#         return render_template_string('''\n",
    "#         <h1>Sentiment Prediction</h1>\n",
    "#         <form method=\"POST\">\n",
    "#             <label for=\"text\">Enter your text:</label><br>\n",
    "#             <textarea id=\"text\" name=\"text\" rows=\"4\" cols=\"50\"></textarea><br><br>\n",
    "#             <input type=\"submit\" value=\"Predict\">\n",
    "#         </form>\n",
    "#         ''')\n",
    "#     elif request.method == \"POST\":\n",
    "#         # Get the input text from the form\n",
    "#         text = request.form.get(\"text\")\n",
    "        \n",
    "#         # Validate the input\n",
    "#         if not text:\n",
    "#             return jsonify({\"error\": \"No text provided\"}), 400\n",
    "        \n",
    "#         # Predict the sentiment\n",
    "#         try:\n",
    "#             sentiment = sentiment_inference.predict_sentiment(text)\n",
    "#             return jsonify({\"sentiment\": sentiment})\n",
    "#         except Exception as e:\n",
    "#             return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Run the Flask app\n",
    "#     app.run(host=\"0.0.0.0\", port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n",
      "[2025-03-10 11:10:27,310: INFO: _internal: \u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.1.117:5000]\n",
      "[2025-03-10 11:10:27,312: INFO: _internal: \u001b[33mPress CTRL+C to quit\u001b[0m]\n",
      "[2025-03-10 11:10:32,601: INFO: _internal: 127.0.0.1 - - [10/Mar/2025 11:10:32] \"GET / HTTP/1.1\" 200 -]\n",
      "[2025-03-10 11:10:34,995: INFO: _internal: 127.0.0.1 - - [10/Mar/2025 11:10:34] \"GET /predict HTTP/1.1\" 200 -]\n",
      "[2025-03-10 11:10:42,950: INFO: _internal: 127.0.0.1 - - [10/Mar/2025 11:10:42] \"POST /predict HTTP/1.1\" 200 -]\n",
      "[2025-03-10 11:10:48,498: INFO: _internal: 127.0.0.1 - - [10/Mar/2025 11:10:48] \"GET / HTTP/1.1\" 200 -]\n",
      "[2025-03-10 11:10:51,202: INFO: _internal: 127.0.0.1 - - [10/Mar/2025 11:10:51] \"GET /predict HTTP/1.1\" 200 -]\n",
      "[2025-03-10 11:11:16,212: INFO: _internal: 127.0.0.1 - - [10/Mar/2025 11:11:16] \"POST /predict HTTP/1.1\" 200 -]\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, render_template_string\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize the Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Your SentimentInference class\n",
    "class SentimentInference:\n",
    "    def __init__(self, model_path: str):\n",
    "        \"\"\"\n",
    "        Initializes the SentimentInference class.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): Path to the trained model and tokenizer.\n",
    "        \"\"\"\n",
    "        self.model_path = Path(model_path)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        self.sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "    def predict_sentiment(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Predicts the sentiment of the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text for sentiment prediction.\n",
    "\n",
    "        Returns:\n",
    "            str: Predicted sentiment ('negative', 'neutral', 'positive').\n",
    "        \"\"\"\n",
    "        # Tokenize the input text\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        \n",
    "        # Get model predictions\n",
    "        outputs = self.model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "        \n",
    "        # Map the predicted class to sentiment\n",
    "        return self.sentiment_map[predicted_class]\n",
    "\n",
    "# Initialize the SentimentInference class\n",
    "model_path = \"C:/Users/prass/OneDrive/desktop/practise/new_env/text-classification-using-BERT/artifacts/training/trained_model\"\n",
    "sentiment_inference = SentimentInference(model_path)\n",
    "\n",
    "# Home page route\n",
    "@app.route(\"/\", methods=[\"GET\"])\n",
    "def home():\n",
    "    return \"\"\"\n",
    "    <h1>Welcome to Sentiment Analysis / Test Classification</h1>\n",
    "    <p>Click <a href=\"/predict\">here</a> to go to the prediction page.</p>\n",
    "    \"\"\"\n",
    "\n",
    "# Predict page route\n",
    "@app.route(\"/predict\", methods=[\"GET\", \"POST\"])\n",
    "def predict():\n",
    "    if request.method == \"GET\":\n",
    "        # Render a basic HTML form for text input\n",
    "        return render_template_string('''\n",
    "        <h1>Sentiment Prediction</h1>\n",
    "        <form method=\"POST\">\n",
    "            <label for=\"text\">Enter your text:</label><br>\n",
    "            <textarea id=\"text\" name=\"text\" rows=\"4\" cols=\"50\"></textarea><br><br>\n",
    "            <input type=\"submit\" value=\"Predict\">\n",
    "        </form>\n",
    "        ''')\n",
    "    elif request.method == \"POST\":\n",
    "        # Get the input text from the form\n",
    "        text = request.form.get(\"text\")\n",
    "        \n",
    "        # Validate the input\n",
    "        if not text:\n",
    "            return jsonify({\"error\": \"No text provided\"}), 400\n",
    "        \n",
    "        # Predict the sentiment\n",
    "        try:\n",
    "            sentiment = sentiment_inference.predict_sentiment(text)\n",
    "            return jsonify({\"sentiment\": sentiment})\n",
    "        except Exception as e:\n",
    "            return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the Flask app\n",
    "    app.run(host=\"0.0.0.0\", port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='127.0.0.1', port=5000): Max retries exceeded with url: /predict (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000023397B16490>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\urllib3\\connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\urllib3\\connectionpool.py:493\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 493\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\urllib3\\connection.py:445\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[1;32m--> 445\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\http\\client.py:1280\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1280\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\http\\client.py:1040\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1040\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1043\u001b[0m \n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\http\\client.py:980\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 980\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\urllib3\\connection.py:276\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\urllib3\\connection.py:213\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    215\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    217\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.client.connect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport)\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x0000023397B16490>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='127.0.0.1', port=5000): Max retries exceeded with url: /predict (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000023397B16490>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://127.0.0.1:5000/predict\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a sample text for sentiment analysis.\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m----> 5\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\prass\\miniconda3\\envs\\tcenv\\lib\\site-packages\\requests\\adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='127.0.0.1', port=5000): Max retries exceeded with url: /predict (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000023397B16490>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))"
     ]
    }
   ],
   "source": [
    "# Code to test the website for prediction\n",
    "# import requests\n",
    "\n",
    "# url = \"http://127.0.0.1:5000/predict\"\n",
    "# data = {\"text\": \"This is a sample text for sentiment analysis.\"}\n",
    "# response = requests.post(url, json=data)\n",
    "\n",
    "# print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('feature_engineering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT\\\\artifacts\\\\prepare_model'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.chdir('prepare_model')\n",
    "# %pwd\n",
    "# %ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataclasses import dataclass\n",
    "\n",
    "# @dataclass\n",
    "# class TrainingConfig:\n",
    "#     datasets_dir: str = \"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT\\\\artifacts\\\\feature_engineering\"  # Directory containing train_dataset.pt, val_dataset.pt, test_dataset.pt\n",
    "#     output_dir: str = \"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT\\\\artifacts\\\\training\"      # Directory to save training outputs\n",
    "#     model_save_path: str = \"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT\\\\artifacts\\\\training\\\\trained_model\"  # Directory to save the trained model\n",
    "#     num_train_epochs: int = 1              # Number of training epochs\n",
    "#     per_device_train_batch_size: int = 16   # Batch size for training\n",
    "#     per_device_eval_batch_size: int = 16    # Batch size for evaluation\n",
    "#     warmup_steps: int = 500                 # Number of warmup steps\n",
    "#     weight_decay: float = 0.01              # Weight decay\n",
    "#     max_steps: int = 1000                   # Maximum number of training steps\n",
    "#     save_steps: int = 500                   # Save model every `save_steps`\n",
    "#     logging_dir: str = \"c:\\\\Users\\\\prass\\\\OneDrive\\\\desktop\\\\practise\\\\new_env\\\\text-classification-using-BERT\\\\logs\"       # Directory for logs\n",
    "\n",
    "# # Initialize TrainingConfig\n",
    "# config = TrainingConfig()\n",
    "\n",
    "# # Initialize ModelTraining\n",
    "# model_training = ModelTraining(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# %os._exit(00)\n",
    "# import accelerate\n",
    "\n",
    "# accelerate.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
